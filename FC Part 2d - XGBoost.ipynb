{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Build an XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import scipy.sparse\n",
    "import os\n",
    "\n",
    "# import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPool1D, Flatten, Input, concatenate, Dropout, Activation, regularizers, BatchNormalization\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print('TF version:',tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brizio\\Documents\\PythonNB\\FICOchallenge\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prepped data\n",
    "- Normal Scaling of numeric variables (prep_option = 1)\n",
    "- Binning (following Rudin) and one hot encoding (prep_option = 2)\n",
    "- Binning and applying WOE, calculating WOE on Rudin's bins (prep_option = 3)\n",
    "- Binning and applying WOE, following Rudin (prep_option = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_option = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Bad (y=1)\n",
      "Bad     5459\n",
      "Good    5000\n",
      "Name: RiskPerformance, dtype: int64\n",
      "[[   0 5000]\n",
      " [   1 5459]]\n",
      "X shape: (10459, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExternalRiskEstimate_bin_WOE</th>\n",
       "      <th>MSinceOldestTradeOpen_bin_WOE</th>\n",
       "      <th>MSinceMostRecentTradeOpen_bin_WOE</th>\n",
       "      <th>AverageMInFile_bin_WOE</th>\n",
       "      <th>NumSatisfactoryTrades_bin_WOE</th>\n",
       "      <th>NumTrades60Ever2DerogPubRec_bin_WOE</th>\n",
       "      <th>NumTrades90Ever2DerogPubRec_bin_WOE</th>\n",
       "      <th>NumTotalTrades_bin_WOE</th>\n",
       "      <th>NumTradesOpeninLast12M_bin_WOE</th>\n",
       "      <th>PercentTradesNeverDelq_bin_WOE</th>\n",
       "      <th>...</th>\n",
       "      <th>PercentInstallTrades_bin_WOE</th>\n",
       "      <th>NetFractionInstallBurden_bin_WOE</th>\n",
       "      <th>NumInstallTradesWBalance_bin_WOE</th>\n",
       "      <th>MSinceMostRecentInqexcl7days_bin_WOE</th>\n",
       "      <th>NumInqLast6M_bin_WOE</th>\n",
       "      <th>NumInqLast6Mexcl7days_bin_WOE</th>\n",
       "      <th>NetFractionRevolvingBurden_bin_WOE</th>\n",
       "      <th>NumRevolvingTradesWBalance_bin_WOE</th>\n",
       "      <th>NumBank2NatlTradesWHighUtilization_bin_WOE</th>\n",
       "      <th>PercentTradesWBalance_bin_WOE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.799</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.952</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.242</td>\n",
       "      <td>1.223</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.799</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1.238</td>\n",
       "      <td>1.999</td>\n",
       "      <td>0.952</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1.223</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.017</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1.238</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.242</td>\n",
       "      <td>1.223</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.017</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.313</td>\n",
       "      <td>1.223</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.094</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.242</td>\n",
       "      <td>1.223</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ExternalRiskEstimate_bin_WOE  MSinceOldestTradeOpen_bin_WOE  \\\n",
       "0                         1.799                          0.086   \n",
       "1                         1.799                          0.549   \n",
       "2                         1.017                          0.549   \n",
       "3                         1.017                          0.086   \n",
       "4                        -1.094                         -0.148   \n",
       "\n",
       "   MSinceMostRecentTradeOpen_bin_WOE  AverageMInFile_bin_WOE  \\\n",
       "0                              0.083                   0.269   \n",
       "1                              0.083                   1.238   \n",
       "2                              0.083                   1.238   \n",
       "3                              0.083                   0.269   \n",
       "4                              0.000                   0.000   \n",
       "\n",
       "   NumSatisfactoryTrades_bin_WOE  NumTrades60Ever2DerogPubRec_bin_WOE  \\\n",
       "0                          0.166                                0.952   \n",
       "1                          1.999                                0.952   \n",
       "2                          0.539                               -0.021   \n",
       "3                         -0.086                               -0.021   \n",
       "4                          0.539                               -0.021   \n",
       "\n",
       "   NumTrades90Ever2DerogPubRec_bin_WOE  NumTotalTrades_bin_WOE  \\\n",
       "0                               -0.021                  -0.097   \n",
       "1                               -0.053                   0.535   \n",
       "2                               -0.021                   0.535   \n",
       "3                               -0.021                  -0.377   \n",
       "4                               -0.021                   0.116   \n",
       "\n",
       "   NumTradesOpeninLast12M_bin_WOE  PercentTradesNeverDelq_bin_WOE  ...  \\\n",
       "0                          -0.021                           1.012  ...   \n",
       "1                          -0.021                          -0.147  ...   \n",
       "2                           0.428                          -0.147  ...   \n",
       "3                           0.287                           0.366  ...   \n",
       "4                          -0.021                          -0.147  ...   \n",
       "\n",
       "   PercentInstallTrades_bin_WOE  NetFractionInstallBurden_bin_WOE  \\\n",
       "0                        -0.503                             0.047   \n",
       "1                         0.161                             0.047   \n",
       "2                        -0.503                             0.147   \n",
       "3                        -0.145                             0.363   \n",
       "4                        -0.620                             0.363   \n",
       "\n",
       "   NumInstallTradesWBalance_bin_WOE  MSinceMostRecentInqexcl7days_bin_WOE  \\\n",
       "0                             0.242                                 1.223   \n",
       "1                             0.256                                 1.223   \n",
       "2                             0.242                                 1.223   \n",
       "3                             0.313                                 1.223   \n",
       "4                             0.242                                 1.223   \n",
       "\n",
       "   NumInqLast6M_bin_WOE  NumInqLast6Mexcl7days_bin_WOE  \\\n",
       "0                -0.047                         -0.051   \n",
       "1                -0.047                         -0.051   \n",
       "2                 0.170                          0.021   \n",
       "3                 0.471                          0.021   \n",
       "4                -0.047                         -0.051   \n",
       "\n",
       "   NetFractionRevolvingBurden_bin_WOE  NumRevolvingTradesWBalance_bin_WOE  \\\n",
       "0                              -0.088                               0.034   \n",
       "1                              -0.739                              -0.188   \n",
       "2                               0.633                              -0.263   \n",
       "3                               0.633                              -0.150   \n",
       "4                               0.633                              -0.188   \n",
       "\n",
       "   NumBank2NatlTradesWHighUtilization_bin_WOE  PercentTradesWBalance_bin_WOE  \n",
       "0                                      -0.601                         -0.130  \n",
       "1                                       0.601                         -0.982  \n",
       "2                                      -0.601                          0.203  \n",
       "3                                       0.541                          0.772  \n",
       "4                                      -0.601                          0.203  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if prep_option == 1:\n",
    "    data_path = \"Data/Scaled_data.csv\"\n",
    "if prep_option == 2:\n",
    "    data_path = \"Data/Bin_Encoded_data_v2.csv\"\n",
    "if prep_option == 3:\n",
    "    data_path = \"Data/WOE_data.csv\"\n",
    "if prep_option == 4:\n",
    "    data_path = \"Data/WOE_Rud_data.csv\"\n",
    "    \n",
    "ori_path = \"Data/heloc_dataset_v1.csv\"\n",
    "\n",
    "CLASS = 'RiskPerformance' \n",
    "\n",
    "data = pd.read_csv(ori_path)\n",
    "X1 = pd.read_csv(data_path)\n",
    "y = pd.read_csv(\"Data/y_data.csv\")\n",
    "\n",
    "print('Target: Bad (y=1)')\n",
    "class_names = sorted(y[CLASS].unique(),  reverse=True)\n",
    "print(y[CLASS].value_counts())\n",
    "y_onehot = pd.get_dummies(y[CLASS])[['Bad']]\n",
    "print(np.array(np.unique(y_onehot, return_counts=True)).T)\n",
    "\n",
    "print('X shape:',X1.shape)\n",
    "X1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:57:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 69.87%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_onehot_train, y_onehot_train)\n",
    "# make predictions for test data\n",
    "y_pred = xgb.predict(X_onehot_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_onehot_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using a random forest to optimize\n",
    "# import scikit_optimize as skopt\n",
    "from skopt import forest_minimize\n",
    "\n",
    "def tune_xgbc(params):\n",
    "# Implementation learned on a lesson of Mario Filho (Kagle Grandmaster) for parametes optmization.\n",
    "# Link to the video: https://www.youtube.com/watch?v=WhnkeasZNHI\n",
    "\n",
    "    \"\"\"Function to be passed as scikit-optimize minimizer/maximizer input\n",
    "\n",
    "    Parameters:\n",
    "    Tuples with information about the range that the optimizer should use for that parameter, \n",
    "    as well as the behaviour that it should follow in that range.\n",
    "\n",
    "    Returns:\n",
    "    float: the metric that should be minimized. If the objective is maximization, then the negative \n",
    "    of the desired metric must be returned. In this case, the negative AUC average generated by CV is returned.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #Hyperparameters to be optimized\n",
    "    print(params)\n",
    "    learning_rate = params[0] \n",
    "    n_estimators = params[1] \n",
    "    max_depth = params[2]\n",
    "    min_child_weight = params[3]\n",
    "    gamma = params[4]\n",
    "    subsample = params[5]\n",
    "    colsample_bytree = params[6]\n",
    "\n",
    "\n",
    "    #Model to be optimized\n",
    "    mdl = XGBClassifier(learning_rate = learning_rate, \n",
    "                            n_estimators = n_estimators, \n",
    "                            max_depth = max_depth, \n",
    "                            min_child_weight = min_child_weight, \n",
    "                            gamma = gamma, \n",
    "                            subsample = subsample, \n",
    "                            colsample_bytree = colsample_bytree, seed = 42)\n",
    "\n",
    "\n",
    "    #Cross-Validation in order to avoid overfitting\n",
    "    auc = cross_val_score(mdl, X_onehot_train, y_onehot_train, cv = 10, scoring = 'roc_auc')\n",
    "\n",
    "    print(auc.mean())\n",
    "    # as the function is minimization (forest_minimize), we need to use the negative of the desired metric (AUC)\n",
    "    return -auc.mean()\n",
    "\n",
    "# Creating a sample space in which the initial randomic search should be performed\n",
    "space = [(1e-3, 1e-1, 'log-uniform'), # learning rate\n",
    "          (100, 2000), # n_estimators\n",
    "          (1, 10), # max_depth \n",
    "          (1, 6.), # min_child_weight \n",
    "          (0, 0.5), # gamma \n",
    "          (0.5, 1.), # subsample \n",
    "          (0.5, 1.)] # colsample_bytree \n",
    "\n",
    "# Minimization using a random forest with 20 random samples and 50 iterations for Bayesian optimization.\n",
    "result = forest_minimize(tune_xgbc, space, random_state = 42, n_random_starts = 20, n_calls  = 25, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0026587543983272693,\n",
       " 1315,\n",
       " 5,\n",
       " 4.087407548138583,\n",
       " 0.3058265802441405,\n",
       " 0.5035331526098588,\n",
       " 0.5115312125207079]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:39:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 72.12%\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(learning_rate = result.x[0], \n",
    "                     n_estimators = result.x[1], \n",
    "                     max_depth = result.x[2], \n",
    "                     min_child_weight = result.x[3], \n",
    "                     gamma = result.x[4], \n",
    "                     subsample = result.x[5], \n",
    "                     colsample_bytree = result.x[6], \n",
    "                     seed = 42)\n",
    "\n",
    "\n",
    "xgb1.fit(X_onehot_train, y_onehot_train)\n",
    "# make predictions for test data\n",
    "y_pred = xgb1.predict(X_onehot_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_onehot_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "CLASS = 'RiskPerformance'\n",
    "\n",
    "# Split X and y\n",
    "X = data.drop(columns=[CLASS])\n",
    "y = data[CLASS]\n",
    "y_onehot = pd.get_dummies(y)[['Bad']]\n",
    "\n",
    "np.array(np.unique(y_onehot, return_counts=True)).T\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y_onehot['Bad'].astype('int8'))\n",
    "\n",
    "X_onehot_train, X_onehot_test, y_onehot_train, y_onehot_test, = \\\n",
    "train_test_split(X, y, test_size = .25, random_state = 2020, shuffle = True)\n",
    "print(X_onehot_train.shape, X_onehot_test.shape, y_onehot_train.shape, y_onehot_test.shape)\n",
    "\n",
    "RS = RobustScaler()\n",
    "scaledX = RS.fit_transform(X_onehot_train)\n",
    "scaledX = pd.DataFrame(X_onehot_train, columns = X_onehot_train.columns)\n",
    "colList = X_onehot_train.columns\n",
    "\n",
    "scaledX_test = pd.DataFrame(RS.transform(X_onehot_test), columns = X_onehot_test.columns)\n",
    "\n",
    "params = {\"verbosity\":0,\n",
    "          \"nthread\":-1,\n",
    "          \"seed\":1,\n",
    "          \"booster\":\"gbtree\",\n",
    "          \"lambda\":1,\n",
    "          \"alpha\":0,\n",
    "          \"learning_rate\":0.0085,\n",
    "          \"gamma\":0.642,\n",
    "          \"max_depth\":16,\n",
    "          \"min_child_weight\":5,\n",
    "          \"max_delta_step\":2,\n",
    "          \"subsample\":0.374,\n",
    "          \"colsample_bytree\":0.8280000000000001,\n",
    "          \"colsample_bylevel\":1,\n",
    "          \"scale_pos_weight\":1,\n",
    "          \"process_type\":\"default\",\n",
    "          \"tree_method\":\"auto\",\n",
    "          \"objective\":\"binary:logistic\",\n",
    "          \"eval_metric\":'auc'}\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "dtrain = xgb.DMatrix(scipy.sparse.csc_matrix(scaledX.to_numpy()), label = y_onehot_train, feature_names = colList)\n",
    "\n",
    "dtest  = xgb.DMatrix(scipy.sparse.csc_matrix(scaledX_test.to_numpy()), label = y_onehot_test, feature_names = colList)\n",
    "\n",
    "# evallist = (dtrain, 'train')\n",
    "xg_reg = xgb.train(params, dtrain, 750,  [(dtrain,'train'),(dtest,'test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.23%\n"
     ]
    }
   ],
   "source": [
    "# make predictions for test data\n",
    "y_pred = xg_reg.predict(dtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_onehot_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using a random forest to optimize\n",
    "# import scikit_optimize as skopt\n",
    "from skopt import forest_minimize\n",
    "\n",
    "def tune_xgbc(params):\n",
    "# Implementation learned on a lesson of Mario Filho (Kagle Grandmaster) for parametes optmization.\n",
    "# Link to the video: https://www.youtube.com/watch?v=WhnkeasZNHI\n",
    "\n",
    "    \"\"\"Function to be passed as scikit-optimize minimizer/maximizer input\n",
    "\n",
    "    Parameters:\n",
    "    Tuples with information about the range that the optimizer should use for that parameter, \n",
    "    as well as the behaviour that it should follow in that range.\n",
    "\n",
    "    Returns:\n",
    "    float: the metric that should be minimized. If the objective is maximization, then the negative \n",
    "    of the desired metric must be returned. In this case, the negative AUC average generated by CV is returned.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #Hyperparameters to be optimized\n",
    "    print(params)\n",
    "    learning_rate = params[0] \n",
    "    gamma = params[1] \n",
    "    max_depth = params[2]\n",
    "    min_child_weight = params[3]\n",
    "    max_delta_step = params[4]\n",
    "    subsample = params[5]\n",
    "    colsample_bytree = params[6]\n",
    "    rounds = params[7]\n",
    "\n",
    "    params1 = {\"silent\":1,\n",
    "              \"nthread\":-1,\n",
    "              \"seed\":1,\n",
    "              \"booster\":\"gbtree\",\n",
    "              \"lambda\":1,\n",
    "              \"alpha\":0,\n",
    "              \"learning_rate\":learning_rate,\n",
    "              \"gamma\":gamma,\n",
    "              \"max_depth\":max_depth,\n",
    "              \"min_child_weight\":min_child_weight,\n",
    "              \"max_delta_step\":max_delta_step,\n",
    "              \"subsample\":subsample,\n",
    "              \"colsample_bytree\":colsample_bytree,\n",
    "              \"colsample_bylevel\":1,\n",
    "              \"scale_pos_weight\":1,\n",
    "              \"process_type\":\"default\",\n",
    "              \"tree_method\":\"auto\",\n",
    "              \"objective\":\"binary:logistic\",\n",
    "              \"eval_metric\":'auc'}\n",
    "\n",
    "\n",
    "    #Model to be optimized\n",
    "    mdl = xgb.train(params1, dtrain, rounds,  [(dtrain,'train'),(dtest,'test')])\n",
    "\n",
    "\n",
    "    #Cross-Validation in order to avoid overfitting\n",
    "    auc = xgb.cv(params1, dtrain, nfold  = 10, metrics = 'auc')\n",
    "\n",
    "    print(auc.mean()[2])\n",
    "    # as the function is minimization (forest_minimize), we need to use the negative of the desired metric (AUC)\n",
    "    return -auc.mean()[2]\n",
    "\n",
    "# Creating a sample space in which the initial randomic search should be performed\n",
    "space = [(1e-3, 1e-1, 'log-uniform'), # learning rate\n",
    "          (0, 0.5), # gamma\n",
    "          (1, 30), # max_depth \n",
    "          (1, 6.), # min_child_weight \n",
    "          (0, 5), # max_delta_step \n",
    "          (0.5, 1.), # subsample \n",
    "          (0.5, 1.), # colsample_bytree \n",
    "          (10,1000)] # rounds\n",
    "\n",
    "# Minimization using a random forest with 20 random samples and 50 iterations for Bayesian optimization.\n",
    "result = forest_minimize(tune_xgbc, space, random_state = 42, n_random_starts = 20, n_calls  = 25, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04947535032796275,\n",
       " 0.27261958736380937,\n",
       " 6,\n",
       " 3.064296866484532,\n",
       " 4,\n",
       " 0.6680047054202385,\n",
       " 0.9831620375498387,\n",
       " 156]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.79507\ttest-auc:0.50277\n",
      "[1]\ttrain-auc:0.80696\ttest-auc:0.51860\n",
      "[2]\ttrain-auc:0.81502\ttest-auc:0.51386\n",
      "[3]\ttrain-auc:0.81821\ttest-auc:0.53876\n",
      "[4]\ttrain-auc:0.82074\ttest-auc:0.55240\n",
      "[5]\ttrain-auc:0.82293\ttest-auc:0.55316\n",
      "[6]\ttrain-auc:0.82524\ttest-auc:0.54122\n",
      "[7]\ttrain-auc:0.82687\ttest-auc:0.53702\n",
      "[8]\ttrain-auc:0.82805\ttest-auc:0.53702\n",
      "[9]\ttrain-auc:0.82877\ttest-auc:0.52892\n",
      "[10]\ttrain-auc:0.83034\ttest-auc:0.53534\n",
      "[11]\ttrain-auc:0.83231\ttest-auc:0.53307\n",
      "[12]\ttrain-auc:0.83368\ttest-auc:0.53607\n",
      "[13]\ttrain-auc:0.83436\ttest-auc:0.53100\n",
      "[14]\ttrain-auc:0.83565\ttest-auc:0.56098\n",
      "[15]\ttrain-auc:0.83632\ttest-auc:0.56276\n",
      "[16]\ttrain-auc:0.83734\ttest-auc:0.57935\n",
      "[17]\ttrain-auc:0.83751\ttest-auc:0.57944\n",
      "[18]\ttrain-auc:0.83832\ttest-auc:0.55937\n",
      "[19]\ttrain-auc:0.83939\ttest-auc:0.56898\n",
      "[20]\ttrain-auc:0.84054\ttest-auc:0.56722\n",
      "[21]\ttrain-auc:0.84147\ttest-auc:0.57966\n",
      "[22]\ttrain-auc:0.84176\ttest-auc:0.57997\n",
      "[23]\ttrain-auc:0.84302\ttest-auc:0.55693\n",
      "[24]\ttrain-auc:0.84416\ttest-auc:0.55703\n",
      "[25]\ttrain-auc:0.84469\ttest-auc:0.55974\n",
      "[26]\ttrain-auc:0.84546\ttest-auc:0.54400\n",
      "[27]\ttrain-auc:0.84639\ttest-auc:0.56818\n",
      "[28]\ttrain-auc:0.84691\ttest-auc:0.56884\n",
      "[29]\ttrain-auc:0.84748\ttest-auc:0.55069\n",
      "[30]\ttrain-auc:0.84834\ttest-auc:0.55403\n",
      "[31]\ttrain-auc:0.84876\ttest-auc:0.55482\n",
      "[32]\ttrain-auc:0.84941\ttest-auc:0.56194\n",
      "[33]\ttrain-auc:0.85030\ttest-auc:0.54625\n",
      "[34]\ttrain-auc:0.85065\ttest-auc:0.54621\n",
      "[35]\ttrain-auc:0.85126\ttest-auc:0.54760\n",
      "[36]\ttrain-auc:0.85175\ttest-auc:0.54757\n",
      "[37]\ttrain-auc:0.85253\ttest-auc:0.55389\n",
      "[38]\ttrain-auc:0.85293\ttest-auc:0.55171\n",
      "[39]\ttrain-auc:0.85328\ttest-auc:0.56334\n",
      "[40]\ttrain-auc:0.85366\ttest-auc:0.56175\n",
      "[41]\ttrain-auc:0.85398\ttest-auc:0.56496\n",
      "[42]\ttrain-auc:0.85480\ttest-auc:0.56503\n",
      "[43]\ttrain-auc:0.85551\ttest-auc:0.56066\n",
      "[44]\ttrain-auc:0.85592\ttest-auc:0.56128\n",
      "[45]\ttrain-auc:0.85703\ttest-auc:0.56291\n",
      "[46]\ttrain-auc:0.85759\ttest-auc:0.56319\n",
      "[47]\ttrain-auc:0.85803\ttest-auc:0.56319\n",
      "[48]\ttrain-auc:0.85910\ttest-auc:0.56524\n",
      "[49]\ttrain-auc:0.85973\ttest-auc:0.56483\n",
      "[50]\ttrain-auc:0.86036\ttest-auc:0.56679\n",
      "[51]\ttrain-auc:0.86123\ttest-auc:0.55945\n",
      "[52]\ttrain-auc:0.86184\ttest-auc:0.56556\n",
      "[53]\ttrain-auc:0.86270\ttest-auc:0.56525\n",
      "[54]\ttrain-auc:0.86317\ttest-auc:0.57509\n",
      "[55]\ttrain-auc:0.86379\ttest-auc:0.57852\n",
      "[56]\ttrain-auc:0.86418\ttest-auc:0.57881\n",
      "[57]\ttrain-auc:0.86493\ttest-auc:0.57276\n",
      "[58]\ttrain-auc:0.86563\ttest-auc:0.56095\n",
      "[59]\ttrain-auc:0.86639\ttest-auc:0.56363\n",
      "[60]\ttrain-auc:0.86709\ttest-auc:0.57660\n",
      "[61]\ttrain-auc:0.86811\ttest-auc:0.57778\n",
      "[62]\ttrain-auc:0.86848\ttest-auc:0.57676\n",
      "[63]\ttrain-auc:0.86880\ttest-auc:0.57581\n",
      "[64]\ttrain-auc:0.86958\ttest-auc:0.57538\n",
      "[65]\ttrain-auc:0.87019\ttest-auc:0.57541\n",
      "[66]\ttrain-auc:0.87044\ttest-auc:0.57752\n",
      "[67]\ttrain-auc:0.87119\ttest-auc:0.57766\n",
      "[68]\ttrain-auc:0.87173\ttest-auc:0.57615\n",
      "[69]\ttrain-auc:0.87211\ttest-auc:0.58288\n",
      "[70]\ttrain-auc:0.87258\ttest-auc:0.58436\n",
      "[71]\ttrain-auc:0.87324\ttest-auc:0.58020\n",
      "[72]\ttrain-auc:0.87399\ttest-auc:0.57994\n",
      "[73]\ttrain-auc:0.87476\ttest-auc:0.57879\n",
      "[74]\ttrain-auc:0.87505\ttest-auc:0.57860\n",
      "[75]\ttrain-auc:0.87570\ttest-auc:0.58593\n",
      "[76]\ttrain-auc:0.87607\ttest-auc:0.57698\n",
      "[77]\ttrain-auc:0.87623\ttest-auc:0.57885\n",
      "[78]\ttrain-auc:0.87661\ttest-auc:0.57940\n",
      "[79]\ttrain-auc:0.87686\ttest-auc:0.57872\n",
      "[80]\ttrain-auc:0.87716\ttest-auc:0.57692\n",
      "[81]\ttrain-auc:0.87755\ttest-auc:0.57603\n",
      "[82]\ttrain-auc:0.87791\ttest-auc:0.57595\n",
      "[83]\ttrain-auc:0.87800\ttest-auc:0.58390\n",
      "[84]\ttrain-auc:0.87866\ttest-auc:0.58043\n",
      "[85]\ttrain-auc:0.87904\ttest-auc:0.58173\n",
      "[86]\ttrain-auc:0.87955\ttest-auc:0.58168\n",
      "[87]\ttrain-auc:0.87989\ttest-auc:0.58232\n",
      "[88]\ttrain-auc:0.88033\ttest-auc:0.58687\n",
      "[89]\ttrain-auc:0.88081\ttest-auc:0.58608\n",
      "[90]\ttrain-auc:0.88116\ttest-auc:0.58607\n",
      "[91]\ttrain-auc:0.88165\ttest-auc:0.58665\n",
      "[92]\ttrain-auc:0.88253\ttest-auc:0.58774\n",
      "[93]\ttrain-auc:0.88287\ttest-auc:0.58796\n",
      "[94]\ttrain-auc:0.88331\ttest-auc:0.58782\n",
      "[95]\ttrain-auc:0.88393\ttest-auc:0.58784\n",
      "[96]\ttrain-auc:0.88429\ttest-auc:0.58786\n",
      "[97]\ttrain-auc:0.88448\ttest-auc:0.58786\n",
      "[98]\ttrain-auc:0.88506\ttest-auc:0.58766\n",
      "[99]\ttrain-auc:0.88516\ttest-auc:0.58795\n",
      "[100]\ttrain-auc:0.88560\ttest-auc:0.58562\n",
      "[101]\ttrain-auc:0.88578\ttest-auc:0.58443\n",
      "[102]\ttrain-auc:0.88607\ttest-auc:0.58483\n",
      "[103]\ttrain-auc:0.88639\ttest-auc:0.58454\n",
      "[104]\ttrain-auc:0.88688\ttest-auc:0.58427\n",
      "[105]\ttrain-auc:0.88727\ttest-auc:0.58427\n",
      "[106]\ttrain-auc:0.88759\ttest-auc:0.58420\n",
      "[107]\ttrain-auc:0.88783\ttest-auc:0.58470\n",
      "[108]\ttrain-auc:0.88836\ttest-auc:0.58614\n",
      "[109]\ttrain-auc:0.88870\ttest-auc:0.58589\n",
      "[110]\ttrain-auc:0.88919\ttest-auc:0.58806\n",
      "[111]\ttrain-auc:0.88949\ttest-auc:0.59356\n",
      "[112]\ttrain-auc:0.88980\ttest-auc:0.59283\n",
      "[113]\ttrain-auc:0.89025\ttest-auc:0.59284\n",
      "[114]\ttrain-auc:0.89044\ttest-auc:0.59772\n",
      "[115]\ttrain-auc:0.89092\ttest-auc:0.59789\n",
      "[116]\ttrain-auc:0.89114\ttest-auc:0.59787\n",
      "[117]\ttrain-auc:0.89135\ttest-auc:0.59774\n",
      "[118]\ttrain-auc:0.89191\ttest-auc:0.59745\n",
      "[119]\ttrain-auc:0.89255\ttest-auc:0.59632\n",
      "[120]\ttrain-auc:0.89325\ttest-auc:0.60394\n",
      "[121]\ttrain-auc:0.89422\ttest-auc:0.60346\n",
      "[122]\ttrain-auc:0.89464\ttest-auc:0.60363\n",
      "[123]\ttrain-auc:0.89526\ttest-auc:0.60427\n",
      "[124]\ttrain-auc:0.89565\ttest-auc:0.60696\n",
      "[125]\ttrain-auc:0.89585\ttest-auc:0.60460\n",
      "[126]\ttrain-auc:0.89650\ttest-auc:0.60378\n",
      "[127]\ttrain-auc:0.89674\ttest-auc:0.60399\n",
      "[128]\ttrain-auc:0.89746\ttest-auc:0.60416\n",
      "[129]\ttrain-auc:0.89755\ttest-auc:0.60229\n",
      "[130]\ttrain-auc:0.89823\ttest-auc:0.59665\n",
      "[131]\ttrain-auc:0.89880\ttest-auc:0.60000\n",
      "[132]\ttrain-auc:0.89909\ttest-auc:0.59978\n",
      "[133]\ttrain-auc:0.89931\ttest-auc:0.59947\n",
      "[134]\ttrain-auc:0.89975\ttest-auc:0.59868\n",
      "[135]\ttrain-auc:0.90025\ttest-auc:0.59804\n",
      "[136]\ttrain-auc:0.90047\ttest-auc:0.59804\n",
      "[137]\ttrain-auc:0.90086\ttest-auc:0.59793\n",
      "[138]\ttrain-auc:0.90122\ttest-auc:0.59800\n",
      "[139]\ttrain-auc:0.90150\ttest-auc:0.59832\n",
      "[140]\ttrain-auc:0.90170\ttest-auc:0.59497\n",
      "[141]\ttrain-auc:0.90201\ttest-auc:0.59508\n",
      "[142]\ttrain-auc:0.90248\ttest-auc:0.59475\n",
      "[143]\ttrain-auc:0.90305\ttest-auc:0.59531\n",
      "[144]\ttrain-auc:0.90331\ttest-auc:0.59545\n",
      "[145]\ttrain-auc:0.90414\ttest-auc:0.59553\n",
      "[146]\ttrain-auc:0.90473\ttest-auc:0.59514\n",
      "[147]\ttrain-auc:0.90522\ttest-auc:0.59531\n",
      "[148]\ttrain-auc:0.90555\ttest-auc:0.59288\n",
      "[149]\ttrain-auc:0.90591\ttest-auc:0.58469\n",
      "[150]\ttrain-auc:0.90639\ttest-auc:0.58412\n",
      "[151]\ttrain-auc:0.90685\ttest-auc:0.58376\n",
      "[152]\ttrain-auc:0.90702\ttest-auc:0.58335\n",
      "[153]\ttrain-auc:0.90750\ttest-auc:0.58321\n",
      "[154]\ttrain-auc:0.90805\ttest-auc:0.58315\n",
      "[155]\ttrain-auc:0.90836\ttest-auc:0.58309\n",
      "Accuracy: 53.46%\n"
     ]
    }
   ],
   "source": [
    "learning_rate = result.x[0] \n",
    "gamma = result.x[1] \n",
    "max_depth = result.x[2]\n",
    "min_child_weight = result.x[3]\n",
    "max_delta_step = result.x[4]\n",
    "subsample = result.x[5]\n",
    "colsample_bytree = result.x[6]\n",
    "rounds = result.x[7]\n",
    "\n",
    "params1 = {\"silent\":1,\n",
    "          \"nthread\":-1,\n",
    "          \"seed\":1,\n",
    "          \"booster\":\"gbtree\",\n",
    "          \"lambda\":1,\n",
    "          \"alpha\":0,\n",
    "          \"learning_rate\":learning_rate,\n",
    "          \"gamma\":gamma,\n",
    "          \"max_depth\":max_depth,\n",
    "          \"min_child_weight\":min_child_weight,\n",
    "          \"max_delta_step\":max_delta_step,\n",
    "          \"subsample\":subsample,\n",
    "          \"colsample_bytree\":colsample_bytree,\n",
    "          \"colsample_bylevel\":1,\n",
    "          \"scale_pos_weight\":1,\n",
    "          \"process_type\":\"default\",\n",
    "          \"tree_method\":\"auto\",\n",
    "          \"objective\":\"binary:logistic\",\n",
    "          \"eval_metric\":'auc'}\n",
    "\n",
    "mdl = xgb.train(params1, dtrain, rounds,  [(dtrain,'train'),(dtest,'test')])\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = mdl.predict(dtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_onehot_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
