{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Build a feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "from modules.utils import *\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPool1D, Flatten, Input, concatenate, Dropout, Activation, regularizers, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "print('TF version:',tf.__version__)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.metrics import classification_report,accuracy_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brizio\\Documents\\PythonNB\\FICOchallenge\n",
      "./results\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "os.chdir(cwd)\n",
    "\n",
    "RESULT_PATH = './results'\n",
    "if not os.path.exists(RESULT_PATH):\n",
    "        os.mkdir(RESULT_PATH)\n",
    "print(RESULT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prepped data\n",
    "- Normal Scaling of numeric variables: f_load_data(1)\n",
    "- Binning (following Rudin) and one hot encoding: f_load_data(2)\n",
    "- Binning and applying WOE, calculating WOE on Rudin's bins: f_load_data(3)\n",
    "- Binning and applying WOE, following Rudin: f_load_data(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Scaling of numeric variables (prep_option = 1)\n",
      "Target: Bad (y=1)\n",
      "Bad     5459\n",
      "Good    5000\n",
      "Name: RiskPerformance, dtype: int64\n",
      "[[   0 5000]\n",
      " [   1 5459]]\n",
      "X shape: (10459, 23)\n",
      "(7844, 23) (2615, 23) (7844, 1) (2615, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExternalRiskEstimate</th>\n",
       "      <th>MSinceOldestTradeOpen</th>\n",
       "      <th>MSinceMostRecentTradeOpen</th>\n",
       "      <th>AverageMInFile</th>\n",
       "      <th>NumSatisfactoryTrades</th>\n",
       "      <th>NumTrades60Ever2DerogPubRec</th>\n",
       "      <th>NumTrades90Ever2DerogPubRec</th>\n",
       "      <th>PercentTradesNeverDelq</th>\n",
       "      <th>MSinceMostRecentDelq</th>\n",
       "      <th>MaxDelq2PublicRecLast12M</th>\n",
       "      <th>...</th>\n",
       "      <th>PercentInstallTrades</th>\n",
       "      <th>MSinceMostRecentInqexcl7days</th>\n",
       "      <th>NumInqLast6M</th>\n",
       "      <th>NumInqLast6Mexcl7days</th>\n",
       "      <th>NetFractionRevolvingBurden</th>\n",
       "      <th>NetFractionInstallBurden</th>\n",
       "      <th>NumRevolvingTradesWBalance</th>\n",
       "      <th>NumInstallTradesWBalance</th>\n",
       "      <th>NumBank2NatlTradesWHighUtilization</th>\n",
       "      <th>PercentTradesWBalance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.588324</td>\n",
       "      <td>-0.366575</td>\n",
       "      <td>-0.341585</td>\n",
       "      <td>0.261899</td>\n",
       "      <td>0.043983</td>\n",
       "      <td>1.176415</td>\n",
       "      <td>0.060341</td>\n",
       "      <td>-0.140837</td>\n",
       "      <td>-0.232309</td>\n",
       "      <td>-0.513377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538241</td>\n",
       "      <td>0.053626</td>\n",
       "      <td>-0.273077</td>\n",
       "      <td>-0.258498</td>\n",
       "      <td>0.045581</td>\n",
       "      <td>-1.120175</td>\n",
       "      <td>1.091102</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>0.292417</td>\n",
       "      <td>0.249757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.304241</td>\n",
       "      <td>-1.150684</td>\n",
       "      <td>0.485414</td>\n",
       "      <td>-0.846892</td>\n",
       "      <td>-1.340237</td>\n",
       "      <td>1.574221</td>\n",
       "      <td>1.750041</td>\n",
       "      <td>0.513051</td>\n",
       "      <td>-0.671328</td>\n",
       "      <td>-1.312078</td>\n",
       "      <td>...</td>\n",
       "      <td>1.730629</td>\n",
       "      <td>0.053626</td>\n",
       "      <td>-0.273077</td>\n",
       "      <td>-0.258498</td>\n",
       "      <td>-1.052271</td>\n",
       "      <td>-1.120175</td>\n",
       "      <td>-0.721739</td>\n",
       "      <td>-2.210425</td>\n",
       "      <td>-2.387770</td>\n",
       "      <td>-2.240297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.020158</td>\n",
       "      <td>-1.077744</td>\n",
       "      <td>-0.266403</td>\n",
       "      <td>-1.285252</td>\n",
       "      <td>-0.801929</td>\n",
       "      <td>-0.017002</td>\n",
       "      <td>0.060341</td>\n",
       "      <td>0.513051</td>\n",
       "      <td>-0.671328</td>\n",
       "      <td>0.551559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587924</td>\n",
       "      <td>0.053626</td>\n",
       "      <td>0.985121</td>\n",
       "      <td>1.013949</td>\n",
       "      <td>0.710946</td>\n",
       "      <td>0.637562</td>\n",
       "      <td>0.184681</td>\n",
       "      <td>0.252143</td>\n",
       "      <td>0.292417</td>\n",
       "      <td>0.863249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.067506</td>\n",
       "      <td>-0.138636</td>\n",
       "      <td>-0.567130</td>\n",
       "      <td>-0.021745</td>\n",
       "      <td>0.659193</td>\n",
       "      <td>0.380804</td>\n",
       "      <td>0.482766</td>\n",
       "      <td>0.243803</td>\n",
       "      <td>3.377399</td>\n",
       "      <td>0.285325</td>\n",
       "      <td>...</td>\n",
       "      <td>1.233801</td>\n",
       "      <td>0.053626</td>\n",
       "      <td>1.299670</td>\n",
       "      <td>1.013949</td>\n",
       "      <td>1.343042</td>\n",
       "      <td>1.041366</td>\n",
       "      <td>0.637892</td>\n",
       "      <td>0.744657</td>\n",
       "      <td>0.888014</td>\n",
       "      <td>1.043688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.642701</td>\n",
       "      <td>1.356643</td>\n",
       "      <td>1.387594</td>\n",
       "      <td>1.499621</td>\n",
       "      <td>-0.571226</td>\n",
       "      <td>-0.017002</td>\n",
       "      <td>0.060341</td>\n",
       "      <td>0.513051</td>\n",
       "      <td>-0.671328</td>\n",
       "      <td>0.551559</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.356050</td>\n",
       "      <td>0.053626</td>\n",
       "      <td>0.041473</td>\n",
       "      <td>0.059614</td>\n",
       "      <td>0.644409</td>\n",
       "      <td>1.183886</td>\n",
       "      <td>-0.041924</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>-0.005381</td>\n",
       "      <td>0.646722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ExternalRiskEstimate  MSinceOldestTradeOpen  MSinceMostRecentTradeOpen  \\\n",
       "0             -0.588324              -0.366575                  -0.341585   \n",
       "1             -0.304241              -1.150684                   0.485414   \n",
       "2             -0.020158              -1.077744                  -0.266403   \n",
       "3             -0.067506              -0.138636                  -0.567130   \n",
       "4              0.642701               1.356643                   1.387594   \n",
       "\n",
       "   AverageMInFile  NumSatisfactoryTrades  NumTrades60Ever2DerogPubRec  \\\n",
       "0        0.261899               0.043983                     1.176415   \n",
       "1       -0.846892              -1.340237                     1.574221   \n",
       "2       -1.285252              -0.801929                    -0.017002   \n",
       "3       -0.021745               0.659193                     0.380804   \n",
       "4        1.499621              -0.571226                    -0.017002   \n",
       "\n",
       "   NumTrades90Ever2DerogPubRec  PercentTradesNeverDelq  MSinceMostRecentDelq  \\\n",
       "0                     0.060341               -0.140837             -0.232309   \n",
       "1                     1.750041                0.513051             -0.671328   \n",
       "2                     0.060341                0.513051             -0.671328   \n",
       "3                     0.482766                0.243803              3.377399   \n",
       "4                     0.060341                0.513051             -0.671328   \n",
       "\n",
       "   MaxDelq2PublicRecLast12M  ...  PercentInstallTrades  \\\n",
       "0                 -0.513377  ...              0.538241   \n",
       "1                 -1.312078  ...              1.730629   \n",
       "2                  0.551559  ...              0.587924   \n",
       "3                  0.285325  ...              1.233801   \n",
       "4                  0.551559  ...             -0.356050   \n",
       "\n",
       "   MSinceMostRecentInqexcl7days  NumInqLast6M  NumInqLast6Mexcl7days  \\\n",
       "0                      0.053626     -0.273077              -0.258498   \n",
       "1                      0.053626     -0.273077              -0.258498   \n",
       "2                      0.053626      0.985121               1.013949   \n",
       "3                      0.053626      1.299670               1.013949   \n",
       "4                      0.053626      0.041473               0.059614   \n",
       "\n",
       "   NetFractionRevolvingBurden  NetFractionInstallBurden  \\\n",
       "0                    0.045581                 -1.120175   \n",
       "1                   -1.052271                 -1.120175   \n",
       "2                    0.710946                  0.637562   \n",
       "3                    1.343042                  1.041366   \n",
       "4                    0.644409                  1.183886   \n",
       "\n",
       "   NumRevolvingTradesWBalance  NumInstallTradesWBalance  \\\n",
       "0                    1.091102                  0.005886   \n",
       "1                   -0.721739                 -2.210425   \n",
       "2                    0.184681                  0.252143   \n",
       "3                    0.637892                  0.744657   \n",
       "4                   -0.041924                  0.005886   \n",
       "\n",
       "   NumBank2NatlTradesWHighUtilization  PercentTradesWBalance  \n",
       "0                            0.292417               0.249757  \n",
       "1                           -2.387770              -2.240297  \n",
       "2                            0.292417               0.863249  \n",
       "3                            0.888014               1.043688  \n",
       "4                           -0.005381               0.646722  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, X1, y, y_onehot = f_load_data(1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y_onehot, test_size = .25, random_state = 2020, shuffle = True)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "X1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and/or Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "\n",
    "def get_FFNN_model(X, y, hidden_layers_size = [4]):\n",
    "    \"\"\"\n",
    "        BASIC MODEL for the FF-NN\n",
    "    \"\"\"\n",
    "    input_size = len(X.columns.values)\n",
    "    output_size = len(y.columns.values)\n",
    "    \n",
    "    if len(hidden_layers_size) == 0:\n",
    "        # No hidden layer (linear regression equivalent)\n",
    "        ff_layers = [ Dense(output_size, input_shape=(input_size,), activation='softmax') ]\n",
    "    else:\n",
    "        ff_layers = [\n",
    "            tf.keras.layers.Dense(hidden_layers_size[0], input_shape=(input_size,), activation=ACTIVATION,\n",
    "                  kernel_initializer='he_uniform'\n",
    "#                   bias_initializer = 'zeros',\n",
    "                ## specific layer regularizer\n",
    "#                   kernel_regularizer=regularizers.l1(0.001)\n",
    "#                   ,activity_regularizer=regularizers.l2(0.001)\n",
    "                 ),\n",
    "            ## batch normalisation\n",
    "#             tf.keras.layers.BatchNormalization(),  # <- Batch normalisation layer\n",
    "            ## dropout\n",
    "#             Dropout(0.3),\n",
    "            ## elastic regularization\n",
    "#             Dense(hidden_layers_size[0], input_shape=(input_size,), activation='sigmoid', activity_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)),\n",
    "            tf.keras.layers.Dense(output_size, activation='sigmoid') \n",
    "        ]\n",
    "          \n",
    "            \n",
    "    print(ff_layers)\n",
    "    model = tf.keras.models.Sequential(ff_layers)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=L_RATE),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy','AUC',recall_m,precision_m,f1_m])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Model Training\n",
    "\n",
    "def net_train(model, bestmodel_path, X_train, y_train_onehot, X_validate, y_validate_onehot, epochs, save_model, VERBOSE):\n",
    "    # Define four callbacks to use\n",
    "    checkpointer = ModelCheckpoint(filepath = bestmodel_path, verbose = VERBOSE, save_best_only = True, save_weights_only=False)\n",
    "    early_stopping = EarlyStopping(monitor = 'val_acc', mode = 'max', patience = PATIENCE)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 20, verbose = VERBOSE)\n",
    "\n",
    "    # Train the model\n",
    "    if save_model:\n",
    "        history = model.fit(X_train, y_train_onehot, verbose = VERBOSE, epochs=epochs, batch_size=BATCH_SIZE, \n",
    "                            callbacks=[\n",
    "                                checkpointer,\n",
    "                                early_stopping, \n",
    "                                learning_rate_reduction\n",
    "                            ], \n",
    "    #                         validation_data=(X_validate, y_validate_onehot)\n",
    "                            validation_split = 0.2\n",
    "                           )\n",
    "    else:\n",
    "        history = model.fit(X_train, y_train_onehot, verbose = VERBOSE, epochs=epochs, batch_size=BATCH_SIZE, \n",
    "                            callbacks=[\n",
    "                                early_stopping, \n",
    "                                learning_rate_reduction\n",
    "                            ], \n",
    "                            validation_split = 0.2\n",
    "                           )        \n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.layers.core.Dense object at 0x000001838638C408>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838C08F488>]\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6694 samples, validate on 1674 samples\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56627, saving model to ./results\\FF_ori_ep500_pa500_bs8_hs[10]_lr_0.001_relu\n",
      "6694/6694 - 3s - loss: 0.6168 - acc: 0.6612 - auc_32: 0.7215 - recall_m: 0.6987 - precision_m: 0.6735 - f1_m: 0.6580 - val_loss: 0.5663 - val_acc: 0.7168 - val_auc_32: 0.7855 - val_recall_m: 0.7688 - val_precision_m: 0.6718 - val_f1_m: 0.6923\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56627 to 0.54783, saving model to ./results\\FF_ori_ep500_pa500_bs8_hs[10]_lr_0.001_relu\n",
      "6694/6694 - 1s - loss: 0.5623 - acc: 0.7217 - auc_32: 0.7818 - recall_m: 0.7569 - precision_m: 0.7195 - f1_m: 0.7153 - val_loss: 0.5478 - val_acc: 0.7401 - val_auc_32: 0.8023 - val_recall_m: 0.7777 - val_precision_m: 0.7046 - val_f1_m: 0.7154\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54783\n",
      "6694/6694 - 2s - loss: 0.5563 - acc: 0.7227 - auc_32: 0.7874 - recall_m: 0.7604 - precision_m: 0.7215 - f1_m: 0.7170 - val_loss: 0.5483 - val_acc: 0.7306 - val_auc_32: 0.8026 - val_recall_m: 0.7981 - val_precision_m: 0.6996 - val_f1_m: 0.7167\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54783 to 0.54769, saving model to ./results\\FF_ori_ep500_pa500_bs8_hs[10]_lr_0.001_relu\n",
      "6694/6694 - 2s - loss: 0.5533 - acc: 0.7235 - auc_32: 0.7904 - recall_m: 0.7582 - precision_m: 0.7282 - f1_m: 0.7187 - val_loss: 0.5477 - val_acc: 0.7330 - val_auc_32: 0.8007 - val_recall_m: 0.7651 - val_precision_m: 0.6995 - val_f1_m: 0.7111\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54769 to 0.54258, saving model to ./results\\FF_ori_ep500_pa500_bs8_hs[10]_lr_0.001_relu\n",
      "6694/6694 - 1s - loss: 0.5516 - acc: 0.7263 - auc_32: 0.7916 - recall_m: 0.7659 - precision_m: 0.7255 - f1_m: 0.7223 - val_loss: 0.5426 - val_acc: 0.7354 - val_auc_32: 0.8056 - val_recall_m: 0.7823 - val_precision_m: 0.7027 - val_f1_m: 0.7154\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.54258 to 0.54056, saving model to ./results\\FF_ori_ep500_pa500_bs8_hs[10]_lr_0.001_relu\n",
      "6694/6694 - 1s - loss: 0.5505 - acc: 0.7253 - auc_32: 0.7924 - recall_m: 0.7692 - precision_m: 0.7270 - f1_m: 0.7246 - val_loss: 0.5406 - val_acc: 0.7360 - val_auc_32: 0.8068 - val_recall_m: 0.7848 - val_precision_m: 0.7070 - val_f1_m: 0.7242\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.54056\n",
      "6694/6694 - 1s - loss: 0.5489 - acc: 0.7248 - auc_32: 0.7944 - recall_m: 0.7623 - precision_m: 0.7260 - f1_m: 0.7180 - val_loss: 0.5500 - val_acc: 0.7288 - val_auc_32: 0.7992 - val_recall_m: 0.7935 - val_precision_m: 0.7117 - val_f1_m: 0.7223\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.54056\n",
      "6694/6694 - 2s - loss: 0.5481 - acc: 0.7289 - auc_32: 0.7946 - recall_m: 0.7776 - precision_m: 0.7330 - f1_m: 0.7290 - val_loss: 0.5427 - val_acc: 0.7312 - val_auc_32: 0.8055 - val_recall_m: 0.7838 - val_precision_m: 0.6991 - val_f1_m: 0.7159\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.54056\n",
      "6694/6694 - 2s - loss: 0.5475 - acc: 0.7283 - auc_32: 0.7955 - recall_m: 0.7647 - precision_m: 0.7261 - f1_m: 0.7227 - val_loss: 0.5443 - val_acc: 0.7461 - val_auc_32: 0.8082 - val_recall_m: 0.7924 - val_precision_m: 0.7064 - val_f1_m: 0.7260\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.54056\n",
      "6694/6694 - 1s - loss: 0.5460 - acc: 0.7289 - auc_32: 0.7964 - recall_m: 0.7692 - precision_m: 0.7290 - f1_m: 0.7238 - val_loss: 0.5451 - val_acc: 0.7318 - val_auc_32: 0.8057 - val_recall_m: 0.7887 - val_precision_m: 0.6820 - val_f1_m: 0.7119\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.54056 to 0.53803, saving model to ./results\\FF_ori_ep500_pa500_bs8_hs[10]_lr_0.001_relu\n",
      "6694/6694 - 2s - loss: 0.5459 - acc: 0.7295 - auc_32: 0.7966 - recall_m: 0.7790 - precision_m: 0.7332 - f1_m: 0.7311 - val_loss: 0.5380 - val_acc: 0.7473 - val_auc_32: 0.8085 - val_recall_m: 0.7777 - val_precision_m: 0.7251 - val_f1_m: 0.7273\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5460 - acc: 0.7290 - auc_32: 0.7966 - recall_m: 0.7794 - precision_m: 0.7360 - f1_m: 0.7306 - val_loss: 0.5530 - val_acc: 0.7294 - val_auc_32: 0.7954 - val_recall_m: 0.7874 - val_precision_m: 0.7013 - val_f1_m: 0.7158\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.53803\n",
      "6694/6694 - 2s - loss: 0.5455 - acc: 0.7275 - auc_32: 0.7971 - recall_m: 0.7714 - precision_m: 0.7312 - f1_m: 0.7257 - val_loss: 0.5425 - val_acc: 0.7306 - val_auc_32: 0.8059 - val_recall_m: 0.7944 - val_precision_m: 0.6969 - val_f1_m: 0.7167\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5445 - acc: 0.7278 - auc_32: 0.7979 - recall_m: 0.7691 - precision_m: 0.7313 - f1_m: 0.7237 - val_loss: 0.5419 - val_acc: 0.7330 - val_auc_32: 0.8097 - val_recall_m: 0.8087 - val_precision_m: 0.6905 - val_f1_m: 0.7156\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.53803\n",
      "6694/6694 - 2s - loss: 0.5437 - acc: 0.7283 - auc_32: 0.7985 - recall_m: 0.7619 - precision_m: 0.7223 - f1_m: 0.7188 - val_loss: 0.5452 - val_acc: 0.7312 - val_auc_32: 0.8042 - val_recall_m: 0.7948 - val_precision_m: 0.6970 - val_f1_m: 0.7179\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.53803\n",
      "6694/6694 - 2s - loss: 0.5435 - acc: 0.7311 - auc_32: 0.7988 - recall_m: 0.7789 - precision_m: 0.7257 - f1_m: 0.7276 - val_loss: 0.5402 - val_acc: 0.7348 - val_auc_32: 0.8089 - val_recall_m: 0.8066 - val_precision_m: 0.7009 - val_f1_m: 0.7290\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5430 - acc: 0.7302 - auc_32: 0.7991 - recall_m: 0.7726 - precision_m: 0.7267 - f1_m: 0.7263 - val_loss: 0.5415 - val_acc: 0.7306 - val_auc_32: 0.8073 - val_recall_m: 0.8027 - val_precision_m: 0.6991 - val_f1_m: 0.7247\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5421 - acc: 0.7304 - auc_32: 0.8002 - recall_m: 0.7771 - precision_m: 0.7305 - f1_m: 0.7290 - val_loss: 0.5395 - val_acc: 0.7336 - val_auc_32: 0.8084 - val_recall_m: 0.7853 - val_precision_m: 0.6852 - val_f1_m: 0.7091\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5423 - acc: 0.7296 - auc_32: 0.8002 - recall_m: 0.7691 - precision_m: 0.7228 - f1_m: 0.7220 - val_loss: 0.5417 - val_acc: 0.7306 - val_auc_32: 0.8085 - val_recall_m: 0.7932 - val_precision_m: 0.6917 - val_f1_m: 0.7147\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5415 - acc: 0.7317 - auc_32: 0.8005 - recall_m: 0.7781 - precision_m: 0.7237 - f1_m: 0.7279 - val_loss: 0.5446 - val_acc: 0.7318 - val_auc_32: 0.8052 - val_recall_m: 0.7901 - val_precision_m: 0.6965 - val_f1_m: 0.7122\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5412 - acc: 0.7295 - auc_32: 0.8009 - recall_m: 0.7756 - precision_m: 0.7294 - f1_m: 0.7284 - val_loss: 0.5442 - val_acc: 0.7306 - val_auc_32: 0.8057 - val_recall_m: 0.7898 - val_precision_m: 0.6873 - val_f1_m: 0.7135\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5399 - acc: 0.7289 - auc_32: 0.8022 - recall_m: 0.7754 - precision_m: 0.7340 - f1_m: 0.7303 - val_loss: 0.5380 - val_acc: 0.7312 - val_auc_32: 0.8081 - val_recall_m: 0.7970 - val_precision_m: 0.7018 - val_f1_m: 0.7132\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5401 - acc: 0.7318 - auc_32: 0.8019 - recall_m: 0.7701 - precision_m: 0.7238 - f1_m: 0.7235 - val_loss: 0.5440 - val_acc: 0.7318 - val_auc_32: 0.8024 - val_recall_m: 0.7872 - val_precision_m: 0.7100 - val_f1_m: 0.7228\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5399 - acc: 0.7299 - auc_32: 0.8018 - recall_m: 0.7736 - precision_m: 0.7310 - f1_m: 0.7272 - val_loss: 0.5406 - val_acc: 0.7246 - val_auc_32: 0.8089 - val_recall_m: 0.7905 - val_precision_m: 0.6958 - val_f1_m: 0.7109\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5401 - acc: 0.7259 - auc_32: 0.8018 - recall_m: 0.7719 - precision_m: 0.7272 - f1_m: 0.7233 - val_loss: 0.5428 - val_acc: 0.7288 - val_auc_32: 0.8049 - val_recall_m: 0.7727 - val_precision_m: 0.6999 - val_f1_m: 0.7080\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5383 - acc: 0.7326 - auc_32: 0.8036 - recall_m: 0.7730 - precision_m: 0.7374 - f1_m: 0.7309 - val_loss: 0.5414 - val_acc: 0.7372 - val_auc_32: 0.8094 - val_recall_m: 0.7872 - val_precision_m: 0.6983 - val_f1_m: 0.7160\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.53803\n",
      "6694/6694 - 1s - loss: 0.5389 - acc: 0.7317 - auc_32: 0.8029 - recall_m: 0.7760 - precision_m: 0.7283 - f1_m: 0.7267 - val_loss: 0.5431 - val_acc: 0.7288 - val_auc_32: 0.8034 - val_recall_m: 0.7738 - val_precision_m: 0.6952 - val_f1_m: 0.7108\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.53803 to 0.53742, saving model to ./results\\FF_ori_ep500_pa500_bs8_hs[10]_lr_0.001_relu\n",
      "6694/6694 - 1s - loss: 0.5386 - acc: 0.7304 - auc_32: 0.8033 - recall_m: 0.7727 - precision_m: 0.7280 - f1_m: 0.7251 - val_loss: 0.5374 - val_acc: 0.7378 - val_auc_32: 0.8095 - val_recall_m: 0.7616 - val_precision_m: 0.7076 - val_f1_m: 0.7097\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5378 - acc: 0.7311 - auc_32: 0.8041 - recall_m: 0.7705 - precision_m: 0.7304 - f1_m: 0.7262 - val_loss: 0.5404 - val_acc: 0.7336 - val_auc_32: 0.8092 - val_recall_m: 0.7629 - val_precision_m: 0.6868 - val_f1_m: 0.7008\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5375 - acc: 0.7330 - auc_32: 0.8042 - recall_m: 0.7778 - precision_m: 0.7324 - f1_m: 0.7317 - val_loss: 0.5480 - val_acc: 0.7264 - val_auc_32: 0.8026 - val_recall_m: 0.7978 - val_precision_m: 0.7000 - val_f1_m: 0.7198\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5370 - acc: 0.7295 - auc_32: 0.8047 - recall_m: 0.7741 - precision_m: 0.7294 - f1_m: 0.7270 - val_loss: 0.5451 - val_acc: 0.7234 - val_auc_32: 0.8055 - val_recall_m: 0.7868 - val_precision_m: 0.6739 - val_f1_m: 0.7009\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5375 - acc: 0.7323 - auc_32: 0.8039 - recall_m: 0.7841 - precision_m: 0.7308 - f1_m: 0.7323 - val_loss: 0.5410 - val_acc: 0.7240 - val_auc_32: 0.8083 - val_recall_m: 0.7851 - val_precision_m: 0.6902 - val_f1_m: 0.7091\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5370 - acc: 0.7313 - auc_32: 0.8043 - recall_m: 0.7787 - precision_m: 0.7256 - f1_m: 0.7274 - val_loss: 0.5391 - val_acc: 0.7246 - val_auc_32: 0.8081 - val_recall_m: 0.7870 - val_precision_m: 0.6916 - val_f1_m: 0.7120\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5355 - acc: 0.7345 - auc_32: 0.8059 - recall_m: 0.7798 - precision_m: 0.7365 - f1_m: 0.7333 - val_loss: 0.5440 - val_acc: 0.7240 - val_auc_32: 0.8072 - val_recall_m: 0.7984 - val_precision_m: 0.6731 - val_f1_m: 0.7079\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5358 - acc: 0.7317 - auc_32: 0.8056 - recall_m: 0.7811 - precision_m: 0.7321 - f1_m: 0.7310 - val_loss: 0.5489 - val_acc: 0.7222 - val_auc_32: 0.7999 - val_recall_m: 0.7886 - val_precision_m: 0.6886 - val_f1_m: 0.7088\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5359 - acc: 0.7302 - auc_32: 0.8055 - recall_m: 0.7738 - precision_m: 0.7317 - f1_m: 0.7275 - val_loss: 0.5483 - val_acc: 0.7228 - val_auc_32: 0.8070 - val_recall_m: 0.8217 - val_precision_m: 0.6808 - val_f1_m: 0.7197\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5349 - acc: 0.7354 - auc_32: 0.8063 - recall_m: 0.7812 - precision_m: 0.7333 - f1_m: 0.7321 - val_loss: 0.5446 - val_acc: 0.7264 - val_auc_32: 0.8075 - val_recall_m: 0.7974 - val_precision_m: 0.6843 - val_f1_m: 0.7114\n",
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5356 - acc: 0.7307 - auc_32: 0.8056 - recall_m: 0.7745 - precision_m: 0.7263 - f1_m: 0.7272 - val_loss: 0.5430 - val_acc: 0.7240 - val_auc_32: 0.8062 - val_recall_m: 0.7973 - val_precision_m: 0.6909 - val_f1_m: 0.7173\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5349 - acc: 0.7298 - auc_32: 0.8063 - recall_m: 0.7781 - precision_m: 0.7291 - f1_m: 0.7293 - val_loss: 0.5434 - val_acc: 0.7258 - val_auc_32: 0.8063 - val_recall_m: 0.8065 - val_precision_m: 0.6928 - val_f1_m: 0.7209\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5343 - acc: 0.7342 - auc_32: 0.8070 - recall_m: 0.7867 - precision_m: 0.7320 - f1_m: 0.7342 - val_loss: 0.5439 - val_acc: 0.7258 - val_auc_32: 0.8069 - val_recall_m: 0.8034 - val_precision_m: 0.6923 - val_f1_m: 0.7194\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5347 - acc: 0.7327 - auc_32: 0.8062 - recall_m: 0.7810 - precision_m: 0.7292 - f1_m: 0.7298 - val_loss: 0.5439 - val_acc: 0.7252 - val_auc_32: 0.8073 - val_recall_m: 0.7866 - val_precision_m: 0.6719 - val_f1_m: 0.7028\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5346 - acc: 0.7286 - auc_32: 0.8065 - recall_m: 0.7735 - precision_m: 0.7254 - f1_m: 0.7282 - val_loss: 0.5458 - val_acc: 0.7222 - val_auc_32: 0.8058 - val_recall_m: 0.7876 - val_precision_m: 0.6770 - val_f1_m: 0.7077\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5340 - acc: 0.7338 - auc_32: 0.8069 - recall_m: 0.7856 - precision_m: 0.7302 - f1_m: 0.7351 - val_loss: 0.5434 - val_acc: 0.7234 - val_auc_32: 0.8062 - val_recall_m: 0.7949 - val_precision_m: 0.6868 - val_f1_m: 0.7134\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5333 - acc: 0.7316 - auc_32: 0.8078 - recall_m: 0.7771 - precision_m: 0.7272 - f1_m: 0.7276 - val_loss: 0.5395 - val_acc: 0.7258 - val_auc_32: 0.8075 - val_recall_m: 0.7692 - val_precision_m: 0.6871 - val_f1_m: 0.7032\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5338 - acc: 0.7307 - auc_32: 0.8071 - recall_m: 0.7686 - precision_m: 0.7319 - f1_m: 0.7260 - val_loss: 0.5443 - val_acc: 0.7240 - val_auc_32: 0.8077 - val_recall_m: 0.7912 - val_precision_m: 0.6784 - val_f1_m: 0.7076\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5332 - acc: 0.7335 - auc_32: 0.8075 - recall_m: 0.7807 - precision_m: 0.7282 - f1_m: 0.7280 - val_loss: 0.5457 - val_acc: 0.7264 - val_auc_32: 0.8075 - val_recall_m: 0.8073 - val_precision_m: 0.6807 - val_f1_m: 0.7198\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5337 - acc: 0.7292 - auc_32: 0.8071 - recall_m: 0.7800 - precision_m: 0.7265 - f1_m: 0.7283 - val_loss: 0.5416 - val_acc: 0.7234 - val_auc_32: 0.8050 - val_recall_m: 0.7878 - val_precision_m: 0.6983 - val_f1_m: 0.7149\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5335 - acc: 0.7344 - auc_32: 0.8074 - recall_m: 0.7882 - precision_m: 0.7374 - f1_m: 0.7372 - val_loss: 0.5460 - val_acc: 0.7234 - val_auc_32: 0.8043 - val_recall_m: 0.7945 - val_precision_m: 0.6880 - val_f1_m: 0.7084\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5328 - acc: 0.7330 - auc_32: 0.8079 - recall_m: 0.7804 - precision_m: 0.7313 - f1_m: 0.7300 - val_loss: 0.5460 - val_acc: 0.7240 - val_auc_32: 0.8049 - val_recall_m: 0.8032 - val_precision_m: 0.6763 - val_f1_m: 0.7121\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5326 - acc: 0.7359 - auc_32: 0.8083 - recall_m: 0.7798 - precision_m: 0.7355 - f1_m: 0.7333 - val_loss: 0.5466 - val_acc: 0.7228 - val_auc_32: 0.8051 - val_recall_m: 0.8139 - val_precision_m: 0.6849 - val_f1_m: 0.7183\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5325 - acc: 0.7369 - auc_32: 0.8084 - recall_m: 0.7801 - precision_m: 0.7358 - f1_m: 0.7353 - val_loss: 0.5416 - val_acc: 0.7372 - val_auc_32: 0.8083 - val_recall_m: 0.7827 - val_precision_m: 0.7086 - val_f1_m: 0.7171\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5328 - acc: 0.7320 - auc_32: 0.8082 - recall_m: 0.7806 - precision_m: 0.7361 - f1_m: 0.7316 - val_loss: 0.5444 - val_acc: 0.7222 - val_auc_32: 0.8048 - val_recall_m: 0.7882 - val_precision_m: 0.6937 - val_f1_m: 0.7110\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5321 - acc: 0.7344 - auc_32: 0.8087 - recall_m: 0.7834 - precision_m: 0.7379 - f1_m: 0.7357 - val_loss: 0.5483 - val_acc: 0.7186 - val_auc_32: 0.8026 - val_recall_m: 0.7844 - val_precision_m: 0.6757 - val_f1_m: 0.7043\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5321 - acc: 0.7347 - auc_32: 0.8086 - recall_m: 0.7871 - precision_m: 0.7311 - f1_m: 0.7346 - val_loss: 0.5426 - val_acc: 0.7240 - val_auc_32: 0.8061 - val_recall_m: 0.7859 - val_precision_m: 0.6876 - val_f1_m: 0.7097\n",
      "Epoch 55/500\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5321 - acc: 0.7338 - auc_32: 0.8088 - recall_m: 0.7785 - precision_m: 0.7382 - f1_m: 0.7339 - val_loss: 0.5416 - val_acc: 0.7246 - val_auc_32: 0.8052 - val_recall_m: 0.7617 - val_precision_m: 0.6881 - val_f1_m: 0.6992\n",
      "Epoch 56/500\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5323 - acc: 0.7318 - auc_32: 0.8081 - recall_m: 0.7825 - precision_m: 0.7395 - f1_m: 0.7334 - val_loss: 0.5418 - val_acc: 0.7264 - val_auc_32: 0.8060 - val_recall_m: 0.7766 - val_precision_m: 0.6878 - val_f1_m: 0.7069\n",
      "Epoch 57/500\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5319 - acc: 0.7333 - auc_32: 0.8090 - recall_m: 0.7805 - precision_m: 0.7314 - f1_m: 0.7326 - val_loss: 0.5412 - val_acc: 0.7354 - val_auc_32: 0.8061 - val_recall_m: 0.7767 - val_precision_m: 0.7145 - val_f1_m: 0.7207\n",
      "Epoch 58/500\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5323 - acc: 0.7353 - auc_32: 0.8085 - recall_m: 0.7819 - precision_m: 0.7384 - f1_m: 0.7356 - val_loss: 0.5430 - val_acc: 0.7354 - val_auc_32: 0.8067 - val_recall_m: 0.7801 - val_precision_m: 0.6964 - val_f1_m: 0.7084\n",
      "Epoch 59/500\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5324 - acc: 0.7330 - auc_32: 0.8083 - recall_m: 0.7829 - precision_m: 0.7271 - f1_m: 0.7308 - val_loss: 0.5468 - val_acc: 0.7258 - val_auc_32: 0.8015 - val_recall_m: 0.7865 - val_precision_m: 0.6936 - val_f1_m: 0.7086\n",
      "Epoch 60/500\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5321 - acc: 0.7341 - auc_32: 0.8086 - recall_m: 0.7793 - precision_m: 0.7367 - f1_m: 0.7338 - val_loss: 0.5449 - val_acc: 0.7234 - val_auc_32: 0.8046 - val_recall_m: 0.7839 - val_precision_m: 0.6790 - val_f1_m: 0.6994\n",
      "Epoch 61/500\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5317 - acc: 0.7333 - auc_32: 0.8089 - recall_m: 0.7846 - precision_m: 0.7315 - f1_m: 0.7317 - val_loss: 0.5463 - val_acc: 0.7240 - val_auc_32: 0.8052 - val_recall_m: 0.7949 - val_precision_m: 0.6925 - val_f1_m: 0.7144\n",
      "Epoch 62/500\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5318 - acc: 0.7335 - auc_32: 0.8087 - recall_m: 0.7890 - precision_m: 0.7334 - f1_m: 0.7334 - val_loss: 0.5452 - val_acc: 0.7252 - val_auc_32: 0.8055 - val_recall_m: 0.7902 - val_precision_m: 0.6831 - val_f1_m: 0.7092\n",
      "Epoch 63/500\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5310 - acc: 0.7321 - auc_32: 0.8095 - recall_m: 0.7737 - precision_m: 0.7376 - f1_m: 0.7293 - val_loss: 0.5643 - val_acc: 0.7240 - val_auc_32: 0.7887 - val_recall_m: 0.7855 - val_precision_m: 0.6937 - val_f1_m: 0.7133\n",
      "Epoch 64/500\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5312 - acc: 0.7359 - auc_32: 0.8094 - recall_m: 0.7829 - precision_m: 0.7317 - f1_m: 0.7316 - val_loss: 0.5448 - val_acc: 0.7276 - val_auc_32: 0.8057 - val_recall_m: 0.8044 - val_precision_m: 0.6983 - val_f1_m: 0.7204\n",
      "Epoch 65/500\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.53742\n",
      "6694/6694 - 3s - loss: 0.5313 - acc: 0.7336 - auc_32: 0.8094 - recall_m: 0.7804 - precision_m: 0.7358 - f1_m: 0.7330 - val_loss: 0.5569 - val_acc: 0.7210 - val_auc_32: 0.7998 - val_recall_m: 0.8032 - val_precision_m: 0.6767 - val_f1_m: 0.7095\n",
      "Epoch 66/500\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5317 - acc: 0.7362 - auc_32: 0.8090 - recall_m: 0.7762 - precision_m: 0.7363 - f1_m: 0.7313 - val_loss: 0.5462 - val_acc: 0.7240 - val_auc_32: 0.8062 - val_recall_m: 0.7992 - val_precision_m: 0.6777 - val_f1_m: 0.7102\n",
      "Epoch 67/500\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5311 - acc: 0.7318 - auc_32: 0.8091 - recall_m: 0.7769 - precision_m: 0.7274 - f1_m: 0.7280 - val_loss: 0.5507 - val_acc: 0.7222 - val_auc_32: 0.8034 - val_recall_m: 0.7982 - val_precision_m: 0.6702 - val_f1_m: 0.7075\n",
      "Epoch 68/500\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5320 - acc: 0.7323 - auc_32: 0.8087 - recall_m: 0.7829 - precision_m: 0.7388 - f1_m: 0.7342 - val_loss: 0.5475 - val_acc: 0.7234 - val_auc_32: 0.8062 - val_recall_m: 0.8083 - val_precision_m: 0.6852 - val_f1_m: 0.7162\n",
      "Epoch 69/500\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5313 - acc: 0.7332 - auc_32: 0.8093 - recall_m: 0.7828 - precision_m: 0.7383 - f1_m: 0.7345 - val_loss: 0.5449 - val_acc: 0.7246 - val_auc_32: 0.8026 - val_recall_m: 0.7807 - val_precision_m: 0.6946 - val_f1_m: 0.7098\n",
      "Epoch 70/500\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5314 - acc: 0.7324 - auc_32: 0.8092 - recall_m: 0.7770 - precision_m: 0.7311 - f1_m: 0.7293 - val_loss: 0.5398 - val_acc: 0.7366 - val_auc_32: 0.8077 - val_recall_m: 0.7622 - val_precision_m: 0.6901 - val_f1_m: 0.7038\n",
      "Epoch 71/500\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5318 - acc: 0.7339 - auc_32: 0.8087 - recall_m: 0.7661 - precision_m: 0.7340 - f1_m: 0.7257 - val_loss: 0.5508 - val_acc: 0.7252 - val_auc_32: 0.8007 - val_recall_m: 0.7878 - val_precision_m: 0.6792 - val_f1_m: 0.7040\n",
      "Epoch 72/500\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5306 - acc: 0.7327 - auc_32: 0.8100 - recall_m: 0.7854 - precision_m: 0.7296 - f1_m: 0.7299 - val_loss: 0.5432 - val_acc: 0.7252 - val_auc_32: 0.8036 - val_recall_m: 0.7598 - val_precision_m: 0.6924 - val_f1_m: 0.6995\n",
      "Epoch 73/500\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5315 - acc: 0.7332 - auc_32: 0.8090 - recall_m: 0.7747 - precision_m: 0.7311 - f1_m: 0.7282 - val_loss: 0.5529 - val_acc: 0.7252 - val_auc_32: 0.7992 - val_recall_m: 0.8005 - val_precision_m: 0.6846 - val_f1_m: 0.7143\n",
      "Epoch 74/500\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5308 - acc: 0.7342 - auc_32: 0.8098 - recall_m: 0.7819 - precision_m: 0.7349 - f1_m: 0.7344 - val_loss: 0.5443 - val_acc: 0.7294 - val_auc_32: 0.8052 - val_recall_m: 0.7812 - val_precision_m: 0.6850 - val_f1_m: 0.7027\n",
      "Epoch 75/500\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5303 - acc: 0.7336 - auc_32: 0.8099 - recall_m: 0.7835 - precision_m: 0.7335 - f1_m: 0.7350 - val_loss: 0.5506 - val_acc: 0.7198 - val_auc_32: 0.8026 - val_recall_m: 0.7996 - val_precision_m: 0.6745 - val_f1_m: 0.7045\n",
      "Epoch 76/500\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5314 - acc: 0.7308 - auc_32: 0.8087 - recall_m: 0.7794 - precision_m: 0.7269 - f1_m: 0.7282 - val_loss: 0.5420 - val_acc: 0.7360 - val_auc_32: 0.8062 - val_recall_m: 0.7772 - val_precision_m: 0.7082 - val_f1_m: 0.7160\n",
      "Epoch 77/500\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5309 - acc: 0.7307 - auc_32: 0.8094 - recall_m: 0.7707 - precision_m: 0.7320 - f1_m: 0.7250 - val_loss: 0.5523 - val_acc: 0.7246 - val_auc_32: 0.8004 - val_recall_m: 0.7977 - val_precision_m: 0.6915 - val_f1_m: 0.7130\n",
      "Epoch 78/500\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5307 - acc: 0.7360 - auc_32: 0.8099 - recall_m: 0.7869 - precision_m: 0.7377 - f1_m: 0.7333 - val_loss: 0.5435 - val_acc: 0.7222 - val_auc_32: 0.8037 - val_recall_m: 0.7721 - val_precision_m: 0.6896 - val_f1_m: 0.7035\n",
      "Epoch 79/500\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5303 - acc: 0.7329 - auc_32: 0.8101 - recall_m: 0.7778 - precision_m: 0.7381 - f1_m: 0.7310 - val_loss: 0.5435 - val_acc: 0.7324 - val_auc_32: 0.8069 - val_recall_m: 0.7670 - val_precision_m: 0.6962 - val_f1_m: 0.7056\n",
      "Epoch 80/500\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5302 - acc: 0.7333 - auc_32: 0.8105 - recall_m: 0.7747 - precision_m: 0.7319 - f1_m: 0.7278 - val_loss: 0.5413 - val_acc: 0.7407 - val_auc_32: 0.8087 - val_recall_m: 0.7869 - val_precision_m: 0.7084 - val_f1_m: 0.7174\n",
      "Epoch 81/500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5302 - acc: 0.7338 - auc_32: 0.8102 - recall_m: 0.7766 - precision_m: 0.7354 - f1_m: 0.7300 - val_loss: 0.5505 - val_acc: 0.7222 - val_auc_32: 0.8009 - val_recall_m: 0.7789 - val_precision_m: 0.6847 - val_f1_m: 0.7014\n",
      "Epoch 82/500\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5306 - acc: 0.7345 - auc_32: 0.8100 - recall_m: 0.7781 - precision_m: 0.7296 - f1_m: 0.7293 - val_loss: 0.5450 - val_acc: 0.7264 - val_auc_32: 0.8033 - val_recall_m: 0.7838 - val_precision_m: 0.6947 - val_f1_m: 0.7117\n",
      "Epoch 83/500\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5302 - acc: 0.7357 - auc_32: 0.8101 - recall_m: 0.7834 - precision_m: 0.7324 - f1_m: 0.7350 - val_loss: 0.5450 - val_acc: 0.7264 - val_auc_32: 0.8042 - val_recall_m: 0.7796 - val_precision_m: 0.6870 - val_f1_m: 0.7074\n",
      "Epoch 84/500\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5293 - acc: 0.7381 - auc_32: 0.8111 - recall_m: 0.7817 - precision_m: 0.7372 - f1_m: 0.7335 - val_loss: 0.5463 - val_acc: 0.7204 - val_auc_32: 0.8044 - val_recall_m: 0.7931 - val_precision_m: 0.6752 - val_f1_m: 0.7076\n",
      "Epoch 85/500\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5295 - acc: 0.7341 - auc_32: 0.8107 - recall_m: 0.7771 - precision_m: 0.7362 - f1_m: 0.7342 - val_loss: 0.5464 - val_acc: 0.7240 - val_auc_32: 0.8054 - val_recall_m: 0.7912 - val_precision_m: 0.6855 - val_f1_m: 0.7083\n",
      "Epoch 86/500\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5304 - acc: 0.7335 - auc_32: 0.8099 - recall_m: 0.7823 - precision_m: 0.7331 - f1_m: 0.7319 - val_loss: 0.5432 - val_acc: 0.7264 - val_auc_32: 0.8037 - val_recall_m: 0.7754 - val_precision_m: 0.6994 - val_f1_m: 0.7088\n",
      "Epoch 87/500\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5296 - acc: 0.7335 - auc_32: 0.8106 - recall_m: 0.7829 - precision_m: 0.7357 - f1_m: 0.7351 - val_loss: 0.5449 - val_acc: 0.7312 - val_auc_32: 0.8050 - val_recall_m: 0.7721 - val_precision_m: 0.6857 - val_f1_m: 0.6988\n",
      "Epoch 88/500\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5307 - acc: 0.7336 - auc_32: 0.8097 - recall_m: 0.7829 - precision_m: 0.7333 - f1_m: 0.7334 - val_loss: 0.5454 - val_acc: 0.7318 - val_auc_32: 0.8056 - val_recall_m: 0.7800 - val_precision_m: 0.7053 - val_f1_m: 0.7145\n",
      "Epoch 89/500\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5298 - acc: 0.7330 - auc_32: 0.8106 - recall_m: 0.7793 - precision_m: 0.7358 - f1_m: 0.7326 - val_loss: 0.5556 - val_acc: 0.7234 - val_auc_32: 0.7965 - val_recall_m: 0.7850 - val_precision_m: 0.6752 - val_f1_m: 0.7050\n",
      "Epoch 90/500\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5299 - acc: 0.7318 - auc_32: 0.8105 - recall_m: 0.7798 - precision_m: 0.7281 - f1_m: 0.7291 - val_loss: 0.5483 - val_acc: 0.7228 - val_auc_32: 0.8007 - val_recall_m: 0.7644 - val_precision_m: 0.6899 - val_f1_m: 0.7015\n",
      "Epoch 91/500\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5293 - acc: 0.7345 - auc_32: 0.8112 - recall_m: 0.7803 - precision_m: 0.7352 - f1_m: 0.7338 - val_loss: 0.5536 - val_acc: 0.7264 - val_auc_32: 0.7961 - val_recall_m: 0.7717 - val_precision_m: 0.6987 - val_f1_m: 0.7094\n",
      "Epoch 92/500\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5292 - acc: 0.7342 - auc_32: 0.8111 - recall_m: 0.7805 - precision_m: 0.7341 - f1_m: 0.7312 - val_loss: 0.5447 - val_acc: 0.7288 - val_auc_32: 0.8060 - val_recall_m: 0.7685 - val_precision_m: 0.7106 - val_f1_m: 0.7143\n",
      "Epoch 93/500\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5291 - acc: 0.7338 - auc_32: 0.8112 - recall_m: 0.7767 - precision_m: 0.7340 - f1_m: 0.7308 - val_loss: 0.5535 - val_acc: 0.7180 - val_auc_32: 0.8019 - val_recall_m: 0.7916 - val_precision_m: 0.6697 - val_f1_m: 0.6996\n",
      "Epoch 94/500\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5297 - acc: 0.7345 - auc_32: 0.8104 - recall_m: 0.7720 - precision_m: 0.7352 - f1_m: 0.7274 - val_loss: 0.5539 - val_acc: 0.7210 - val_auc_32: 0.7998 - val_recall_m: 0.7958 - val_precision_m: 0.6879 - val_f1_m: 0.7139\n",
      "Epoch 95/500\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5292 - acc: 0.7339 - auc_32: 0.8108 - recall_m: 0.7878 - precision_m: 0.7323 - f1_m: 0.7344 - val_loss: 0.5511 - val_acc: 0.7210 - val_auc_32: 0.8003 - val_recall_m: 0.7785 - val_precision_m: 0.6839 - val_f1_m: 0.7048\n",
      "Epoch 96/500\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5290 - acc: 0.7393 - auc_32: 0.8116 - recall_m: 0.7826 - precision_m: 0.7392 - f1_m: 0.7350 - val_loss: 0.5450 - val_acc: 0.7240 - val_auc_32: 0.8046 - val_recall_m: 0.7844 - val_precision_m: 0.6859 - val_f1_m: 0.7096\n",
      "Epoch 97/500\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5289 - acc: 0.7368 - auc_32: 0.8112 - recall_m: 0.7921 - precision_m: 0.7339 - f1_m: 0.7381 - val_loss: 0.5454 - val_acc: 0.7210 - val_auc_32: 0.8049 - val_recall_m: 0.7819 - val_precision_m: 0.6839 - val_f1_m: 0.7053\n",
      "Epoch 98/500\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5293 - acc: 0.7341 - auc_32: 0.8109 - recall_m: 0.7792 - precision_m: 0.7353 - f1_m: 0.7334 - val_loss: 0.5478 - val_acc: 0.7222 - val_auc_32: 0.8026 - val_recall_m: 0.7768 - val_precision_m: 0.6883 - val_f1_m: 0.7064\n",
      "Epoch 99/500\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5288 - acc: 0.7377 - auc_32: 0.8113 - recall_m: 0.7847 - precision_m: 0.7361 - f1_m: 0.7344 - val_loss: 0.5449 - val_acc: 0.7228 - val_auc_32: 0.8038 - val_recall_m: 0.7700 - val_precision_m: 0.6777 - val_f1_m: 0.6971\n",
      "Epoch 100/500\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5291 - acc: 0.7336 - auc_32: 0.8113 - recall_m: 0.7789 - precision_m: 0.7367 - f1_m: 0.7325 - val_loss: 0.5487 - val_acc: 0.7192 - val_auc_32: 0.8034 - val_recall_m: 0.7821 - val_precision_m: 0.6783 - val_f1_m: 0.6992\n",
      "Epoch 101/500\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5291 - acc: 0.7318 - auc_32: 0.8110 - recall_m: 0.7771 - precision_m: 0.7299 - f1_m: 0.7294 - val_loss: 0.5453 - val_acc: 0.7198 - val_auc_32: 0.8038 - val_recall_m: 0.7710 - val_precision_m: 0.6846 - val_f1_m: 0.6981\n",
      "Epoch 102/500\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5286 - acc: 0.7342 - auc_32: 0.8115 - recall_m: 0.7705 - precision_m: 0.7334 - f1_m: 0.7278 - val_loss: 0.5489 - val_acc: 0.7186 - val_auc_32: 0.8016 - val_recall_m: 0.7758 - val_precision_m: 0.6845 - val_f1_m: 0.7024\n",
      "Epoch 103/500\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5295 - acc: 0.7357 - auc_32: 0.8109 - recall_m: 0.7745 - precision_m: 0.7380 - f1_m: 0.7322 - val_loss: 0.5509 - val_acc: 0.7192 - val_auc_32: 0.8028 - val_recall_m: 0.7876 - val_precision_m: 0.6822 - val_f1_m: 0.7080\n",
      "Epoch 104/500\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5281 - acc: 0.7377 - auc_32: 0.8119 - recall_m: 0.7872 - precision_m: 0.7411 - f1_m: 0.7393 - val_loss: 0.5435 - val_acc: 0.7342 - val_auc_32: 0.8041 - val_recall_m: 0.7480 - val_precision_m: 0.7103 - val_f1_m: 0.7016\n",
      "Epoch 105/500\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5282 - acc: 0.7378 - auc_32: 0.8121 - recall_m: 0.7811 - precision_m: 0.7360 - f1_m: 0.7343 - val_loss: 0.5482 - val_acc: 0.7234 - val_auc_32: 0.8022 - val_recall_m: 0.7895 - val_precision_m: 0.6874 - val_f1_m: 0.7070\n",
      "Epoch 106/500\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5287 - acc: 0.7329 - auc_32: 0.8115 - recall_m: 0.7755 - precision_m: 0.7338 - f1_m: 0.7302 - val_loss: 0.5465 - val_acc: 0.7276 - val_auc_32: 0.8042 - val_recall_m: 0.7764 - val_precision_m: 0.7091 - val_f1_m: 0.7166\n",
      "Epoch 107/500\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5290 - acc: 0.7342 - auc_32: 0.8114 - recall_m: 0.7771 - precision_m: 0.7393 - f1_m: 0.7319 - val_loss: 0.5475 - val_acc: 0.7204 - val_auc_32: 0.8027 - val_recall_m: 0.7786 - val_precision_m: 0.6848 - val_f1_m: 0.7073\n",
      "Epoch 108/500\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5284 - acc: 0.7354 - auc_32: 0.8119 - recall_m: 0.7824 - precision_m: 0.7352 - f1_m: 0.7350 - val_loss: 0.5456 - val_acc: 0.7234 - val_auc_32: 0.8029 - val_recall_m: 0.7665 - val_precision_m: 0.6917 - val_f1_m: 0.6991\n",
      "Epoch 109/500\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5286 - acc: 0.7354 - auc_32: 0.8117 - recall_m: 0.7761 - precision_m: 0.7337 - f1_m: 0.7316 - val_loss: 0.5507 - val_acc: 0.7210 - val_auc_32: 0.8014 - val_recall_m: 0.7717 - val_precision_m: 0.6728 - val_f1_m: 0.6943\n",
      "Epoch 110/500\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5289 - acc: 0.7347 - auc_32: 0.8114 - recall_m: 0.7777 - precision_m: 0.7375 - f1_m: 0.7335 - val_loss: 0.5461 - val_acc: 0.7240 - val_auc_32: 0.8027 - val_recall_m: 0.7539 - val_precision_m: 0.6788 - val_f1_m: 0.6897\n",
      "Epoch 111/500\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5289 - acc: 0.7326 - auc_32: 0.8111 - recall_m: 0.7740 - precision_m: 0.7376 - f1_m: 0.7307 - val_loss: 0.5508 - val_acc: 0.7210 - val_auc_32: 0.8029 - val_recall_m: 0.7885 - val_precision_m: 0.6836 - val_f1_m: 0.7095\n",
      "Epoch 112/500\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5280 - acc: 0.7363 - auc_32: 0.8121 - recall_m: 0.7885 - precision_m: 0.7389 - f1_m: 0.7393 - val_loss: 0.5490 - val_acc: 0.7246 - val_auc_32: 0.8009 - val_recall_m: 0.7722 - val_precision_m: 0.6900 - val_f1_m: 0.6984\n",
      "Epoch 113/500\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5275 - acc: 0.7359 - auc_32: 0.8124 - recall_m: 0.7778 - precision_m: 0.7414 - f1_m: 0.7360 - val_loss: 0.5483 - val_acc: 0.7180 - val_auc_32: 0.8031 - val_recall_m: 0.7988 - val_precision_m: 0.6849 - val_f1_m: 0.7110\n",
      "Epoch 114/500\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5280 - acc: 0.7393 - auc_32: 0.8121 - recall_m: 0.7799 - precision_m: 0.7399 - f1_m: 0.7348 - val_loss: 0.5472 - val_acc: 0.7216 - val_auc_32: 0.8031 - val_recall_m: 0.7813 - val_precision_m: 0.6911 - val_f1_m: 0.7059\n",
      "Epoch 115/500\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5279 - acc: 0.7338 - auc_32: 0.8120 - recall_m: 0.7798 - precision_m: 0.7374 - f1_m: 0.7346 - val_loss: 0.5521 - val_acc: 0.7264 - val_auc_32: 0.7991 - val_recall_m: 0.7662 - val_precision_m: 0.6754 - val_f1_m: 0.6957\n",
      "Epoch 116/500\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5284 - acc: 0.7333 - auc_32: 0.8117 - recall_m: 0.7834 - precision_m: 0.7238 - f1_m: 0.7290 - val_loss: 0.5426 - val_acc: 0.7384 - val_auc_32: 0.8048 - val_recall_m: 0.7438 - val_precision_m: 0.7143 - val_f1_m: 0.7072\n",
      "Epoch 117/500\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5289 - acc: 0.7371 - auc_32: 0.8111 - recall_m: 0.7788 - precision_m: 0.7354 - f1_m: 0.7329 - val_loss: 0.5519 - val_acc: 0.7198 - val_auc_32: 0.8016 - val_recall_m: 0.7935 - val_precision_m: 0.6836 - val_f1_m: 0.7074\n",
      "Epoch 118/500\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5280 - acc: 0.7357 - auc_32: 0.8122 - recall_m: 0.7854 - precision_m: 0.7347 - f1_m: 0.7338 - val_loss: 0.5453 - val_acc: 0.7282 - val_auc_32: 0.8033 - val_recall_m: 0.7697 - val_precision_m: 0.6986 - val_f1_m: 0.7047\n",
      "Epoch 119/500\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5282 - acc: 0.7365 - auc_32: 0.8120 - recall_m: 0.7708 - precision_m: 0.7360 - f1_m: 0.7308 - val_loss: 0.5461 - val_acc: 0.7348 - val_auc_32: 0.8027 - val_recall_m: 0.7691 - val_precision_m: 0.7086 - val_f1_m: 0.7110\n",
      "Epoch 120/500\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5274 - acc: 0.7345 - auc_32: 0.8125 - recall_m: 0.7759 - precision_m: 0.7344 - f1_m: 0.7300 - val_loss: 0.5491 - val_acc: 0.7234 - val_auc_32: 0.8021 - val_recall_m: 0.7649 - val_precision_m: 0.6869 - val_f1_m: 0.7018\n",
      "Epoch 121/500\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5278 - acc: 0.7360 - auc_32: 0.8122 - recall_m: 0.7753 - precision_m: 0.7341 - f1_m: 0.7292 - val_loss: 0.5461 - val_acc: 0.7234 - val_auc_32: 0.8031 - val_recall_m: 0.7637 - val_precision_m: 0.6927 - val_f1_m: 0.7009\n",
      "Epoch 122/500\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5273 - acc: 0.7360 - auc_32: 0.8128 - recall_m: 0.7755 - precision_m: 0.7407 - f1_m: 0.7344 - val_loss: 0.5556 - val_acc: 0.7210 - val_auc_32: 0.7976 - val_recall_m: 0.7737 - val_precision_m: 0.6785 - val_f1_m: 0.6993\n",
      "Epoch 123/500\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5276 - acc: 0.7342 - auc_32: 0.8121 - recall_m: 0.7821 - precision_m: 0.7351 - f1_m: 0.7340 - val_loss: 0.5562 - val_acc: 0.7145 - val_auc_32: 0.8024 - val_recall_m: 0.7958 - val_precision_m: 0.6732 - val_f1_m: 0.7015\n",
      "Epoch 124/500\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5275 - acc: 0.7374 - auc_32: 0.8125 - recall_m: 0.7888 - precision_m: 0.7318 - f1_m: 0.7373 - val_loss: 0.5515 - val_acc: 0.7240 - val_auc_32: 0.7983 - val_recall_m: 0.7590 - val_precision_m: 0.6788 - val_f1_m: 0.6914\n",
      "Epoch 125/500\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5284 - acc: 0.7372 - auc_32: 0.8117 - recall_m: 0.7793 - precision_m: 0.7360 - f1_m: 0.7354 - val_loss: 0.5450 - val_acc: 0.7324 - val_auc_32: 0.8050 - val_recall_m: 0.7627 - val_precision_m: 0.7137 - val_f1_m: 0.7047\n",
      "Epoch 126/500\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5266 - acc: 0.7383 - auc_32: 0.8132 - recall_m: 0.7786 - precision_m: 0.7457 - f1_m: 0.7362 - val_loss: 0.5497 - val_acc: 0.7234 - val_auc_32: 0.8016 - val_recall_m: 0.7765 - val_precision_m: 0.6874 - val_f1_m: 0.7086\n",
      "Epoch 127/500\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5273 - acc: 0.7384 - auc_32: 0.8127 - recall_m: 0.7856 - precision_m: 0.7429 - f1_m: 0.7388 - val_loss: 0.5537 - val_acc: 0.7174 - val_auc_32: 0.8011 - val_recall_m: 0.7970 - val_precision_m: 0.6862 - val_f1_m: 0.7135\n",
      "Epoch 128/500\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5273 - acc: 0.7378 - auc_32: 0.8128 - recall_m: 0.7778 - precision_m: 0.7355 - f1_m: 0.7330 - val_loss: 0.5522 - val_acc: 0.7192 - val_auc_32: 0.8022 - val_recall_m: 0.7938 - val_precision_m: 0.6949 - val_f1_m: 0.7133\n",
      "Epoch 129/500\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5278 - acc: 0.7350 - auc_32: 0.8119 - recall_m: 0.7798 - precision_m: 0.7351 - f1_m: 0.7333 - val_loss: 0.5500 - val_acc: 0.7186 - val_auc_32: 0.8028 - val_recall_m: 0.7715 - val_precision_m: 0.6752 - val_f1_m: 0.7017\n",
      "Epoch 130/500\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5268 - acc: 0.7374 - auc_32: 0.8132 - recall_m: 0.7786 - precision_m: 0.7402 - f1_m: 0.7366 - val_loss: 0.5522 - val_acc: 0.7168 - val_auc_32: 0.8012 - val_recall_m: 0.8077 - val_precision_m: 0.6791 - val_f1_m: 0.7154\n",
      "Epoch 131/500\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5270 - acc: 0.7356 - auc_32: 0.8130 - recall_m: 0.7819 - precision_m: 0.7303 - f1_m: 0.7324 - val_loss: 0.5485 - val_acc: 0.7216 - val_auc_32: 0.8006 - val_recall_m: 0.7596 - val_precision_m: 0.6832 - val_f1_m: 0.6920\n",
      "Epoch 132/500\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5275 - acc: 0.7386 - auc_32: 0.8125 - recall_m: 0.7751 - precision_m: 0.7408 - f1_m: 0.7336 - val_loss: 0.5571 - val_acc: 0.7103 - val_auc_32: 0.8022 - val_recall_m: 0.7839 - val_precision_m: 0.6611 - val_f1_m: 0.6930\n",
      "Epoch 133/500\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5281 - acc: 0.7351 - auc_32: 0.8119 - recall_m: 0.7808 - precision_m: 0.7343 - f1_m: 0.7313 - val_loss: 0.5521 - val_acc: 0.7216 - val_auc_32: 0.7992 - val_recall_m: 0.7733 - val_precision_m: 0.6824 - val_f1_m: 0.6949\n",
      "Epoch 134/500\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5270 - acc: 0.7366 - auc_32: 0.8129 - recall_m: 0.7782 - precision_m: 0.7408 - f1_m: 0.7336 - val_loss: 0.5581 - val_acc: 0.7204 - val_auc_32: 0.7949 - val_recall_m: 0.7829 - val_precision_m: 0.6910 - val_f1_m: 0.7048\n",
      "Epoch 135/500\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5272 - acc: 0.7381 - auc_32: 0.8127 - recall_m: 0.7706 - precision_m: 0.7398 - f1_m: 0.7323 - val_loss: 0.5522 - val_acc: 0.7151 - val_auc_32: 0.8028 - val_recall_m: 0.7867 - val_precision_m: 0.6707 - val_f1_m: 0.6964\n",
      "Epoch 136/500\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5274 - acc: 0.7371 - auc_32: 0.8125 - recall_m: 0.7771 - precision_m: 0.7370 - f1_m: 0.7327 - val_loss: 0.5478 - val_acc: 0.7222 - val_auc_32: 0.8035 - val_recall_m: 0.7824 - val_precision_m: 0.6924 - val_f1_m: 0.7092\n",
      "Epoch 137/500\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5278 - acc: 0.7383 - auc_32: 0.8120 - recall_m: 0.7812 - precision_m: 0.7344 - f1_m: 0.7330 - val_loss: 0.5474 - val_acc: 0.7198 - val_auc_32: 0.8044 - val_recall_m: 0.7837 - val_precision_m: 0.6901 - val_f1_m: 0.7080\n",
      "Epoch 138/500\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5279 - acc: 0.7393 - auc_32: 0.8122 - recall_m: 0.7806 - precision_m: 0.7362 - f1_m: 0.7336 - val_loss: 0.5465 - val_acc: 0.7240 - val_auc_32: 0.8019 - val_recall_m: 0.7725 - val_precision_m: 0.6909 - val_f1_m: 0.7059\n",
      "Epoch 139/500\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5269 - acc: 0.7369 - auc_32: 0.8129 - recall_m: 0.7756 - precision_m: 0.7348 - f1_m: 0.7330 - val_loss: 0.5548 - val_acc: 0.7127 - val_auc_32: 0.8008 - val_recall_m: 0.7818 - val_precision_m: 0.6681 - val_f1_m: 0.6935\n",
      "Epoch 140/500\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5277 - acc: 0.7357 - auc_32: 0.8123 - recall_m: 0.7732 - precision_m: 0.7346 - f1_m: 0.7272 - val_loss: 0.5510 - val_acc: 0.7157 - val_auc_32: 0.8012 - val_recall_m: 0.7773 - val_precision_m: 0.6842 - val_f1_m: 0.6991\n",
      "Epoch 141/500\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5281 - acc: 0.7350 - auc_32: 0.8119 - recall_m: 0.7721 - precision_m: 0.7383 - f1_m: 0.7309 - val_loss: 0.5532 - val_acc: 0.7222 - val_auc_32: 0.7976 - val_recall_m: 0.7663 - val_precision_m: 0.7098 - val_f1_m: 0.7082\n",
      "Epoch 142/500\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5284 - acc: 0.7362 - auc_32: 0.8113 - recall_m: 0.7761 - precision_m: 0.7376 - f1_m: 0.7307 - val_loss: 0.5497 - val_acc: 0.7151 - val_auc_32: 0.8031 - val_recall_m: 0.7648 - val_precision_m: 0.6717 - val_f1_m: 0.6924\n",
      "Epoch 143/500\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5268 - acc: 0.7365 - auc_32: 0.8131 - recall_m: 0.7733 - precision_m: 0.7354 - f1_m: 0.7301 - val_loss: 0.5563 - val_acc: 0.7157 - val_auc_32: 0.7998 - val_recall_m: 0.7779 - val_precision_m: 0.6751 - val_f1_m: 0.6986\n",
      "Epoch 144/500\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5273 - acc: 0.7360 - auc_32: 0.8126 - recall_m: 0.7826 - precision_m: 0.7430 - f1_m: 0.7366 - val_loss: 0.5523 - val_acc: 0.7145 - val_auc_32: 0.8033 - val_recall_m: 0.7848 - val_precision_m: 0.6701 - val_f1_m: 0.7001\n",
      "Epoch 145/500\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5265 - acc: 0.7341 - auc_32: 0.8133 - recall_m: 0.7718 - precision_m: 0.7346 - f1_m: 0.7288 - val_loss: 0.5436 - val_acc: 0.7312 - val_auc_32: 0.8058 - val_recall_m: 0.7440 - val_precision_m: 0.6965 - val_f1_m: 0.6957\n",
      "Epoch 146/500\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5268 - acc: 0.7356 - auc_32: 0.8131 - recall_m: 0.7703 - precision_m: 0.7381 - f1_m: 0.7294 - val_loss: 0.5567 - val_acc: 0.7162 - val_auc_32: 0.7981 - val_recall_m: 0.7710 - val_precision_m: 0.6819 - val_f1_m: 0.7000\n",
      "Epoch 147/500\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5273 - acc: 0.7357 - auc_32: 0.8123 - recall_m: 0.7767 - precision_m: 0.7365 - f1_m: 0.7343 - val_loss: 0.5516 - val_acc: 0.7121 - val_auc_32: 0.8042 - val_recall_m: 0.7881 - val_precision_m: 0.6724 - val_f1_m: 0.7016\n",
      "Epoch 148/500\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5273 - acc: 0.7338 - auc_32: 0.8125 - recall_m: 0.7814 - precision_m: 0.7368 - f1_m: 0.7327 - val_loss: 0.5492 - val_acc: 0.7139 - val_auc_32: 0.8023 - val_recall_m: 0.7841 - val_precision_m: 0.6739 - val_f1_m: 0.7005\n",
      "Epoch 149/500\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5253 - acc: 0.7390 - auc_32: 0.8145 - recall_m: 0.7817 - precision_m: 0.7371 - f1_m: 0.7336 - val_loss: 0.5647 - val_acc: 0.7121 - val_auc_32: 0.7928 - val_recall_m: 0.7920 - val_precision_m: 0.6772 - val_f1_m: 0.7058\n",
      "Epoch 150/500\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5270 - acc: 0.7377 - auc_32: 0.8128 - recall_m: 0.7801 - precision_m: 0.7351 - f1_m: 0.7343 - val_loss: 0.5483 - val_acc: 0.7252 - val_auc_32: 0.8044 - val_recall_m: 0.7777 - val_precision_m: 0.6956 - val_f1_m: 0.7107\n",
      "Epoch 151/500\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5272 - acc: 0.7351 - auc_32: 0.8125 - recall_m: 0.7733 - precision_m: 0.7378 - f1_m: 0.7297 - val_loss: 0.5459 - val_acc: 0.7240 - val_auc_32: 0.8024 - val_recall_m: 0.7585 - val_precision_m: 0.6905 - val_f1_m: 0.6971\n",
      "Epoch 152/500\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5266 - acc: 0.7372 - auc_32: 0.8133 - recall_m: 0.7689 - precision_m: 0.7390 - f1_m: 0.7301 - val_loss: 0.5599 - val_acc: 0.7097 - val_auc_32: 0.8006 - val_recall_m: 0.7946 - val_precision_m: 0.6622 - val_f1_m: 0.6996\n",
      "Epoch 153/500\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5263 - acc: 0.7339 - auc_32: 0.8133 - recall_m: 0.7753 - precision_m: 0.7282 - f1_m: 0.7295 - val_loss: 0.5596 - val_acc: 0.7115 - val_auc_32: 0.7974 - val_recall_m: 0.7706 - val_precision_m: 0.6531 - val_f1_m: 0.6838\n",
      "Epoch 154/500\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5278 - acc: 0.7357 - auc_32: 0.8122 - recall_m: 0.7785 - precision_m: 0.7372 - f1_m: 0.7340 - val_loss: 0.5516 - val_acc: 0.7180 - val_auc_32: 0.8006 - val_recall_m: 0.7431 - val_precision_m: 0.6780 - val_f1_m: 0.6879\n",
      "Epoch 155/500\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5271 - acc: 0.7371 - auc_32: 0.8126 - recall_m: 0.7786 - precision_m: 0.7388 - f1_m: 0.7309 - val_loss: 0.5561 - val_acc: 0.7151 - val_auc_32: 0.8008 - val_recall_m: 0.8013 - val_precision_m: 0.6821 - val_f1_m: 0.7068\n",
      "Epoch 156/500\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5261 - acc: 0.7357 - auc_32: 0.8135 - recall_m: 0.7906 - precision_m: 0.7375 - f1_m: 0.7373 - val_loss: 0.5475 - val_acc: 0.7198 - val_auc_32: 0.8021 - val_recall_m: 0.7861 - val_precision_m: 0.6963 - val_f1_m: 0.7088\n",
      "Epoch 157/500\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5267 - acc: 0.7368 - auc_32: 0.8132 - recall_m: 0.7697 - precision_m: 0.7387 - f1_m: 0.7311 - val_loss: 0.5488 - val_acc: 0.7157 - val_auc_32: 0.8045 - val_recall_m: 0.7812 - val_precision_m: 0.6818 - val_f1_m: 0.7030\n",
      "Epoch 158/500\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5261 - acc: 0.7351 - auc_32: 0.8137 - recall_m: 0.7757 - precision_m: 0.7332 - f1_m: 0.7294 - val_loss: 0.5535 - val_acc: 0.7246 - val_auc_32: 0.7969 - val_recall_m: 0.7672 - val_precision_m: 0.7084 - val_f1_m: 0.7118\n",
      "Epoch 159/500\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5267 - acc: 0.7347 - auc_32: 0.8127 - recall_m: 0.7735 - precision_m: 0.7320 - f1_m: 0.7286 - val_loss: 0.5574 - val_acc: 0.7192 - val_auc_32: 0.7946 - val_recall_m: 0.7634 - val_precision_m: 0.6834 - val_f1_m: 0.6965\n",
      "Epoch 160/500\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5261 - acc: 0.7354 - auc_32: 0.8134 - recall_m: 0.7812 - precision_m: 0.7333 - f1_m: 0.7343 - val_loss: 0.5465 - val_acc: 0.7222 - val_auc_32: 0.8028 - val_recall_m: 0.7590 - val_precision_m: 0.6884 - val_f1_m: 0.6958\n",
      "Epoch 161/500\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5261 - acc: 0.7332 - auc_32: 0.8135 - recall_m: 0.7731 - precision_m: 0.7409 - f1_m: 0.7339 - val_loss: 0.5565 - val_acc: 0.7210 - val_auc_32: 0.7956 - val_recall_m: 0.7761 - val_precision_m: 0.7064 - val_f1_m: 0.7104\n",
      "Epoch 162/500\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5267 - acc: 0.7381 - auc_32: 0.8129 - recall_m: 0.7728 - precision_m: 0.7373 - f1_m: 0.7306 - val_loss: 0.5517 - val_acc: 0.7157 - val_auc_32: 0.8021 - val_recall_m: 0.7923 - val_precision_m: 0.6828 - val_f1_m: 0.7040\n",
      "Epoch 163/500\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5262 - acc: 0.7365 - auc_32: 0.8135 - recall_m: 0.7810 - precision_m: 0.7351 - f1_m: 0.7344 - val_loss: 0.5551 - val_acc: 0.7186 - val_auc_32: 0.7972 - val_recall_m: 0.7748 - val_precision_m: 0.6776 - val_f1_m: 0.6959\n",
      "Epoch 164/500\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5266 - acc: 0.7342 - auc_32: 0.8130 - recall_m: 0.7744 - precision_m: 0.7387 - f1_m: 0.7332 - val_loss: 0.5551 - val_acc: 0.7085 - val_auc_32: 0.8006 - val_recall_m: 0.8001 - val_precision_m: 0.6755 - val_f1_m: 0.7075\n",
      "Epoch 165/500\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5262 - acc: 0.7378 - auc_32: 0.8134 - recall_m: 0.7848 - precision_m: 0.7379 - f1_m: 0.7365 - val_loss: 0.5429 - val_acc: 0.7336 - val_auc_32: 0.8052 - val_recall_m: 0.7502 - val_precision_m: 0.7069 - val_f1_m: 0.7016\n",
      "Epoch 166/500\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5259 - acc: 0.7384 - auc_32: 0.8139 - recall_m: 0.7780 - precision_m: 0.7406 - f1_m: 0.7348 - val_loss: 0.5514 - val_acc: 0.7151 - val_auc_32: 0.8018 - val_recall_m: 0.7869 - val_precision_m: 0.6769 - val_f1_m: 0.7015\n",
      "Epoch 167/500\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5262 - acc: 0.7359 - auc_32: 0.8134 - recall_m: 0.7758 - precision_m: 0.7334 - f1_m: 0.7303 - val_loss: 0.5549 - val_acc: 0.7145 - val_auc_32: 0.8017 - val_recall_m: 0.7852 - val_precision_m: 0.6856 - val_f1_m: 0.7085\n",
      "Epoch 168/500\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5266 - acc: 0.7345 - auc_32: 0.8130 - recall_m: 0.7827 - precision_m: 0.7370 - f1_m: 0.7366 - val_loss: 0.5516 - val_acc: 0.7186 - val_auc_32: 0.8010 - val_recall_m: 0.7908 - val_precision_m: 0.6823 - val_f1_m: 0.7078\n",
      "Epoch 169/500\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "6694/6694 - 2s - loss: 0.5263 - acc: 0.7362 - auc_32: 0.8132 - recall_m: 0.7782 - precision_m: 0.7402 - f1_m: 0.7327 - val_loss: 0.5491 - val_acc: 0.7180 - val_auc_32: 0.8025 - val_recall_m: 0.7882 - val_precision_m: 0.6851 - val_f1_m: 0.7073\n",
      "Epoch 170/500\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5220 - acc: 0.7393 - auc_32: 0.8170 - recall_m: 0.7949 - precision_m: 0.7360 - f1_m: 0.7417 - val_loss: 0.5500 - val_acc: 0.7186 - val_auc_32: 0.8020 - val_recall_m: 0.7847 - val_precision_m: 0.6802 - val_f1_m: 0.7047\n",
      "Epoch 171/500\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5219 - acc: 0.7404 - auc_32: 0.8172 - recall_m: 0.8034 - precision_m: 0.7400 - f1_m: 0.7463 - val_loss: 0.5487 - val_acc: 0.7192 - val_auc_32: 0.8027 - val_recall_m: 0.7787 - val_precision_m: 0.6871 - val_f1_m: 0.7058\n",
      "Epoch 172/500\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5218 - acc: 0.7395 - auc_32: 0.8172 - recall_m: 0.7961 - precision_m: 0.7373 - f1_m: 0.7428 - val_loss: 0.5511 - val_acc: 0.7186 - val_auc_32: 0.8010 - val_recall_m: 0.7683 - val_precision_m: 0.6791 - val_f1_m: 0.6981\n",
      "Epoch 173/500\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5218 - acc: 0.7410 - auc_32: 0.8171 - recall_m: 0.7961 - precision_m: 0.7410 - f1_m: 0.7442 - val_loss: 0.5503 - val_acc: 0.7174 - val_auc_32: 0.8015 - val_recall_m: 0.7724 - val_precision_m: 0.6718 - val_f1_m: 0.6934\n",
      "Epoch 174/500\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5218 - acc: 0.7405 - auc_32: 0.8172 - recall_m: 0.7966 - precision_m: 0.7331 - f1_m: 0.7422 - val_loss: 0.5490 - val_acc: 0.7186 - val_auc_32: 0.8025 - val_recall_m: 0.7757 - val_precision_m: 0.6806 - val_f1_m: 0.7014\n",
      "Epoch 175/500\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5217 - acc: 0.7408 - auc_32: 0.8173 - recall_m: 0.7942 - precision_m: 0.7390 - f1_m: 0.7419 - val_loss: 0.5493 - val_acc: 0.7180 - val_auc_32: 0.8026 - val_recall_m: 0.7784 - val_precision_m: 0.6874 - val_f1_m: 0.7047\n",
      "Epoch 176/500\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5218 - acc: 0.7413 - auc_32: 0.8172 - recall_m: 0.7949 - precision_m: 0.7342 - f1_m: 0.7406 - val_loss: 0.5483 - val_acc: 0.7192 - val_auc_32: 0.8031 - val_recall_m: 0.7790 - val_precision_m: 0.6887 - val_f1_m: 0.7026\n",
      "Epoch 177/500\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5218 - acc: 0.7410 - auc_32: 0.8170 - recall_m: 0.7947 - precision_m: 0.7335 - f1_m: 0.7398 - val_loss: 0.5503 - val_acc: 0.7186 - val_auc_32: 0.8017 - val_recall_m: 0.7785 - val_precision_m: 0.6911 - val_f1_m: 0.7011\n",
      "Epoch 178/500\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5218 - acc: 0.7408 - auc_32: 0.8172 - recall_m: 0.7923 - precision_m: 0.7343 - f1_m: 0.7406 - val_loss: 0.5512 - val_acc: 0.7180 - val_auc_32: 0.8011 - val_recall_m: 0.7720 - val_precision_m: 0.6817 - val_f1_m: 0.6961\n",
      "Epoch 179/500\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5218 - acc: 0.7410 - auc_32: 0.8172 - recall_m: 0.8004 - precision_m: 0.7380 - f1_m: 0.7440 - val_loss: 0.5499 - val_acc: 0.7180 - val_auc_32: 0.8021 - val_recall_m: 0.7776 - val_precision_m: 0.6817 - val_f1_m: 0.7011\n",
      "Epoch 180/500\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7401 - auc_32: 0.8175 - recall_m: 0.7907 - precision_m: 0.7323 - f1_m: 0.7362 - val_loss: 0.5518 - val_acc: 0.7180 - val_auc_32: 0.8009 - val_recall_m: 0.7685 - val_precision_m: 0.6799 - val_f1_m: 0.6956\n",
      "Epoch 181/500\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7413 - auc_32: 0.8175 - recall_m: 0.7933 - precision_m: 0.7346 - f1_m: 0.7416 - val_loss: 0.5504 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7789 - val_precision_m: 0.6814 - val_f1_m: 0.7052\n",
      "Epoch 182/500\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5217 - acc: 0.7413 - auc_32: 0.8174 - recall_m: 0.7976 - precision_m: 0.7322 - f1_m: 0.7406 - val_loss: 0.5502 - val_acc: 0.7186 - val_auc_32: 0.8018 - val_recall_m: 0.7812 - val_precision_m: 0.6872 - val_f1_m: 0.7011\n",
      "Epoch 183/500\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7407 - auc_32: 0.8174 - recall_m: 0.7938 - precision_m: 0.7351 - f1_m: 0.7419 - val_loss: 0.5521 - val_acc: 0.7186 - val_auc_32: 0.8007 - val_recall_m: 0.7865 - val_precision_m: 0.6904 - val_f1_m: 0.7080\n",
      "Epoch 184/500\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5217 - acc: 0.7416 - auc_32: 0.8174 - recall_m: 0.7974 - precision_m: 0.7361 - f1_m: 0.7415 - val_loss: 0.5494 - val_acc: 0.7186 - val_auc_32: 0.8025 - val_recall_m: 0.7840 - val_precision_m: 0.6836 - val_f1_m: 0.7069\n",
      "Epoch 185/500\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5217 - acc: 0.7417 - auc_32: 0.8174 - recall_m: 0.7995 - precision_m: 0.7346 - f1_m: 0.7444 - val_loss: 0.5513 - val_acc: 0.7180 - val_auc_32: 0.8014 - val_recall_m: 0.7660 - val_precision_m: 0.6730 - val_f1_m: 0.6905\n",
      "Epoch 186/500\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7413 - auc_32: 0.8174 - recall_m: 0.7978 - precision_m: 0.7309 - f1_m: 0.7397 - val_loss: 0.5507 - val_acc: 0.7186 - val_auc_32: 0.8014 - val_recall_m: 0.7749 - val_precision_m: 0.6863 - val_f1_m: 0.7025\n",
      "Epoch 187/500\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5217 - acc: 0.7408 - auc_32: 0.8173 - recall_m: 0.8012 - precision_m: 0.7330 - f1_m: 0.7435 - val_loss: 0.5495 - val_acc: 0.7186 - val_auc_32: 0.8022 - val_recall_m: 0.7848 - val_precision_m: 0.6913 - val_f1_m: 0.7043\n",
      "Epoch 188/500\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5217 - acc: 0.7405 - auc_32: 0.8173 - recall_m: 0.7956 - precision_m: 0.7368 - f1_m: 0.7415 - val_loss: 0.5519 - val_acc: 0.7186 - val_auc_32: 0.8007 - val_recall_m: 0.7804 - val_precision_m: 0.6837 - val_f1_m: 0.7017\n",
      "Epoch 189/500\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7402 - auc_32: 0.8174 - recall_m: 0.7987 - precision_m: 0.7392 - f1_m: 0.7439 - val_loss: 0.5497 - val_acc: 0.7180 - val_auc_32: 0.8022 - val_recall_m: 0.7772 - val_precision_m: 0.6827 - val_f1_m: 0.6981\n",
      "Epoch 190/500\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7413 - auc_32: 0.8174 - recall_m: 0.7997 - precision_m: 0.7345 - f1_m: 0.7411 - val_loss: 0.5507 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7861 - val_precision_m: 0.6870 - val_f1_m: 0.7090\n",
      "Epoch 191/500\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5215 - acc: 0.7416 - auc_32: 0.8176 - recall_m: 0.7969 - precision_m: 0.7339 - f1_m: 0.7417 - val_loss: 0.5498 - val_acc: 0.7180 - val_auc_32: 0.8025 - val_recall_m: 0.7717 - val_precision_m: 0.6792 - val_f1_m: 0.6983\n",
      "Epoch 192/500\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7422 - auc_32: 0.8172 - recall_m: 0.7894 - precision_m: 0.7312 - f1_m: 0.7366 - val_loss: 0.5512 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7924 - val_precision_m: 0.6924 - val_f1_m: 0.7110\n",
      "Epoch 193/500\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7413 - auc_32: 0.8174 - recall_m: 0.7951 - precision_m: 0.7322 - f1_m: 0.7387 - val_loss: 0.5520 - val_acc: 0.7180 - val_auc_32: 0.8008 - val_recall_m: 0.7761 - val_precision_m: 0.6741 - val_f1_m: 0.6976\n",
      "Epoch 194/500\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7407 - auc_32: 0.8174 - recall_m: 0.7979 - precision_m: 0.7364 - f1_m: 0.7422 - val_loss: 0.5500 - val_acc: 0.7186 - val_auc_32: 0.8021 - val_recall_m: 0.7708 - val_precision_m: 0.6781 - val_f1_m: 0.7003\n",
      "Epoch 195/500\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5215 - acc: 0.7405 - auc_32: 0.8174 - recall_m: 0.7913 - precision_m: 0.7314 - f1_m: 0.7357 - val_loss: 0.5515 - val_acc: 0.7180 - val_auc_32: 0.8010 - val_recall_m: 0.7752 - val_precision_m: 0.6826 - val_f1_m: 0.7028\n",
      "Epoch 196/500\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7408 - auc_32: 0.8175 - recall_m: 0.7960 - precision_m: 0.7362 - f1_m: 0.7418 - val_loss: 0.5502 - val_acc: 0.7186 - val_auc_32: 0.8017 - val_recall_m: 0.7877 - val_precision_m: 0.6957 - val_f1_m: 0.7107\n",
      "Epoch 197/500\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5215 - acc: 0.7408 - auc_32: 0.8174 - recall_m: 0.7894 - precision_m: 0.7327 - f1_m: 0.7384 - val_loss: 0.5519 - val_acc: 0.7180 - val_auc_32: 0.8007 - val_recall_m: 0.7829 - val_precision_m: 0.6908 - val_f1_m: 0.7100\n",
      "Epoch 198/500\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5217 - acc: 0.7419 - auc_32: 0.8174 - recall_m: 0.7937 - precision_m: 0.7349 - f1_m: 0.7404 - val_loss: 0.5498 - val_acc: 0.7186 - val_auc_32: 0.8022 - val_recall_m: 0.7849 - val_precision_m: 0.6839 - val_f1_m: 0.7041\n",
      "Epoch 199/500\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7411 - auc_32: 0.8174 - recall_m: 0.7929 - precision_m: 0.7326 - f1_m: 0.7391 - val_loss: 0.5503 - val_acc: 0.7186 - val_auc_32: 0.8018 - val_recall_m: 0.7618 - val_precision_m: 0.6719 - val_f1_m: 0.6911\n",
      "Epoch 200/500\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7405 - auc_32: 0.8174 - recall_m: 0.7954 - precision_m: 0.7307 - f1_m: 0.7374 - val_loss: 0.5506 - val_acc: 0.7186 - val_auc_32: 0.8015 - val_recall_m: 0.7688 - val_precision_m: 0.6825 - val_f1_m: 0.7007\n",
      "Epoch 201/500\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7414 - auc_32: 0.8174 - recall_m: 0.7975 - precision_m: 0.7378 - f1_m: 0.7428 - val_loss: 0.5510 - val_acc: 0.7186 - val_auc_32: 0.8015 - val_recall_m: 0.7855 - val_precision_m: 0.6840 - val_f1_m: 0.7043\n",
      "Epoch 202/500\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7411 - auc_32: 0.8174 - recall_m: 0.7960 - precision_m: 0.7371 - f1_m: 0.7418 - val_loss: 0.5503 - val_acc: 0.7192 - val_auc_32: 0.8019 - val_recall_m: 0.7785 - val_precision_m: 0.6823 - val_f1_m: 0.7008\n",
      "Epoch 203/500\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7404 - auc_32: 0.8173 - recall_m: 0.7930 - precision_m: 0.7278 - f1_m: 0.7378 - val_loss: 0.5510 - val_acc: 0.7186 - val_auc_32: 0.8013 - val_recall_m: 0.7677 - val_precision_m: 0.6708 - val_f1_m: 0.6899\n",
      "Epoch 204/500\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7411 - auc_32: 0.8175 - recall_m: 0.7989 - precision_m: 0.7331 - f1_m: 0.7426 - val_loss: 0.5495 - val_acc: 0.7180 - val_auc_32: 0.8023 - val_recall_m: 0.7896 - val_precision_m: 0.6882 - val_f1_m: 0.7078\n",
      "Epoch 205/500\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7413 - auc_32: 0.8173 - recall_m: 0.7996 - precision_m: 0.7398 - f1_m: 0.7435 - val_loss: 0.5498 - val_acc: 0.7168 - val_auc_32: 0.8017 - val_recall_m: 0.7715 - val_precision_m: 0.6874 - val_f1_m: 0.7006\n",
      "Epoch 206/500\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7410 - auc_32: 0.8174 - recall_m: 0.7941 - precision_m: 0.7372 - f1_m: 0.7406 - val_loss: 0.5514 - val_acc: 0.7174 - val_auc_32: 0.8010 - val_recall_m: 0.7645 - val_precision_m: 0.6778 - val_f1_m: 0.6964\n",
      "Epoch 207/500\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5215 - acc: 0.7410 - auc_32: 0.8173 - recall_m: 0.7941 - precision_m: 0.7372 - f1_m: 0.7426 - val_loss: 0.5499 - val_acc: 0.7186 - val_auc_32: 0.8020 - val_recall_m: 0.7802 - val_precision_m: 0.6862 - val_f1_m: 0.6985\n",
      "Epoch 208/500\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5215 - acc: 0.7405 - auc_32: 0.8175 - recall_m: 0.8023 - precision_m: 0.7359 - f1_m: 0.7447 - val_loss: 0.5495 - val_acc: 0.7186 - val_auc_32: 0.8021 - val_recall_m: 0.7767 - val_precision_m: 0.6986 - val_f1_m: 0.7055\n",
      "Epoch 209/500\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5214 - acc: 0.7405 - auc_32: 0.8176 - recall_m: 0.7943 - precision_m: 0.7380 - f1_m: 0.7401 - val_loss: 0.5488 - val_acc: 0.7180 - val_auc_32: 0.8027 - val_recall_m: 0.7759 - val_precision_m: 0.6881 - val_f1_m: 0.6999\n",
      "Epoch 210/500\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7410 - auc_32: 0.8173 - recall_m: 0.7923 - precision_m: 0.7318 - f1_m: 0.7380 - val_loss: 0.5510 - val_acc: 0.7180 - val_auc_32: 0.8014 - val_recall_m: 0.7756 - val_precision_m: 0.6822 - val_f1_m: 0.6965\n",
      "Epoch 211/500\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00211: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "6694/6694 - 1s - loss: 0.5216 - acc: 0.7420 - auc_32: 0.8173 - recall_m: 0.7922 - precision_m: 0.7339 - f1_m: 0.7402 - val_loss: 0.5502 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7900 - val_precision_m: 0.6934 - val_f1_m: 0.7110\n",
      "Epoch 212/500\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7416 - auc_32: 0.8180 - recall_m: 0.7932 - precision_m: 0.7323 - f1_m: 0.7387 - val_loss: 0.5503 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7828 - val_precision_m: 0.6804 - val_f1_m: 0.6988\n",
      "Epoch 213/500\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7419 - auc_32: 0.8179 - recall_m: 0.7950 - precision_m: 0.7371 - f1_m: 0.7423 - val_loss: 0.5504 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7654 - val_precision_m: 0.6776 - val_f1_m: 0.6948\n",
      "Epoch 214/500\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7417 - auc_32: 0.8180 - recall_m: 0.7963 - precision_m: 0.7350 - f1_m: 0.7416 - val_loss: 0.5506 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7860 - val_precision_m: 0.6871 - val_f1_m: 0.7041\n",
      "Epoch 215/500\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7416 - auc_32: 0.8180 - recall_m: 0.8000 - precision_m: 0.7390 - f1_m: 0.7437 - val_loss: 0.5506 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7863 - val_precision_m: 0.6975 - val_f1_m: 0.7080\n",
      "Epoch 216/500\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7416 - auc_32: 0.8179 - recall_m: 0.7948 - precision_m: 0.7385 - f1_m: 0.7401 - val_loss: 0.5507 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7631 - val_precision_m: 0.6762 - val_f1_m: 0.6926\n",
      "Epoch 217/500\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7417 - auc_32: 0.8180 - recall_m: 0.7998 - precision_m: 0.7432 - f1_m: 0.7469 - val_loss: 0.5507 - val_acc: 0.7180 - val_auc_32: 0.8014 - val_recall_m: 0.7732 - val_precision_m: 0.6795 - val_f1_m: 0.6986\n",
      "Epoch 218/500\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7419 - auc_32: 0.8180 - recall_m: 0.7934 - precision_m: 0.7328 - f1_m: 0.7402 - val_loss: 0.5507 - val_acc: 0.7180 - val_auc_32: 0.8014 - val_recall_m: 0.7679 - val_precision_m: 0.6859 - val_f1_m: 0.6982\n",
      "Epoch 219/500\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7416 - auc_32: 0.8180 - recall_m: 0.7928 - precision_m: 0.7336 - f1_m: 0.7384 - val_loss: 0.5507 - val_acc: 0.7180 - val_auc_32: 0.8014 - val_recall_m: 0.7616 - val_precision_m: 0.6763 - val_f1_m: 0.6891\n",
      "Epoch 220/500\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7417 - auc_32: 0.8180 - recall_m: 0.7911 - precision_m: 0.7327 - f1_m: 0.7382 - val_loss: 0.5508 - val_acc: 0.7180 - val_auc_32: 0.8014 - val_recall_m: 0.7647 - val_precision_m: 0.6874 - val_f1_m: 0.6984\n",
      "Epoch 221/500\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7419 - auc_32: 0.8180 - recall_m: 0.7960 - precision_m: 0.7355 - f1_m: 0.7427 - val_loss: 0.5508 - val_acc: 0.7180 - val_auc_32: 0.8014 - val_recall_m: 0.7871 - val_precision_m: 0.6884 - val_f1_m: 0.7104\n",
      "Epoch 222/500\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7422 - auc_32: 0.8181 - recall_m: 0.7969 - precision_m: 0.7351 - f1_m: 0.7421 - val_loss: 0.5507 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7741 - val_precision_m: 0.6799 - val_f1_m: 0.7017\n",
      "Epoch 223/500\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7422 - auc_32: 0.8181 - recall_m: 0.8001 - precision_m: 0.7369 - f1_m: 0.7439 - val_loss: 0.5507 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7807 - val_precision_m: 0.6877 - val_f1_m: 0.7007\n",
      "Epoch 224/500\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7417 - auc_32: 0.8181 - recall_m: 0.7888 - precision_m: 0.7293 - f1_m: 0.7366 - val_loss: 0.5507 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7839 - val_precision_m: 0.6900 - val_f1_m: 0.7031\n",
      "Epoch 225/500\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7417 - auc_32: 0.8181 - recall_m: 0.7985 - precision_m: 0.7334 - f1_m: 0.7420 - val_loss: 0.5507 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7730 - val_precision_m: 0.6822 - val_f1_m: 0.7009\n",
      "Epoch 226/500\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7420 - auc_32: 0.8181 - recall_m: 0.7979 - precision_m: 0.7358 - f1_m: 0.7446 - val_loss: 0.5506 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7643 - val_precision_m: 0.6841 - val_f1_m: 0.6996\n",
      "Epoch 227/500\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7419 - auc_32: 0.8180 - recall_m: 0.7994 - precision_m: 0.7399 - f1_m: 0.7461 - val_loss: 0.5508 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7727 - val_precision_m: 0.6816 - val_f1_m: 0.6985\n",
      "Epoch 228/500\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7417 - auc_32: 0.8181 - recall_m: 0.7915 - precision_m: 0.7359 - f1_m: 0.7404 - val_loss: 0.5508 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7646 - val_precision_m: 0.6766 - val_f1_m: 0.6969\n",
      "Epoch 229/500\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7420 - auc_32: 0.8180 - recall_m: 0.7969 - precision_m: 0.7393 - f1_m: 0.7455 - val_loss: 0.5508 - val_acc: 0.7180 - val_auc_32: 0.8016 - val_recall_m: 0.7802 - val_precision_m: 0.6861 - val_f1_m: 0.7050\n",
      "Epoch 230/500\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7419 - auc_32: 0.8181 - recall_m: 0.7956 - precision_m: 0.7322 - f1_m: 0.7432 - val_loss: 0.5508 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7782 - val_precision_m: 0.6924 - val_f1_m: 0.7070\n",
      "Epoch 231/500\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7420 - auc_32: 0.8182 - recall_m: 0.7985 - precision_m: 0.7379 - f1_m: 0.7441 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7724 - val_precision_m: 0.6743 - val_f1_m: 0.6969\n",
      "Epoch 232/500\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00232: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "6694/6694 - 1s - loss: 0.5210 - acc: 0.7419 - auc_32: 0.8181 - recall_m: 0.8024 - precision_m: 0.7380 - f1_m: 0.7457 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7730 - val_precision_m: 0.6820 - val_f1_m: 0.6981\n",
      "Epoch 233/500\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7422 - auc_32: 0.8183 - recall_m: 0.7931 - precision_m: 0.7338 - f1_m: 0.7395 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7701 - val_precision_m: 0.6759 - val_f1_m: 0.6938\n",
      "Epoch 234/500\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7420 - auc_32: 0.8183 - recall_m: 0.7958 - precision_m: 0.7330 - f1_m: 0.7400 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7741 - val_precision_m: 0.6914 - val_f1_m: 0.7042\n",
      "Epoch 235/500\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8182 - recall_m: 0.7950 - precision_m: 0.7358 - f1_m: 0.7411 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7640 - val_precision_m: 0.6746 - val_f1_m: 0.6891\n",
      "Epoch 236/500\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7422 - auc_32: 0.8183 - recall_m: 0.7971 - precision_m: 0.7362 - f1_m: 0.7433 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7742 - val_precision_m: 0.6767 - val_f1_m: 0.6963\n",
      "Epoch 237/500\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7963 - precision_m: 0.7354 - f1_m: 0.7424 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7782 - val_precision_m: 0.6804 - val_f1_m: 0.6988\n",
      "Epoch 238/500\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7998 - precision_m: 0.7348 - f1_m: 0.7433 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7851 - val_precision_m: 0.6908 - val_f1_m: 0.7037\n",
      "Epoch 239/500\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7978 - precision_m: 0.7359 - f1_m: 0.7420 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7824 - val_precision_m: 0.6779 - val_f1_m: 0.6987\n",
      "Epoch 240/500\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7902 - precision_m: 0.7360 - f1_m: 0.7383 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7830 - val_precision_m: 0.6841 - val_f1_m: 0.7044\n",
      "Epoch 241/500\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8182 - recall_m: 0.7967 - precision_m: 0.7364 - f1_m: 0.7442 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7716 - val_precision_m: 0.6839 - val_f1_m: 0.7010\n",
      "Epoch 242/500\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7932 - precision_m: 0.7367 - f1_m: 0.7440 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7788 - val_precision_m: 0.6800 - val_f1_m: 0.7014\n",
      "Epoch 243/500\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7420 - auc_32: 0.8183 - recall_m: 0.7987 - precision_m: 0.7358 - f1_m: 0.7445 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7829 - val_precision_m: 0.6882 - val_f1_m: 0.7111\n",
      "Epoch 244/500\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7973 - precision_m: 0.7365 - f1_m: 0.7426 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7734 - val_precision_m: 0.6919 - val_f1_m: 0.7058\n",
      "Epoch 245/500\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7939 - precision_m: 0.7334 - f1_m: 0.7411 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7826 - val_precision_m: 0.6895 - val_f1_m: 0.7071\n",
      "Epoch 246/500\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8012 - precision_m: 0.7377 - f1_m: 0.7458 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7784 - val_precision_m: 0.6814 - val_f1_m: 0.6963\n",
      "Epoch 247/500\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8182 - recall_m: 0.7974 - precision_m: 0.7373 - f1_m: 0.7444 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7934 - val_precision_m: 0.6951 - val_f1_m: 0.7117\n",
      "Epoch 248/500\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8182 - recall_m: 0.8001 - precision_m: 0.7400 - f1_m: 0.7456 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7782 - val_precision_m: 0.6814 - val_f1_m: 0.7014\n",
      "Epoch 249/500\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8019 - precision_m: 0.7377 - f1_m: 0.7451 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7766 - val_precision_m: 0.6813 - val_f1_m: 0.7029\n",
      "Epoch 250/500\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7925 - precision_m: 0.7363 - f1_m: 0.7415 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7731 - val_precision_m: 0.6838 - val_f1_m: 0.7007\n",
      "Epoch 251/500\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7986 - precision_m: 0.7377 - f1_m: 0.7442 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7755 - val_precision_m: 0.6794 - val_f1_m: 0.6970\n",
      "Epoch 252/500\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00252: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7422 - auc_32: 0.8183 - recall_m: 0.8005 - precision_m: 0.7385 - f1_m: 0.7433 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7641 - val_precision_m: 0.6764 - val_f1_m: 0.6931\n",
      "Epoch 253/500\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7938 - precision_m: 0.7339 - f1_m: 0.7422 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7799 - val_precision_m: 0.6876 - val_f1_m: 0.7040\n",
      "Epoch 254/500\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7938 - precision_m: 0.7339 - f1_m: 0.7412 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7791 - val_precision_m: 0.6884 - val_f1_m: 0.7011\n",
      "Epoch 255/500\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7939 - precision_m: 0.7362 - f1_m: 0.7423 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7755 - val_precision_m: 0.6917 - val_f1_m: 0.7041\n",
      "Epoch 256/500\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7986 - precision_m: 0.7338 - f1_m: 0.7416 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7812 - val_precision_m: 0.6852 - val_f1_m: 0.7065\n",
      "Epoch 257/500\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7984 - precision_m: 0.7351 - f1_m: 0.7428 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7716 - val_precision_m: 0.6798 - val_f1_m: 0.6939\n",
      "Epoch 258/500\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7999 - precision_m: 0.7372 - f1_m: 0.7440 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7657 - val_precision_m: 0.6730 - val_f1_m: 0.6920\n",
      "Epoch 259/500\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7938 - precision_m: 0.7338 - f1_m: 0.7423 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7766 - val_precision_m: 0.6816 - val_f1_m: 0.7004\n",
      "Epoch 260/500\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7972 - precision_m: 0.7354 - f1_m: 0.7402 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7684 - val_precision_m: 0.6738 - val_f1_m: 0.6941\n",
      "Epoch 261/500\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7978 - precision_m: 0.7367 - f1_m: 0.7432 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7729 - val_precision_m: 0.6835 - val_f1_m: 0.6965\n",
      "Epoch 262/500\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7992 - precision_m: 0.7376 - f1_m: 0.7420 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7740 - val_precision_m: 0.6763 - val_f1_m: 0.6981\n",
      "Epoch 263/500\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7967 - precision_m: 0.7344 - f1_m: 0.7437 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7752 - val_precision_m: 0.6857 - val_f1_m: 0.7026\n",
      "Epoch 264/500\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8004 - precision_m: 0.7382 - f1_m: 0.7452 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7692 - val_precision_m: 0.6853 - val_f1_m: 0.7003\n",
      "Epoch 265/500\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7974 - precision_m: 0.7363 - f1_m: 0.7444 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7851 - val_precision_m: 0.6934 - val_f1_m: 0.7102\n",
      "Epoch 266/500\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7981 - precision_m: 0.7348 - f1_m: 0.7425 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7681 - val_precision_m: 0.6856 - val_f1_m: 0.7006\n",
      "Epoch 267/500\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7961 - precision_m: 0.7358 - f1_m: 0.7427 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7688 - val_precision_m: 0.6734 - val_f1_m: 0.6944\n",
      "Epoch 268/500\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7928 - precision_m: 0.7322 - f1_m: 0.7399 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7693 - val_precision_m: 0.6853 - val_f1_m: 0.7006\n",
      "Epoch 269/500\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8008 - precision_m: 0.7340 - f1_m: 0.7427 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7809 - val_precision_m: 0.6872 - val_f1_m: 0.7091\n",
      "Epoch 270/500\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7967 - precision_m: 0.7370 - f1_m: 0.7425 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7809 - val_precision_m: 0.6854 - val_f1_m: 0.7015\n",
      "Epoch 271/500\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7928 - precision_m: 0.7350 - f1_m: 0.7409 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7727 - val_precision_m: 0.6964 - val_f1_m: 0.7042\n",
      "Epoch 272/500\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7963 - precision_m: 0.7355 - f1_m: 0.7419 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7917 - val_precision_m: 0.6864 - val_f1_m: 0.7076\n",
      "Epoch 273/500\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00273: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7921 - precision_m: 0.7359 - f1_m: 0.7408 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7811 - val_precision_m: 0.6863 - val_f1_m: 0.7058\n",
      "Epoch 274/500\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7958 - precision_m: 0.7406 - f1_m: 0.7455 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7682 - val_precision_m: 0.6813 - val_f1_m: 0.6981\n",
      "Epoch 275/500\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7998 - precision_m: 0.7382 - f1_m: 0.7443 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7825 - val_precision_m: 0.6857 - val_f1_m: 0.7019\n",
      "Epoch 276/500\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7955 - precision_m: 0.7325 - f1_m: 0.7410 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7763 - val_precision_m: 0.6843 - val_f1_m: 0.7005\n",
      "Epoch 277/500\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7909 - precision_m: 0.7358 - f1_m: 0.7417 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7750 - val_precision_m: 0.6870 - val_f1_m: 0.7007\n",
      "Epoch 278/500\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7877 - precision_m: 0.7325 - f1_m: 0.7381 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7750 - val_precision_m: 0.6893 - val_f1_m: 0.7028\n",
      "Epoch 279/500\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7972 - precision_m: 0.7346 - f1_m: 0.7410 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7789 - val_precision_m: 0.6890 - val_f1_m: 0.7028\n",
      "Epoch 280/500\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7916 - precision_m: 0.7333 - f1_m: 0.7396 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7626 - val_precision_m: 0.6819 - val_f1_m: 0.6954\n",
      "Epoch 281/500\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7996 - precision_m: 0.7427 - f1_m: 0.7467 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7762 - val_precision_m: 0.6791 - val_f1_m: 0.7012\n",
      "Epoch 282/500\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7947 - precision_m: 0.7379 - f1_m: 0.7421 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7697 - val_precision_m: 0.6695 - val_f1_m: 0.6937\n",
      "Epoch 283/500\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7968 - precision_m: 0.7412 - f1_m: 0.7447 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7689 - val_precision_m: 0.6800 - val_f1_m: 0.6966\n",
      "Epoch 284/500\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7979 - precision_m: 0.7351 - f1_m: 0.7436 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7718 - val_precision_m: 0.6818 - val_f1_m: 0.6951\n",
      "Epoch 285/500\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7955 - precision_m: 0.7333 - f1_m: 0.7424 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7737 - val_precision_m: 0.6787 - val_f1_m: 0.6937\n",
      "Epoch 286/500\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7946 - precision_m: 0.7310 - f1_m: 0.7386 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7772 - val_precision_m: 0.6832 - val_f1_m: 0.6978\n",
      "Epoch 287/500\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7993 - precision_m: 0.7377 - f1_m: 0.7442 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7721 - val_precision_m: 0.6896 - val_f1_m: 0.7042\n",
      "Epoch 288/500\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7989 - precision_m: 0.7405 - f1_m: 0.7457 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7680 - val_precision_m: 0.6836 - val_f1_m: 0.6972\n",
      "Epoch 289/500\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7901 - precision_m: 0.7301 - f1_m: 0.7361 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7693 - val_precision_m: 0.6852 - val_f1_m: 0.7007\n",
      "Epoch 290/500\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7971 - precision_m: 0.7359 - f1_m: 0.7422 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7877 - val_precision_m: 0.6855 - val_f1_m: 0.7067\n",
      "Epoch 291/500\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7956 - precision_m: 0.7361 - f1_m: 0.7430 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7714 - val_precision_m: 0.6794 - val_f1_m: 0.6991\n",
      "Epoch 292/500\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7932 - precision_m: 0.7364 - f1_m: 0.7409 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7811 - val_precision_m: 0.6912 - val_f1_m: 0.7058\n",
      "Epoch 293/500\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00293: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7980 - precision_m: 0.7376 - f1_m: 0.7432 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7720 - val_precision_m: 0.6816 - val_f1_m: 0.6982\n",
      "Epoch 294/500\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7998 - precision_m: 0.7393 - f1_m: 0.7449 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7611 - val_precision_m: 0.6868 - val_f1_m: 0.7028\n",
      "Epoch 295/500\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7991 - precision_m: 0.7362 - f1_m: 0.7450 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7840 - val_precision_m: 0.6917 - val_f1_m: 0.7062\n",
      "Epoch 296/500\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7955 - precision_m: 0.7359 - f1_m: 0.7424 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7853 - val_precision_m: 0.6970 - val_f1_m: 0.7058\n",
      "Epoch 297/500\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7998 - precision_m: 0.7366 - f1_m: 0.7463 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7651 - val_precision_m: 0.6841 - val_f1_m: 0.6962\n",
      "Epoch 298/500\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7965 - precision_m: 0.7365 - f1_m: 0.7431 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7861 - val_precision_m: 0.6881 - val_f1_m: 0.7063\n",
      "Epoch 299/500\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7983 - precision_m: 0.7355 - f1_m: 0.7424 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7718 - val_precision_m: 0.6852 - val_f1_m: 0.7022\n",
      "Epoch 300/500\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7961 - precision_m: 0.7339 - f1_m: 0.7423 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7769 - val_precision_m: 0.6793 - val_f1_m: 0.6982\n",
      "Epoch 301/500\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7927 - precision_m: 0.7349 - f1_m: 0.7402 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7759 - val_precision_m: 0.6917 - val_f1_m: 0.7029\n",
      "Epoch 302/500\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7976 - precision_m: 0.7358 - f1_m: 0.7440 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7733 - val_precision_m: 0.6775 - val_f1_m: 0.6958\n",
      "Epoch 303/500\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7960 - precision_m: 0.7346 - f1_m: 0.7416 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7718 - val_precision_m: 0.6763 - val_f1_m: 0.6985\n",
      "Epoch 304/500\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7956 - precision_m: 0.7373 - f1_m: 0.7433 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7894 - val_precision_m: 0.6873 - val_f1_m: 0.7026\n",
      "Epoch 305/500\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7934 - precision_m: 0.7366 - f1_m: 0.7416 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7708 - val_precision_m: 0.6808 - val_f1_m: 0.6978\n",
      "Epoch 306/500\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7924 - precision_m: 0.7375 - f1_m: 0.7415 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7904 - val_precision_m: 0.6888 - val_f1_m: 0.7037\n",
      "Epoch 307/500\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7990 - precision_m: 0.7364 - f1_m: 0.7452 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7781 - val_precision_m: 0.6874 - val_f1_m: 0.7043\n",
      "Epoch 308/500\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7945 - precision_m: 0.7401 - f1_m: 0.7425 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7834 - val_precision_m: 0.6879 - val_f1_m: 0.7018\n",
      "Epoch 309/500\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8026 - precision_m: 0.7393 - f1_m: 0.7462 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7802 - val_precision_m: 0.6870 - val_f1_m: 0.7042\n",
      "Epoch 310/500\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7979 - precision_m: 0.7341 - f1_m: 0.7412 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7875 - val_precision_m: 0.6999 - val_f1_m: 0.7115\n",
      "Epoch 311/500\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7976 - precision_m: 0.7377 - f1_m: 0.7441 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7776 - val_precision_m: 0.6809 - val_f1_m: 0.7003\n",
      "Epoch 312/500\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7942 - precision_m: 0.7351 - f1_m: 0.7419 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7725 - val_precision_m: 0.6702 - val_f1_m: 0.6909\n",
      "Epoch 313/500\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00313: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7935 - precision_m: 0.7338 - f1_m: 0.7412 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7701 - val_precision_m: 0.6784 - val_f1_m: 0.6967\n",
      "Epoch 314/500\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7959 - precision_m: 0.7330 - f1_m: 0.7406 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7755 - val_precision_m: 0.6812 - val_f1_m: 0.7027\n",
      "Epoch 315/500\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8001 - precision_m: 0.7377 - f1_m: 0.7456 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7702 - val_precision_m: 0.6823 - val_f1_m: 0.6970\n",
      "Epoch 316/500\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7888 - precision_m: 0.7373 - f1_m: 0.7404 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7761 - val_precision_m: 0.6850 - val_f1_m: 0.7057\n",
      "Epoch 317/500\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7957 - precision_m: 0.7369 - f1_m: 0.7431 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7770 - val_precision_m: 0.6810 - val_f1_m: 0.7039\n",
      "Epoch 318/500\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7961 - precision_m: 0.7378 - f1_m: 0.7435 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7827 - val_precision_m: 0.6827 - val_f1_m: 0.7037\n",
      "Epoch 319/500\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7935 - precision_m: 0.7385 - f1_m: 0.7398 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7754 - val_precision_m: 0.6883 - val_f1_m: 0.7056\n",
      "Epoch 320/500\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7996 - precision_m: 0.7371 - f1_m: 0.7438 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7677 - val_precision_m: 0.6917 - val_f1_m: 0.7014\n",
      "Epoch 321/500\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7981 - precision_m: 0.7383 - f1_m: 0.7433 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7746 - val_precision_m: 0.6854 - val_f1_m: 0.7008\n",
      "Epoch 322/500\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7911 - precision_m: 0.7307 - f1_m: 0.7383 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7699 - val_precision_m: 0.6833 - val_f1_m: 0.6969\n",
      "Epoch 323/500\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8051 - precision_m: 0.7410 - f1_m: 0.7447 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7671 - val_precision_m: 0.6690 - val_f1_m: 0.6852\n",
      "Epoch 324/500\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7958 - precision_m: 0.7349 - f1_m: 0.7413 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7664 - val_precision_m: 0.6739 - val_f1_m: 0.6920\n",
      "Epoch 325/500\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8018 - precision_m: 0.7400 - f1_m: 0.7444 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7735 - val_precision_m: 0.6904 - val_f1_m: 0.7084\n",
      "Epoch 326/500\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7960 - precision_m: 0.7308 - f1_m: 0.7417 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7657 - val_precision_m: 0.6760 - val_f1_m: 0.6916\n",
      "Epoch 327/500\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7974 - precision_m: 0.7345 - f1_m: 0.7434 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7748 - val_precision_m: 0.6937 - val_f1_m: 0.7041\n",
      "Epoch 328/500\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7938 - precision_m: 0.7359 - f1_m: 0.7426 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7806 - val_precision_m: 0.6837 - val_f1_m: 0.7037\n",
      "Epoch 329/500\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7948 - precision_m: 0.7330 - f1_m: 0.7417 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7729 - val_precision_m: 0.6784 - val_f1_m: 0.6928\n",
      "Epoch 330/500\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7974 - precision_m: 0.7336 - f1_m: 0.7406 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7890 - val_precision_m: 0.6876 - val_f1_m: 0.7075\n",
      "Epoch 331/500\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8043 - precision_m: 0.7359 - f1_m: 0.7458 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7647 - val_precision_m: 0.6881 - val_f1_m: 0.6974\n",
      "Epoch 332/500\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7878 - precision_m: 0.7335 - f1_m: 0.7373 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7670 - val_precision_m: 0.6794 - val_f1_m: 0.6943\n",
      "Epoch 333/500\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00333: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7979 - precision_m: 0.7384 - f1_m: 0.7442 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7717 - val_precision_m: 0.6743 - val_f1_m: 0.6943\n",
      "Epoch 334/500\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7914 - precision_m: 0.7307 - f1_m: 0.7367 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7727 - val_precision_m: 0.6902 - val_f1_m: 0.7025\n",
      "Epoch 335/500\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7910 - precision_m: 0.7324 - f1_m: 0.7396 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7725 - val_precision_m: 0.6815 - val_f1_m: 0.6956\n",
      "Epoch 336/500\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8004 - precision_m: 0.7424 - f1_m: 0.7459 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7883 - val_precision_m: 0.6908 - val_f1_m: 0.7102\n",
      "Epoch 337/500\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7926 - precision_m: 0.7361 - f1_m: 0.7418 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7689 - val_precision_m: 0.6797 - val_f1_m: 0.7013\n",
      "Epoch 338/500\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7889 - precision_m: 0.7346 - f1_m: 0.7379 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7759 - val_precision_m: 0.6878 - val_f1_m: 0.7053\n",
      "Epoch 339/500\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8002 - precision_m: 0.7365 - f1_m: 0.7447 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7623 - val_precision_m: 0.6770 - val_f1_m: 0.6923\n",
      "Epoch 340/500\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7974 - precision_m: 0.7332 - f1_m: 0.7387 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7872 - val_precision_m: 0.6823 - val_f1_m: 0.7042\n",
      "Epoch 341/500\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7972 - precision_m: 0.7334 - f1_m: 0.7396 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7701 - val_precision_m: 0.6847 - val_f1_m: 0.6977\n",
      "Epoch 342/500\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8006 - precision_m: 0.7407 - f1_m: 0.7477 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7799 - val_precision_m: 0.6935 - val_f1_m: 0.7081\n",
      "Epoch 343/500\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7965 - precision_m: 0.7386 - f1_m: 0.7433 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7826 - val_precision_m: 0.6897 - val_f1_m: 0.7059\n",
      "Epoch 344/500\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7988 - precision_m: 0.7357 - f1_m: 0.7428 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7781 - val_precision_m: 0.6896 - val_f1_m: 0.7050\n",
      "Epoch 345/500\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7962 - precision_m: 0.7363 - f1_m: 0.7442 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7781 - val_precision_m: 0.6880 - val_f1_m: 0.7017\n",
      "Epoch 346/500\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7999 - precision_m: 0.7381 - f1_m: 0.7439 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7862 - val_precision_m: 0.6834 - val_f1_m: 0.7073\n",
      "Epoch 347/500\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7976 - precision_m: 0.7341 - f1_m: 0.7411 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7678 - val_precision_m: 0.6783 - val_f1_m: 0.6979\n",
      "Epoch 348/500\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7958 - precision_m: 0.7384 - f1_m: 0.7439 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7672 - val_precision_m: 0.6898 - val_f1_m: 0.7018\n",
      "Epoch 349/500\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7994 - precision_m: 0.7350 - f1_m: 0.7420 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7749 - val_precision_m: 0.6870 - val_f1_m: 0.6978\n",
      "Epoch 350/500\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7988 - precision_m: 0.7390 - f1_m: 0.7444 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7881 - val_precision_m: 0.6827 - val_f1_m: 0.7051\n",
      "Epoch 351/500\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7976 - precision_m: 0.7333 - f1_m: 0.7433 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7751 - val_precision_m: 0.6828 - val_f1_m: 0.7055\n",
      "Epoch 352/500\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7994 - precision_m: 0.7393 - f1_m: 0.7434 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7716 - val_precision_m: 0.6817 - val_f1_m: 0.7011\n",
      "Epoch 353/500\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00353: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7954 - precision_m: 0.7355 - f1_m: 0.7432 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7807 - val_precision_m: 0.6845 - val_f1_m: 0.6991\n",
      "Epoch 354/500\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8005 - precision_m: 0.7368 - f1_m: 0.7446 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7541 - val_precision_m: 0.6691 - val_f1_m: 0.6885\n",
      "Epoch 355/500\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7992 - precision_m: 0.7379 - f1_m: 0.7429 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7713 - val_precision_m: 0.6801 - val_f1_m: 0.6975\n",
      "Epoch 356/500\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7924 - precision_m: 0.7333 - f1_m: 0.7401 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7759 - val_precision_m: 0.6852 - val_f1_m: 0.7006\n",
      "Epoch 357/500\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7944 - precision_m: 0.7335 - f1_m: 0.7414 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7757 - val_precision_m: 0.6837 - val_f1_m: 0.7010\n",
      "Epoch 358/500\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7988 - precision_m: 0.7363 - f1_m: 0.7451 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7795 - val_precision_m: 0.6853 - val_f1_m: 0.7005\n",
      "Epoch 359/500\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7957 - precision_m: 0.7361 - f1_m: 0.7409 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7762 - val_precision_m: 0.6841 - val_f1_m: 0.7007\n",
      "Epoch 360/500\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8019 - precision_m: 0.7399 - f1_m: 0.7458 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7758 - val_precision_m: 0.6835 - val_f1_m: 0.6999\n",
      "Epoch 361/500\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8011 - precision_m: 0.7403 - f1_m: 0.7453 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7743 - val_precision_m: 0.6872 - val_f1_m: 0.7002\n",
      "Epoch 362/500\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7992 - precision_m: 0.7344 - f1_m: 0.7436 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7699 - val_precision_m: 0.6829 - val_f1_m: 0.7013\n",
      "Epoch 363/500\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7988 - precision_m: 0.7411 - f1_m: 0.7446 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7799 - val_precision_m: 0.6833 - val_f1_m: 0.7047\n",
      "Epoch 364/500\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7976 - precision_m: 0.7348 - f1_m: 0.7410 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7773 - val_precision_m: 0.6818 - val_f1_m: 0.7038\n",
      "Epoch 365/500\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8003 - precision_m: 0.7394 - f1_m: 0.7437 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7752 - val_precision_m: 0.6844 - val_f1_m: 0.6960\n",
      "Epoch 366/500\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7986 - precision_m: 0.7349 - f1_m: 0.7428 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7898 - val_precision_m: 0.6906 - val_f1_m: 0.7076\n",
      "Epoch 367/500\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7952 - precision_m: 0.7345 - f1_m: 0.7405 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7781 - val_precision_m: 0.6774 - val_f1_m: 0.6992\n",
      "Epoch 368/500\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7959 - precision_m: 0.7371 - f1_m: 0.7445 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7582 - val_precision_m: 0.6726 - val_f1_m: 0.6890\n",
      "Epoch 369/500\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8011 - precision_m: 0.7388 - f1_m: 0.7459 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7701 - val_precision_m: 0.6903 - val_f1_m: 0.7003\n",
      "Epoch 370/500\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7974 - precision_m: 0.7313 - f1_m: 0.7419 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7786 - val_precision_m: 0.6897 - val_f1_m: 0.7032\n",
      "Epoch 371/500\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7938 - precision_m: 0.7357 - f1_m: 0.7404 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7582 - val_precision_m: 0.6852 - val_f1_m: 0.6944\n",
      "Epoch 372/500\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7984 - precision_m: 0.7377 - f1_m: 0.7443 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7762 - val_precision_m: 0.6827 - val_f1_m: 0.7010\n",
      "Epoch 373/500\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00373: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7956 - precision_m: 0.7375 - f1_m: 0.7415 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7638 - val_precision_m: 0.6772 - val_f1_m: 0.6971\n",
      "Epoch 374/500\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7941 - precision_m: 0.7335 - f1_m: 0.7405 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7741 - val_precision_m: 0.6958 - val_f1_m: 0.7054\n",
      "Epoch 375/500\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7973 - precision_m: 0.7406 - f1_m: 0.7447 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7696 - val_precision_m: 0.6856 - val_f1_m: 0.6992\n",
      "Epoch 376/500\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7987 - precision_m: 0.7347 - f1_m: 0.7426 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7657 - val_precision_m: 0.6725 - val_f1_m: 0.6923\n",
      "Epoch 377/500\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7996 - precision_m: 0.7355 - f1_m: 0.7430 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7803 - val_precision_m: 0.6939 - val_f1_m: 0.7067\n",
      "Epoch 378/500\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7965 - precision_m: 0.7331 - f1_m: 0.7405 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7776 - val_precision_m: 0.6901 - val_f1_m: 0.7034\n",
      "Epoch 379/500\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7985 - precision_m: 0.7415 - f1_m: 0.7464 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7758 - val_precision_m: 0.6762 - val_f1_m: 0.6967\n",
      "Epoch 380/500\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7925 - precision_m: 0.7323 - f1_m: 0.7395 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7786 - val_precision_m: 0.6829 - val_f1_m: 0.7034\n",
      "Epoch 381/500\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7936 - precision_m: 0.7376 - f1_m: 0.7408 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7868 - val_precision_m: 0.6971 - val_f1_m: 0.7076\n",
      "Epoch 382/500\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7980 - precision_m: 0.7320 - f1_m: 0.7413 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7688 - val_precision_m: 0.6834 - val_f1_m: 0.6966\n",
      "Epoch 383/500\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7947 - precision_m: 0.7359 - f1_m: 0.7426 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7772 - val_precision_m: 0.6891 - val_f1_m: 0.7039\n",
      "Epoch 384/500\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7963 - precision_m: 0.7370 - f1_m: 0.7449 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7670 - val_precision_m: 0.6860 - val_f1_m: 0.7000\n",
      "Epoch 385/500\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8031 - precision_m: 0.7375 - f1_m: 0.7454 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7742 - val_precision_m: 0.6788 - val_f1_m: 0.7000\n",
      "Epoch 386/500\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7976 - precision_m: 0.7396 - f1_m: 0.7421 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7731 - val_precision_m: 0.6819 - val_f1_m: 0.7015\n",
      "Epoch 387/500\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7914 - precision_m: 0.7286 - f1_m: 0.7379 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7769 - val_precision_m: 0.6844 - val_f1_m: 0.7025\n",
      "Epoch 388/500\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8007 - precision_m: 0.7369 - f1_m: 0.7451 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7808 - val_precision_m: 0.6874 - val_f1_m: 0.7017\n",
      "Epoch 389/500\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7943 - precision_m: 0.7350 - f1_m: 0.7417 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7761 - val_precision_m: 0.6875 - val_f1_m: 0.7040\n",
      "Epoch 390/500\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7912 - precision_m: 0.7298 - f1_m: 0.7382 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7711 - val_precision_m: 0.6828 - val_f1_m: 0.6954\n",
      "Epoch 391/500\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7955 - precision_m: 0.7365 - f1_m: 0.7429 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7721 - val_precision_m: 0.6857 - val_f1_m: 0.7003\n",
      "Epoch 392/500\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7973 - precision_m: 0.7348 - f1_m: 0.7432 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7928 - val_precision_m: 0.6911 - val_f1_m: 0.7119\n",
      "Epoch 393/500\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00393: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7967 - precision_m: 0.7393 - f1_m: 0.7447 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7673 - val_precision_m: 0.6769 - val_f1_m: 0.6960\n",
      "Epoch 394/500\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7943 - precision_m: 0.7334 - f1_m: 0.7392 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7771 - val_precision_m: 0.6834 - val_f1_m: 0.7044\n",
      "Epoch 395/500\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7970 - precision_m: 0.7373 - f1_m: 0.7440 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7761 - val_precision_m: 0.6708 - val_f1_m: 0.6936\n",
      "Epoch 396/500\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7942 - precision_m: 0.7350 - f1_m: 0.7414 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7744 - val_precision_m: 0.6811 - val_f1_m: 0.6996\n",
      "Epoch 397/500\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8001 - precision_m: 0.7384 - f1_m: 0.7443 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7741 - val_precision_m: 0.6859 - val_f1_m: 0.6975\n",
      "Epoch 398/500\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7944 - precision_m: 0.7322 - f1_m: 0.7407 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7756 - val_precision_m: 0.6807 - val_f1_m: 0.6996\n",
      "Epoch 399/500\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7979 - precision_m: 0.7341 - f1_m: 0.7431 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7792 - val_precision_m: 0.6914 - val_f1_m: 0.7088\n",
      "Epoch 400/500\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8020 - precision_m: 0.7408 - f1_m: 0.7450 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7630 - val_precision_m: 0.6827 - val_f1_m: 0.6967\n",
      "Epoch 401/500\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7934 - precision_m: 0.7346 - f1_m: 0.7425 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7711 - val_precision_m: 0.6801 - val_f1_m: 0.7016\n",
      "Epoch 402/500\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7950 - precision_m: 0.7364 - f1_m: 0.7434 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7681 - val_precision_m: 0.6784 - val_f1_m: 0.6968\n",
      "Epoch 403/500\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7997 - precision_m: 0.7375 - f1_m: 0.7447 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7626 - val_precision_m: 0.6791 - val_f1_m: 0.6971\n",
      "Epoch 404/500\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7972 - precision_m: 0.7385 - f1_m: 0.7441 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7905 - val_precision_m: 0.6926 - val_f1_m: 0.7066\n",
      "Epoch 405/500\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8018 - precision_m: 0.7356 - f1_m: 0.7447 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7841 - val_precision_m: 0.6797 - val_f1_m: 0.7074\n",
      "Epoch 406/500\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8015 - precision_m: 0.7385 - f1_m: 0.7451 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7654 - val_precision_m: 0.6710 - val_f1_m: 0.6903\n",
      "Epoch 407/500\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7951 - precision_m: 0.7378 - f1_m: 0.7425 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7772 - val_precision_m: 0.6976 - val_f1_m: 0.7062\n",
      "Epoch 408/500\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7940 - precision_m: 0.7335 - f1_m: 0.7429 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7704 - val_precision_m: 0.6729 - val_f1_m: 0.6951\n",
      "Epoch 409/500\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7953 - precision_m: 0.7390 - f1_m: 0.7449 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7707 - val_precision_m: 0.6838 - val_f1_m: 0.7010\n",
      "Epoch 410/500\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7970 - precision_m: 0.7360 - f1_m: 0.7429 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7697 - val_precision_m: 0.6830 - val_f1_m: 0.7004\n",
      "Epoch 411/500\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7924 - precision_m: 0.7369 - f1_m: 0.7413 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7623 - val_precision_m: 0.6752 - val_f1_m: 0.6963\n",
      "Epoch 412/500\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7938 - precision_m: 0.7352 - f1_m: 0.7391 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7794 - val_precision_m: 0.6837 - val_f1_m: 0.7007\n",
      "Epoch 413/500\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00413: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7922 - precision_m: 0.7324 - f1_m: 0.7417 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7864 - val_precision_m: 0.6879 - val_f1_m: 0.7100\n",
      "Epoch 414/500\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7970 - precision_m: 0.7360 - f1_m: 0.7415 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7827 - val_precision_m: 0.6868 - val_f1_m: 0.7011\n",
      "Epoch 415/500\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7949 - precision_m: 0.7346 - f1_m: 0.7420 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7657 - val_precision_m: 0.6857 - val_f1_m: 0.6980\n",
      "Epoch 416/500\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7996 - precision_m: 0.7354 - f1_m: 0.7432 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7807 - val_precision_m: 0.6753 - val_f1_m: 0.6939\n",
      "Epoch 417/500\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7936 - precision_m: 0.7351 - f1_m: 0.7399 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7690 - val_precision_m: 0.6638 - val_f1_m: 0.6879\n",
      "Epoch 418/500\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7993 - precision_m: 0.7363 - f1_m: 0.7441 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7847 - val_precision_m: 0.6901 - val_f1_m: 0.7087\n",
      "Epoch 419/500\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7969 - precision_m: 0.7332 - f1_m: 0.7400 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7759 - val_precision_m: 0.6874 - val_f1_m: 0.6998\n",
      "Epoch 420/500\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7983 - precision_m: 0.7412 - f1_m: 0.7454 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7778 - val_precision_m: 0.6849 - val_f1_m: 0.7034\n",
      "Epoch 421/500\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7963 - precision_m: 0.7349 - f1_m: 0.7428 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7681 - val_precision_m: 0.6748 - val_f1_m: 0.6959\n",
      "Epoch 422/500\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7987 - precision_m: 0.7346 - f1_m: 0.7440 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7800 - val_precision_m: 0.6902 - val_f1_m: 0.7037\n",
      "Epoch 423/500\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7993 - precision_m: 0.7367 - f1_m: 0.7431 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7763 - val_precision_m: 0.6788 - val_f1_m: 0.6980\n",
      "Epoch 424/500\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7971 - precision_m: 0.7349 - f1_m: 0.7407 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7752 - val_precision_m: 0.6818 - val_f1_m: 0.7009\n",
      "Epoch 425/500\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7871 - precision_m: 0.7263 - f1_m: 0.7354 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7823 - val_precision_m: 0.6865 - val_f1_m: 0.7019\n",
      "Epoch 426/500\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8007 - precision_m: 0.7397 - f1_m: 0.7466 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7755 - val_precision_m: 0.6861 - val_f1_m: 0.7005\n",
      "Epoch 427/500\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7946 - precision_m: 0.7362 - f1_m: 0.7420 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7820 - val_precision_m: 0.6809 - val_f1_m: 0.7014\n",
      "Epoch 428/500\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7986 - precision_m: 0.7372 - f1_m: 0.7451 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7650 - val_precision_m: 0.6878 - val_f1_m: 0.6908\n",
      "Epoch 429/500\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7934 - precision_m: 0.7347 - f1_m: 0.7412 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7774 - val_precision_m: 0.6831 - val_f1_m: 0.7050\n",
      "Epoch 430/500\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7960 - precision_m: 0.7335 - f1_m: 0.7401 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7753 - val_precision_m: 0.6854 - val_f1_m: 0.7046\n",
      "Epoch 431/500\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7965 - precision_m: 0.7348 - f1_m: 0.7412 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7605 - val_precision_m: 0.6763 - val_f1_m: 0.6947\n",
      "Epoch 432/500\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7951 - precision_m: 0.7353 - f1_m: 0.7412 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7731 - val_precision_m: 0.6850 - val_f1_m: 0.6990\n",
      "Epoch 433/500\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00433: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7982 - precision_m: 0.7387 - f1_m: 0.7442 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7628 - val_precision_m: 0.6811 - val_f1_m: 0.6971\n",
      "Epoch 434/500\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7988 - precision_m: 0.7399 - f1_m: 0.7458 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7768 - val_precision_m: 0.6842 - val_f1_m: 0.7019\n",
      "Epoch 435/500\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8024 - precision_m: 0.7384 - f1_m: 0.7460 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7856 - val_precision_m: 0.6894 - val_f1_m: 0.7034\n",
      "Epoch 436/500\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7941 - precision_m: 0.7366 - f1_m: 0.7425 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7614 - val_precision_m: 0.6766 - val_f1_m: 0.6938\n",
      "Epoch 437/500\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7938 - precision_m: 0.7355 - f1_m: 0.7416 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7623 - val_precision_m: 0.6823 - val_f1_m: 0.6930\n",
      "Epoch 438/500\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7936 - precision_m: 0.7373 - f1_m: 0.7400 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7670 - val_precision_m: 0.6856 - val_f1_m: 0.6988\n",
      "Epoch 439/500\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7969 - precision_m: 0.7360 - f1_m: 0.7424 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7713 - val_precision_m: 0.6808 - val_f1_m: 0.7028\n",
      "Epoch 440/500\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7989 - precision_m: 0.7324 - f1_m: 0.7429 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7701 - val_precision_m: 0.6830 - val_f1_m: 0.6941\n",
      "Epoch 441/500\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7998 - precision_m: 0.7391 - f1_m: 0.7444 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7809 - val_precision_m: 0.6817 - val_f1_m: 0.6965\n",
      "Epoch 442/500\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7950 - precision_m: 0.7321 - f1_m: 0.7400 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7486 - val_precision_m: 0.6701 - val_f1_m: 0.6838\n",
      "Epoch 443/500\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7979 - precision_m: 0.7347 - f1_m: 0.7421 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7599 - val_precision_m: 0.6872 - val_f1_m: 0.6936\n",
      "Epoch 444/500\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7969 - precision_m: 0.7326 - f1_m: 0.7427 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7887 - val_precision_m: 0.6959 - val_f1_m: 0.7115\n",
      "Epoch 445/500\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7964 - precision_m: 0.7358 - f1_m: 0.7423 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7793 - val_precision_m: 0.6819 - val_f1_m: 0.7007\n",
      "Epoch 446/500\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7935 - precision_m: 0.7383 - f1_m: 0.7421 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7750 - val_precision_m: 0.6761 - val_f1_m: 0.6992\n",
      "Epoch 447/500\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7953 - precision_m: 0.7374 - f1_m: 0.7437 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7774 - val_precision_m: 0.6912 - val_f1_m: 0.7028\n",
      "Epoch 448/500\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8007 - precision_m: 0.7371 - f1_m: 0.7462 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7695 - val_precision_m: 0.6810 - val_f1_m: 0.6978\n",
      "Epoch 449/500\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7996 - precision_m: 0.7371 - f1_m: 0.7438 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7815 - val_precision_m: 0.6916 - val_f1_m: 0.7070\n",
      "Epoch 450/500\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7953 - precision_m: 0.7330 - f1_m: 0.7404 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7806 - val_precision_m: 0.6779 - val_f1_m: 0.7029\n",
      "Epoch 451/500\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7917 - precision_m: 0.7304 - f1_m: 0.7376 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7661 - val_precision_m: 0.6768 - val_f1_m: 0.6971\n",
      "Epoch 452/500\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8012 - precision_m: 0.7355 - f1_m: 0.7428 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7710 - val_precision_m: 0.6839 - val_f1_m: 0.7025\n",
      "Epoch 453/500\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00453: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7997 - precision_m: 0.7352 - f1_m: 0.7430 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7735 - val_precision_m: 0.6798 - val_f1_m: 0.7021\n",
      "Epoch 454/500\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7972 - precision_m: 0.7360 - f1_m: 0.7446 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7810 - val_precision_m: 0.6929 - val_f1_m: 0.7062\n",
      "Epoch 455/500\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7976 - precision_m: 0.7357 - f1_m: 0.7409 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7739 - val_precision_m: 0.6893 - val_f1_m: 0.7025\n",
      "Epoch 456/500\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7979 - precision_m: 0.7340 - f1_m: 0.7419 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7726 - val_precision_m: 0.6785 - val_f1_m: 0.6977\n",
      "Epoch 457/500\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7951 - precision_m: 0.7358 - f1_m: 0.7436 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7655 - val_precision_m: 0.6894 - val_f1_m: 0.7007\n",
      "Epoch 458/500\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7924 - precision_m: 0.7309 - f1_m: 0.7394 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7901 - val_precision_m: 0.6793 - val_f1_m: 0.7023\n",
      "Epoch 459/500\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7978 - precision_m: 0.7405 - f1_m: 0.7448 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7832 - val_precision_m: 0.6795 - val_f1_m: 0.6987\n",
      "Epoch 460/500\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7947 - precision_m: 0.7330 - f1_m: 0.7404 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7783 - val_precision_m: 0.6903 - val_f1_m: 0.7046\n",
      "Epoch 461/500\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8007 - precision_m: 0.7316 - f1_m: 0.7415 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7857 - val_precision_m: 0.6856 - val_f1_m: 0.7045\n",
      "Epoch 462/500\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8013 - precision_m: 0.7401 - f1_m: 0.7443 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7743 - val_precision_m: 0.6945 - val_f1_m: 0.7100\n",
      "Epoch 463/500\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7986 - precision_m: 0.7370 - f1_m: 0.7433 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7777 - val_precision_m: 0.6912 - val_f1_m: 0.7038\n",
      "Epoch 464/500\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8042 - precision_m: 0.7416 - f1_m: 0.7461 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7788 - val_precision_m: 0.6868 - val_f1_m: 0.7049\n",
      "Epoch 465/500\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7945 - precision_m: 0.7383 - f1_m: 0.7435 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7773 - val_precision_m: 0.6848 - val_f1_m: 0.7001\n",
      "Epoch 466/500\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7946 - precision_m: 0.7328 - f1_m: 0.7405 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7738 - val_precision_m: 0.6787 - val_f1_m: 0.6991\n",
      "Epoch 467/500\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7954 - precision_m: 0.7332 - f1_m: 0.7389 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7735 - val_precision_m: 0.6901 - val_f1_m: 0.7028\n",
      "Epoch 468/500\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7932 - precision_m: 0.7352 - f1_m: 0.7408 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7801 - val_precision_m: 0.6894 - val_f1_m: 0.7071\n",
      "Epoch 469/500\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7929 - precision_m: 0.7319 - f1_m: 0.7408 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7694 - val_precision_m: 0.6811 - val_f1_m: 0.7006\n",
      "Epoch 470/500\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7985 - precision_m: 0.7333 - f1_m: 0.7413 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7749 - val_precision_m: 0.6925 - val_f1_m: 0.7053\n",
      "Epoch 471/500\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8006 - precision_m: 0.7361 - f1_m: 0.7446 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7724 - val_precision_m: 0.6768 - val_f1_m: 0.6947\n",
      "Epoch 472/500\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7951 - precision_m: 0.7366 - f1_m: 0.7396 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7746 - val_precision_m: 0.6890 - val_f1_m: 0.7056\n",
      "Epoch 473/500\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00473: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7972 - precision_m: 0.7388 - f1_m: 0.7442 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7594 - val_precision_m: 0.6730 - val_f1_m: 0.6922\n",
      "Epoch 474/500\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8041 - precision_m: 0.7401 - f1_m: 0.7495 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7570 - val_precision_m: 0.6819 - val_f1_m: 0.6947\n",
      "Epoch 475/500\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7961 - precision_m: 0.7356 - f1_m: 0.7413 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7802 - val_precision_m: 0.6871 - val_f1_m: 0.7060\n",
      "Epoch 476/500\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8010 - precision_m: 0.7378 - f1_m: 0.7451 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7812 - val_precision_m: 0.6844 - val_f1_m: 0.7035\n",
      "Epoch 477/500\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8016 - precision_m: 0.7376 - f1_m: 0.7443 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7804 - val_precision_m: 0.6850 - val_f1_m: 0.6997\n",
      "Epoch 478/500\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7963 - precision_m: 0.7368 - f1_m: 0.7431 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7833 - val_precision_m: 0.6894 - val_f1_m: 0.7012\n",
      "Epoch 479/500\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8021 - precision_m: 0.7391 - f1_m: 0.7464 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7710 - val_precision_m: 0.6782 - val_f1_m: 0.6962\n",
      "Epoch 480/500\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7963 - precision_m: 0.7311 - f1_m: 0.7401 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7699 - val_precision_m: 0.6841 - val_f1_m: 0.6983\n",
      "Epoch 481/500\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7975 - precision_m: 0.7370 - f1_m: 0.7424 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7703 - val_precision_m: 0.6810 - val_f1_m: 0.6972\n",
      "Epoch 482/500\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8000 - precision_m: 0.7335 - f1_m: 0.7420 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7774 - val_precision_m: 0.6939 - val_f1_m: 0.7084\n",
      "Epoch 483/500\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8064 - precision_m: 0.7431 - f1_m: 0.7495 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7879 - val_precision_m: 0.6965 - val_f1_m: 0.7093\n",
      "Epoch 484/500\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7959 - precision_m: 0.7352 - f1_m: 0.7430 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7785 - val_precision_m: 0.6908 - val_f1_m: 0.7102\n",
      "Epoch 485/500\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7947 - precision_m: 0.7378 - f1_m: 0.7407 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7734 - val_precision_m: 0.6920 - val_f1_m: 0.7030\n",
      "Epoch 486/500\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8010 - precision_m: 0.7404 - f1_m: 0.7456 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7557 - val_precision_m: 0.6837 - val_f1_m: 0.6952\n",
      "Epoch 487/500\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7981 - precision_m: 0.7378 - f1_m: 0.7430 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7767 - val_precision_m: 0.6934 - val_f1_m: 0.7068\n",
      "Epoch 488/500\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7956 - precision_m: 0.7378 - f1_m: 0.7419 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7806 - val_precision_m: 0.6894 - val_f1_m: 0.7049\n",
      "Epoch 489/500\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7988 - precision_m: 0.7395 - f1_m: 0.7447 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7809 - val_precision_m: 0.6886 - val_f1_m: 0.7026\n",
      "Epoch 490/500\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7946 - precision_m: 0.7370 - f1_m: 0.7405 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7781 - val_precision_m: 0.6742 - val_f1_m: 0.6946\n",
      "Epoch 491/500\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.8005 - precision_m: 0.7402 - f1_m: 0.7456 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7781 - val_precision_m: 0.6845 - val_f1_m: 0.7020\n",
      "Epoch 492/500\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.53742\n",
      "6694/6694 - 2s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7988 - precision_m: 0.7326 - f1_m: 0.7403 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7690 - val_precision_m: 0.6827 - val_f1_m: 0.7019\n",
      "Epoch 493/500\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.53742\n",
      "\n",
      "Epoch 00493: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7972 - precision_m: 0.7394 - f1_m: 0.7458 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7870 - val_precision_m: 0.6834 - val_f1_m: 0.7092\n",
      "Epoch 494/500\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7984 - precision_m: 0.7407 - f1_m: 0.7453 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7792 - val_precision_m: 0.6894 - val_f1_m: 0.7035\n",
      "Epoch 495/500\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7958 - precision_m: 0.7353 - f1_m: 0.7424 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7699 - val_precision_m: 0.6822 - val_f1_m: 0.6963\n",
      "Epoch 496/500\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7943 - precision_m: 0.7352 - f1_m: 0.7416 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7828 - val_precision_m: 0.6867 - val_f1_m: 0.7034\n",
      "Epoch 497/500\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7934 - precision_m: 0.7367 - f1_m: 0.7410 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7731 - val_precision_m: 0.6885 - val_f1_m: 0.6970\n",
      "Epoch 498/500\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7979 - precision_m: 0.7390 - f1_m: 0.7447 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7792 - val_precision_m: 0.6851 - val_f1_m: 0.7050\n",
      "Epoch 499/500\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7949 - precision_m: 0.7351 - f1_m: 0.7428 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7699 - val_precision_m: 0.6909 - val_f1_m: 0.7023\n",
      "Epoch 500/500\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.53742\n",
      "6694/6694 - 1s - loss: 0.5209 - acc: 0.7423 - auc_32: 0.8183 - recall_m: 0.7942 - precision_m: 0.7335 - f1_m: 0.7390 - val_loss: 0.5509 - val_acc: 0.7180 - val_auc_32: 0.8015 - val_recall_m: 0.7783 - val_precision_m: 0.6803 - val_f1_m: 0.7021\n",
      "2615/2615 [==============================] - 0s 50us/sample - loss: 0.5397 - acc: 0.7281 - auc_32: 0.8024 - recall_m: 0.7897 - precision_m: 0.7280 - f1_m: 0.7531\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_name = \"FF_ori\"\n",
    "EPOCHS = 200\n",
    "PATIENCE = 200\n",
    "BATCH_SIZE = 8\n",
    "HIDDEN_LAYERS = [10]\n",
    "ACTIVATION = 'relu'\n",
    "L_RATE = 0.001\n",
    "model_name = '{}_ep{}_pa{}_bs{}_hs{}_lr_{}_{}'.format(base_model_name,EPOCHS,PATIENCE,BATCH_SIZE,HIDDEN_LAYERS,L_RATE,ACTIVATION)\n",
    "model_name\n",
    "model = get_FFNN_model(X_train, y_onehot, HIDDEN_LAYERS)\n",
    "\n",
    "model_path = os.path.join(RESULT_PATH, model_name)\n",
    "\n",
    "forge_gen = True\n",
    "\n",
    "if not os.path.exists(model_path) or forge_gen:\n",
    "    history = net_train(model, model_path, X_train, y_train, X_test, y_test, EPOCHS, save_model=True, VERBOSE=2)    \n",
    "    \n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    plt.figure(figsize=(14,6))\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.history[key], label=key)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=.2)\n",
    "    plt.title(f'batch_size = {BATCH_SIZE}, epochs = {EPOCHS}')\n",
    "    plt.draw()\n",
    "else:\n",
    "    print('Model loaded.')\n",
    "    model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: \n",
      "acc: 73.75%\n",
      "Test Set: \n",
      "acc: 72.81%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 808,  416],\n",
       "       [ 295, 1096]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores = model.evaluate(X_onehot_train.values, y_onehot_train)\n",
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Training Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# scores = model.evaluate(X_onehot_test.values, y_onehot_test)\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Test Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "confusion_matrix(y_test, model.predict_classes(X_test), sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x00000183886EF288>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838C4EC288>]\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_38 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2092/2092 [==============================] - 0s 38us/sample - loss: 0.5592 - acc: 0.7247 - auc_34: 0.7910 - recall_m: 0.7631 - precision_m: 0.6984 - f1_m: 0.7196\n",
      "2\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x000001838C4E8F48>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838F7A5DC8>]\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2092/2092 [==============================] - 0s 43us/sample - loss: 0.5719 - acc: 0.7132 - auc_36: 0.7775 - recall_m: 0.7225 - precision_m: 0.6703 - f1_m: 0.6862\n",
      "3\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x0000018381BD3D48>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838F9AD608>]\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2092/2092 [==============================] - 0s 34us/sample - loss: 0.5555 - acc: 0.7199 - auc_38: 0.7917 - recall_m: 0.7338 - precision_m: 0.6936 - f1_m: 0.7023\n",
      "4\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x000001838C4E8988>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838FA1E1C8>]\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2092/2092 [==============================] - 0s 63us/sample - loss: 0.5660 - acc: 0.7137 - auc_40: 0.7800 - recall_m: 0.7588 - precision_m: 0.6839 - f1_m: 0.7083\n",
      "5\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x000001838C28FF48>, <tensorflow.python.keras.layers.core.Dense object at 0x0000018390BAB5C8>]\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2091/2091 [==============================] - 0s 46us/sample - loss: 0.5667 - acc: 0.7207 - auc_42: 0.7863 - recall_m: 0.7536 - precision_m: 0.6827 - f1_m: 0.7083\n",
      "\u001b[1m\n",
      "Mean Accuracy=  0.7184 , Sd=  0.0044\n",
      "Mean AUC=  0.7853 , Sd=  0.0057\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run_kf = True\n",
    "if run_kf:\n",
    "    n_folds = 5\n",
    "    kf = KFold(n_splits = n_folds, random_state = 1, shuffle = True)\n",
    "    \n",
    "    fold = 0\n",
    "    result_metrics_list = []\n",
    "    for train_idx, val_idx in kf.split(X1):\n",
    "        fold += 1\n",
    "        print(fold)\n",
    "        X_train = X1.iloc[train_idx]\n",
    "        y_train = y_onehot.iloc[train_idx]\n",
    "        X_val = X1.iloc[val_idx]\n",
    "        y_val = y_onehot.iloc[val_idx]\n",
    "\n",
    "        model = get_FFNN_model(X_train, y_train, HIDDEN_LAYERS)\n",
    "        history = net_train(model, model_path, X_train, y_train, X_val, y_val, EPOCHS, save_model=False, VERBOSE=False)    \n",
    "        score = model.evaluate(X_val, y_val)\n",
    "      \n",
    "        result_metrics_list.append([score])\n",
    "        \n",
    "    print(\"\\033[1m\")\n",
    "    print(\"Mean Accuracy= \",\"{:.4}\".format(np.mean(result_metrics_list,axis=0)[0][1]),\", Sd= \",\"{:.2}\".format(np.std(result_metrics_list,axis=0)[0][1]))\n",
    "    print(\"Mean AUC= \",\"{:.4}\".format(np.mean(result_metrics_list,axis=0)[0][2]),\", Sd= \",\"{:.2}\".format(np.std(result_metrics_list, axis=0)[0][2]))\n",
    "    print(\"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prepped data (WoE from Rudin)\n",
    "- Normal Scaling of numeric variables: f_load_data(1)\n",
    "- Binning (following Rudin) and one hot encoding: f_load_data(2)\n",
    "- Binning and applying WOE, calculating WOE on Rudin's bins: f_load_data(3)\n",
    "- Binning and applying WOE, following Rudin: f_load_data(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning and applying WOE, following Rudin (prep_option = 4)\n",
      "Target: Bad (y=1)\n",
      "Bad     5459\n",
      "Good    5000\n",
      "Name: RiskPerformance, dtype: int64\n",
      "[[   0 5000]\n",
      " [   1 5459]]\n",
      "X shape: (10459, 23)\n",
      "(7844, 23) (2615, 23) (7844, 1) (2615, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExternalRiskEstimate_bin_WOE</th>\n",
       "      <th>MSinceOldestTradeOpen_bin_WOE</th>\n",
       "      <th>MSinceMostRecentTradeOpen_bin_WOE</th>\n",
       "      <th>AverageMInFile_bin_WOE</th>\n",
       "      <th>NumSatisfactoryTrades_bin_WOE</th>\n",
       "      <th>NumTrades60Ever2DerogPubRec_bin_WOE</th>\n",
       "      <th>NumTrades90Ever2DerogPubRec_bin_WOE</th>\n",
       "      <th>NumTotalTrades_bin_WOE</th>\n",
       "      <th>NumTradesOpeninLast12M_bin_WOE</th>\n",
       "      <th>PercentTradesNeverDelq_bin_WOE</th>\n",
       "      <th>...</th>\n",
       "      <th>PercentInstallTrades_bin_WOE</th>\n",
       "      <th>NetFractionInstallBurden_bin_WOE</th>\n",
       "      <th>NumInstallTradesWBalance_bin_WOE</th>\n",
       "      <th>MSinceMostRecentInqexcl7days_bin_WOE</th>\n",
       "      <th>NumInqLast6M_bin_WOE</th>\n",
       "      <th>NumInqLast6Mexcl7days_bin_WOE</th>\n",
       "      <th>NetFractionRevolvingBurden_bin_WOE</th>\n",
       "      <th>NumRevolvingTradesWBalance_bin_WOE</th>\n",
       "      <th>NumBank2NatlTradesWHighUtilization_bin_WOE</th>\n",
       "      <th>PercentTradesWBalance_bin_WOE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.799</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.952</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>1.012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.242</td>\n",
       "      <td>1.223</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>-0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.799</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1.238</td>\n",
       "      <td>1.999</td>\n",
       "      <td>0.952</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1.223</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.739</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>0.601</td>\n",
       "      <td>-0.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.017</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1.238</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.242</td>\n",
       "      <td>1.223</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.263</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.017</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.377</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.313</td>\n",
       "      <td>1.223</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.094</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.539</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.116</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.242</td>\n",
       "      <td>1.223</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.633</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.601</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ExternalRiskEstimate_bin_WOE  MSinceOldestTradeOpen_bin_WOE  \\\n",
       "0                         1.799                          0.086   \n",
       "1                         1.799                          0.549   \n",
       "2                         1.017                          0.549   \n",
       "3                         1.017                          0.086   \n",
       "4                        -1.094                         -0.148   \n",
       "\n",
       "   MSinceMostRecentTradeOpen_bin_WOE  AverageMInFile_bin_WOE  \\\n",
       "0                              0.083                   0.269   \n",
       "1                              0.083                   1.238   \n",
       "2                              0.083                   1.238   \n",
       "3                              0.083                   0.269   \n",
       "4                              0.000                   0.000   \n",
       "\n",
       "   NumSatisfactoryTrades_bin_WOE  NumTrades60Ever2DerogPubRec_bin_WOE  \\\n",
       "0                          0.166                                0.952   \n",
       "1                          1.999                                0.952   \n",
       "2                          0.539                               -0.021   \n",
       "3                         -0.086                               -0.021   \n",
       "4                          0.539                               -0.021   \n",
       "\n",
       "   NumTrades90Ever2DerogPubRec_bin_WOE  NumTotalTrades_bin_WOE  \\\n",
       "0                               -0.021                  -0.097   \n",
       "1                               -0.053                   0.535   \n",
       "2                               -0.021                   0.535   \n",
       "3                               -0.021                  -0.377   \n",
       "4                               -0.021                   0.116   \n",
       "\n",
       "   NumTradesOpeninLast12M_bin_WOE  PercentTradesNeverDelq_bin_WOE  ...  \\\n",
       "0                          -0.021                           1.012  ...   \n",
       "1                          -0.021                          -0.147  ...   \n",
       "2                           0.428                          -0.147  ...   \n",
       "3                           0.287                           0.366  ...   \n",
       "4                          -0.021                          -0.147  ...   \n",
       "\n",
       "   PercentInstallTrades_bin_WOE  NetFractionInstallBurden_bin_WOE  \\\n",
       "0                        -0.503                             0.047   \n",
       "1                         0.161                             0.047   \n",
       "2                        -0.503                             0.147   \n",
       "3                        -0.145                             0.363   \n",
       "4                        -0.620                             0.363   \n",
       "\n",
       "   NumInstallTradesWBalance_bin_WOE  MSinceMostRecentInqexcl7days_bin_WOE  \\\n",
       "0                             0.242                                 1.223   \n",
       "1                             0.256                                 1.223   \n",
       "2                             0.242                                 1.223   \n",
       "3                             0.313                                 1.223   \n",
       "4                             0.242                                 1.223   \n",
       "\n",
       "   NumInqLast6M_bin_WOE  NumInqLast6Mexcl7days_bin_WOE  \\\n",
       "0                -0.047                         -0.051   \n",
       "1                -0.047                         -0.051   \n",
       "2                 0.170                          0.021   \n",
       "3                 0.471                          0.021   \n",
       "4                -0.047                         -0.051   \n",
       "\n",
       "   NetFractionRevolvingBurden_bin_WOE  NumRevolvingTradesWBalance_bin_WOE  \\\n",
       "0                              -0.088                               0.034   \n",
       "1                              -0.739                              -0.188   \n",
       "2                               0.633                              -0.263   \n",
       "3                               0.633                              -0.150   \n",
       "4                               0.633                              -0.188   \n",
       "\n",
       "   NumBank2NatlTradesWHighUtilization_bin_WOE  PercentTradesWBalance_bin_WOE  \n",
       "0                                      -0.601                         -0.130  \n",
       "1                                       0.601                         -0.982  \n",
       "2                                      -0.601                          0.203  \n",
       "3                                       0.541                          0.772  \n",
       "4                                      -0.601                          0.203  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, X1, y, y_onehot = f_load_data(4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y_onehot, test_size = .25, random_state = 2020, shuffle = True)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "X1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build same model and assess impact of woe binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.layers.core.Dense object at 0x0000018390BAA788>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838E62DE88>]\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6275 samples, validate on 1569 samples\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56404, saving model to ./results\\FF_WOE_ep200_pa200_bs8_hs[10]_lr_0.001_relu\n",
      "6275/6275 - 4s - loss: 0.5892 - acc: 0.6872 - auc_46: 0.7531 - recall_m: 0.7429 - precision_m: 0.6546 - f1_m: 0.6678 - val_loss: 0.5640 - val_acc: 0.7004 - val_auc_46: 0.7871 - val_recall_m: 0.8009 - val_precision_m: 0.6736 - val_f1_m: 0.7081\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56404 to 0.55162, saving model to ./results\\FF_WOE_ep200_pa200_bs8_hs[10]_lr_0.001_relu\n",
      "6275/6275 - 2s - loss: 0.5496 - acc: 0.7273 - auc_46: 0.7957 - recall_m: 0.8071 - precision_m: 0.7098 - f1_m: 0.7332 - val_loss: 0.5516 - val_acc: 0.7189 - val_auc_46: 0.7959 - val_recall_m: 0.7873 - val_precision_m: 0.6847 - val_f1_m: 0.7113\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55162 to 0.54551, saving model to ./results\\FF_WOE_ep200_pa200_bs8_hs[10]_lr_0.001_relu\n",
      "6275/6275 - 2s - loss: 0.5432 - acc: 0.7337 - auc_46: 0.8006 - recall_m: 0.7958 - precision_m: 0.7135 - f1_m: 0.7296 - val_loss: 0.5455 - val_acc: 0.7189 - val_auc_46: 0.8001 - val_recall_m: 0.7724 - val_precision_m: 0.6955 - val_f1_m: 0.7069\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54551 to 0.54293, saving model to ./results\\FF_WOE_ep200_pa200_bs8_hs[10]_lr_0.001_relu\n",
      "6275/6275 - 2s - loss: 0.5397 - acc: 0.7345 - auc_46: 0.8031 - recall_m: 0.7961 - precision_m: 0.7277 - f1_m: 0.7345 - val_loss: 0.5429 - val_acc: 0.7221 - val_auc_46: 0.8030 - val_recall_m: 0.7982 - val_precision_m: 0.6889 - val_f1_m: 0.7166\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54293 to 0.54290, saving model to ./results\\FF_WOE_ep200_pa200_bs8_hs[10]_lr_0.001_relu\n",
      "6275/6275 - 2s - loss: 0.5380 - acc: 0.7350 - auc_46: 0.8047 - recall_m: 0.8008 - precision_m: 0.7202 - f1_m: 0.7317 - val_loss: 0.5429 - val_acc: 0.7221 - val_auc_46: 0.8042 - val_recall_m: 0.8080 - val_precision_m: 0.6915 - val_f1_m: 0.7186\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.54290 to 0.54079, saving model to ./results\\FF_WOE_ep200_pa200_bs8_hs[10]_lr_0.001_relu\n",
      "6275/6275 - 2s - loss: 0.5367 - acc: 0.7372 - auc_46: 0.8059 - recall_m: 0.7971 - precision_m: 0.7242 - f1_m: 0.7358 - val_loss: 0.5408 - val_acc: 0.7202 - val_auc_46: 0.8049 - val_recall_m: 0.8015 - val_precision_m: 0.6895 - val_f1_m: 0.7161\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.54079\n",
      "6275/6275 - 2s - loss: 0.5354 - acc: 0.7374 - auc_46: 0.8068 - recall_m: 0.7941 - precision_m: 0.7292 - f1_m: 0.7350 - val_loss: 0.5444 - val_acc: 0.7177 - val_auc_46: 0.8063 - val_recall_m: 0.8223 - val_precision_m: 0.6819 - val_f1_m: 0.7200\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.54079 to 0.53921, saving model to ./results\\FF_WOE_ep200_pa200_bs8_hs[10]_lr_0.001_relu\n",
      "6275/6275 - 2s - loss: 0.5351 - acc: 0.7366 - auc_46: 0.8073 - recall_m: 0.8007 - precision_m: 0.7188 - f1_m: 0.7356 - val_loss: 0.5392 - val_acc: 0.7189 - val_auc_46: 0.8050 - val_recall_m: 0.7800 - val_precision_m: 0.6958 - val_f1_m: 0.7130\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53921\n",
      "6275/6275 - 2s - loss: 0.5335 - acc: 0.7359 - auc_46: 0.8086 - recall_m: 0.7909 - precision_m: 0.7285 - f1_m: 0.7361 - val_loss: 0.5416 - val_acc: 0.7183 - val_auc_46: 0.8048 - val_recall_m: 0.8010 - val_precision_m: 0.6778 - val_f1_m: 0.7128\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.53921 to 0.53872, saving model to ./results\\FF_WOE_ep200_pa200_bs8_hs[10]_lr_0.001_relu\n",
      "6275/6275 - 2s - loss: 0.5339 - acc: 0.7396 - auc_46: 0.8085 - recall_m: 0.8026 - precision_m: 0.7313 - f1_m: 0.7407 - val_loss: 0.5387 - val_acc: 0.7189 - val_auc_46: 0.8053 - val_recall_m: 0.7787 - val_precision_m: 0.6849 - val_f1_m: 0.7098\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5336 - acc: 0.7382 - auc_46: 0.8084 - recall_m: 0.7954 - precision_m: 0.7229 - f1_m: 0.7366 - val_loss: 0.5397 - val_acc: 0.7196 - val_auc_46: 0.8053 - val_recall_m: 0.7856 - val_precision_m: 0.6949 - val_f1_m: 0.7142\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5330 - acc: 0.7393 - auc_46: 0.8089 - recall_m: 0.7957 - precision_m: 0.7285 - f1_m: 0.7382 - val_loss: 0.5393 - val_acc: 0.7208 - val_auc_46: 0.8047 - val_recall_m: 0.7855 - val_precision_m: 0.6867 - val_f1_m: 0.7024\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5324 - acc: 0.7399 - auc_46: 0.8093 - recall_m: 0.7933 - precision_m: 0.7282 - f1_m: 0.7377 - val_loss: 0.5433 - val_acc: 0.7215 - val_auc_46: 0.8045 - val_recall_m: 0.8013 - val_precision_m: 0.6875 - val_f1_m: 0.7183\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5323 - acc: 0.7390 - auc_46: 0.8094 - recall_m: 0.7961 - precision_m: 0.7262 - f1_m: 0.7361 - val_loss: 0.5409 - val_acc: 0.7234 - val_auc_46: 0.8040 - val_recall_m: 0.7677 - val_precision_m: 0.7010 - val_f1_m: 0.7042\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5321 - acc: 0.7380 - auc_46: 0.8092 - recall_m: 0.7930 - precision_m: 0.7260 - f1_m: 0.7368 - val_loss: 0.5401 - val_acc: 0.7189 - val_auc_46: 0.8047 - val_recall_m: 0.7947 - val_precision_m: 0.7040 - val_f1_m: 0.7225\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5317 - acc: 0.7390 - auc_46: 0.8097 - recall_m: 0.7972 - precision_m: 0.7215 - f1_m: 0.7358 - val_loss: 0.5416 - val_acc: 0.7221 - val_auc_46: 0.8035 - val_recall_m: 0.7690 - val_precision_m: 0.7105 - val_f1_m: 0.7134\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5314 - acc: 0.7402 - auc_46: 0.8101 - recall_m: 0.8027 - precision_m: 0.7321 - f1_m: 0.7399 - val_loss: 0.5397 - val_acc: 0.7247 - val_auc_46: 0.8042 - val_recall_m: 0.7740 - val_precision_m: 0.7067 - val_f1_m: 0.7142\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5310 - acc: 0.7396 - auc_46: 0.8104 - recall_m: 0.7974 - precision_m: 0.7296 - f1_m: 0.7394 - val_loss: 0.5434 - val_acc: 0.7208 - val_auc_46: 0.8039 - val_recall_m: 0.7923 - val_precision_m: 0.6883 - val_f1_m: 0.7133\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5307 - acc: 0.7398 - auc_46: 0.8106 - recall_m: 0.8024 - precision_m: 0.7309 - f1_m: 0.7408 - val_loss: 0.5445 - val_acc: 0.7196 - val_auc_46: 0.8033 - val_recall_m: 0.7987 - val_precision_m: 0.7008 - val_f1_m: 0.7211\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5298 - acc: 0.7385 - auc_46: 0.8111 - recall_m: 0.7997 - precision_m: 0.7303 - f1_m: 0.7403 - val_loss: 0.5458 - val_acc: 0.7189 - val_auc_46: 0.8038 - val_recall_m: 0.8274 - val_precision_m: 0.6944 - val_f1_m: 0.7309\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5303 - acc: 0.7383 - auc_46: 0.8109 - recall_m: 0.7965 - precision_m: 0.7234 - f1_m: 0.7348 - val_loss: 0.5434 - val_acc: 0.7177 - val_auc_46: 0.8027 - val_recall_m: 0.7712 - val_precision_m: 0.6911 - val_f1_m: 0.7031\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5298 - acc: 0.7382 - auc_46: 0.8112 - recall_m: 0.7905 - precision_m: 0.7284 - f1_m: 0.7341 - val_loss: 0.5416 - val_acc: 0.7221 - val_auc_46: 0.8030 - val_recall_m: 0.7695 - val_precision_m: 0.6941 - val_f1_m: 0.7099\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5290 - acc: 0.7398 - auc_46: 0.8118 - recall_m: 0.7932 - precision_m: 0.7254 - f1_m: 0.7335 - val_loss: 0.5427 - val_acc: 0.7196 - val_auc_46: 0.8035 - val_recall_m: 0.7952 - val_precision_m: 0.6958 - val_f1_m: 0.7144\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5292 - acc: 0.7372 - auc_46: 0.8118 - recall_m: 0.7920 - precision_m: 0.7248 - f1_m: 0.7349 - val_loss: 0.5425 - val_acc: 0.7228 - val_auc_46: 0.8027 - val_recall_m: 0.7610 - val_precision_m: 0.6981 - val_f1_m: 0.7042\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5290 - acc: 0.7402 - auc_46: 0.8116 - recall_m: 0.7982 - precision_m: 0.7280 - f1_m: 0.7364 - val_loss: 0.5434 - val_acc: 0.7183 - val_auc_46: 0.8029 - val_recall_m: 0.7881 - val_precision_m: 0.7008 - val_f1_m: 0.7153\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5288 - acc: 0.7390 - auc_46: 0.8120 - recall_m: 0.7959 - precision_m: 0.7270 - f1_m: 0.7363 - val_loss: 0.5433 - val_acc: 0.7183 - val_auc_46: 0.8018 - val_recall_m: 0.7764 - val_precision_m: 0.7020 - val_f1_m: 0.7119\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5283 - acc: 0.7386 - auc_46: 0.8123 - recall_m: 0.7946 - precision_m: 0.7288 - f1_m: 0.7363 - val_loss: 0.5415 - val_acc: 0.7221 - val_auc_46: 0.8029 - val_recall_m: 0.7625 - val_precision_m: 0.6912 - val_f1_m: 0.7021\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5285 - acc: 0.7396 - auc_46: 0.8122 - recall_m: 0.7966 - precision_m: 0.7311 - f1_m: 0.7393 - val_loss: 0.5421 - val_acc: 0.7202 - val_auc_46: 0.8021 - val_recall_m: 0.7662 - val_precision_m: 0.7019 - val_f1_m: 0.7043\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5279 - acc: 0.7383 - auc_46: 0.8127 - recall_m: 0.7829 - precision_m: 0.7195 - f1_m: 0.7264 - val_loss: 0.5440 - val_acc: 0.7196 - val_auc_46: 0.8016 - val_recall_m: 0.7787 - val_precision_m: 0.7134 - val_f1_m: 0.7130\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5277 - acc: 0.7382 - auc_46: 0.8125 - recall_m: 0.7972 - precision_m: 0.7274 - f1_m: 0.7365 - val_loss: 0.5437 - val_acc: 0.7215 - val_auc_46: 0.8022 - val_recall_m: 0.7863 - val_precision_m: 0.7021 - val_f1_m: 0.7172\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5271 - acc: 0.7390 - auc_46: 0.8136 - recall_m: 0.7911 - precision_m: 0.7291 - f1_m: 0.7341 - val_loss: 0.5438 - val_acc: 0.7208 - val_auc_46: 0.8030 - val_recall_m: 0.8019 - val_precision_m: 0.6923 - val_f1_m: 0.7170\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5268 - acc: 0.7383 - auc_46: 0.8136 - recall_m: 0.7973 - precision_m: 0.7291 - f1_m: 0.7388 - val_loss: 0.5476 - val_acc: 0.7170 - val_auc_46: 0.8017 - val_recall_m: 0.7989 - val_precision_m: 0.6815 - val_f1_m: 0.7156\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5272 - acc: 0.7388 - auc_46: 0.8132 - recall_m: 0.7987 - precision_m: 0.7262 - f1_m: 0.7386 - val_loss: 0.5423 - val_acc: 0.7215 - val_auc_46: 0.8022 - val_recall_m: 0.7836 - val_precision_m: 0.7004 - val_f1_m: 0.7182\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5273 - acc: 0.7359 - auc_46: 0.8130 - recall_m: 0.7924 - precision_m: 0.7272 - f1_m: 0.7351 - val_loss: 0.5442 - val_acc: 0.7196 - val_auc_46: 0.8015 - val_recall_m: 0.7762 - val_precision_m: 0.7114 - val_f1_m: 0.7133\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5271 - acc: 0.7383 - auc_46: 0.8135 - recall_m: 0.7951 - precision_m: 0.7307 - f1_m: 0.7372 - val_loss: 0.5427 - val_acc: 0.7215 - val_auc_46: 0.8016 - val_recall_m: 0.7684 - val_precision_m: 0.6992 - val_f1_m: 0.7072\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5271 - acc: 0.7374 - auc_46: 0.8135 - recall_m: 0.7911 - precision_m: 0.7288 - f1_m: 0.7339 - val_loss: 0.5448 - val_acc: 0.7202 - val_auc_46: 0.8005 - val_recall_m: 0.7605 - val_precision_m: 0.6938 - val_f1_m: 0.7041\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5269 - acc: 0.7406 - auc_46: 0.8134 - recall_m: 0.7991 - precision_m: 0.7305 - f1_m: 0.7375 - val_loss: 0.5436 - val_acc: 0.7215 - val_auc_46: 0.8012 - val_recall_m: 0.7800 - val_precision_m: 0.6986 - val_f1_m: 0.7116\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5266 - acc: 0.7371 - auc_46: 0.8138 - recall_m: 0.7990 - precision_m: 0.7289 - f1_m: 0.7391 - val_loss: 0.5445 - val_acc: 0.7189 - val_auc_46: 0.8007 - val_recall_m: 0.7552 - val_precision_m: 0.7076 - val_f1_m: 0.7053\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.53872\n",
      "6275/6275 - 3s - loss: 0.5264 - acc: 0.7390 - auc_46: 0.8139 - recall_m: 0.7909 - precision_m: 0.7268 - f1_m: 0.7339 - val_loss: 0.5456 - val_acc: 0.7234 - val_auc_46: 0.8014 - val_recall_m: 0.7845 - val_precision_m: 0.6859 - val_f1_m: 0.7127\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.53872\n",
      "6275/6275 - 3s - loss: 0.5261 - acc: 0.7393 - auc_46: 0.8143 - recall_m: 0.7923 - precision_m: 0.7227 - f1_m: 0.7353 - val_loss: 0.5440 - val_acc: 0.7221 - val_auc_46: 0.8015 - val_recall_m: 0.7802 - val_precision_m: 0.6915 - val_f1_m: 0.7122\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.53872\n",
      "6275/6275 - 3s - loss: 0.5256 - acc: 0.7401 - auc_46: 0.8145 - recall_m: 0.7920 - precision_m: 0.7302 - f1_m: 0.7380 - val_loss: 0.5465 - val_acc: 0.7202 - val_auc_46: 0.8012 - val_recall_m: 0.8085 - val_precision_m: 0.6961 - val_f1_m: 0.7238\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.53872\n",
      "6275/6275 - 3s - loss: 0.5261 - acc: 0.7359 - auc_46: 0.8140 - recall_m: 0.7868 - precision_m: 0.7219 - f1_m: 0.7303 - val_loss: 0.5463 - val_acc: 0.7221 - val_auc_46: 0.8011 - val_recall_m: 0.7973 - val_precision_m: 0.6955 - val_f1_m: 0.7150\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5262 - acc: 0.7380 - auc_46: 0.8140 - recall_m: 0.7983 - precision_m: 0.7284 - f1_m: 0.7393 - val_loss: 0.5455 - val_acc: 0.7196 - val_auc_46: 0.7998 - val_recall_m: 0.7674 - val_precision_m: 0.7018 - val_f1_m: 0.7096\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5256 - acc: 0.7367 - auc_46: 0.8142 - recall_m: 0.7930 - precision_m: 0.7286 - f1_m: 0.7340 - val_loss: 0.5468 - val_acc: 0.7208 - val_auc_46: 0.8008 - val_recall_m: 0.7920 - val_precision_m: 0.7028 - val_f1_m: 0.7191\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5257 - acc: 0.7385 - auc_46: 0.8145 - recall_m: 0.7980 - precision_m: 0.7287 - f1_m: 0.7365 - val_loss: 0.5449 - val_acc: 0.7208 - val_auc_46: 0.8004 - val_recall_m: 0.7584 - val_precision_m: 0.7132 - val_f1_m: 0.7091\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5257 - acc: 0.7394 - auc_46: 0.8143 - recall_m: 0.7888 - precision_m: 0.7327 - f1_m: 0.7366 - val_loss: 0.5451 - val_acc: 0.7183 - val_auc_46: 0.8003 - val_recall_m: 0.7508 - val_precision_m: 0.6911 - val_f1_m: 0.6983\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5254 - acc: 0.7367 - auc_46: 0.8148 - recall_m: 0.7925 - precision_m: 0.7310 - f1_m: 0.7346 - val_loss: 0.5450 - val_acc: 0.7240 - val_auc_46: 0.8010 - val_recall_m: 0.7903 - val_precision_m: 0.6985 - val_f1_m: 0.7178\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5248 - acc: 0.7410 - auc_46: 0.8150 - recall_m: 0.7886 - precision_m: 0.7322 - f1_m: 0.7370 - val_loss: 0.5494 - val_acc: 0.7196 - val_auc_46: 0.7991 - val_recall_m: 0.8039 - val_precision_m: 0.6898 - val_f1_m: 0.7191\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5254 - acc: 0.7404 - auc_46: 0.8146 - recall_m: 0.7963 - precision_m: 0.7361 - f1_m: 0.7403 - val_loss: 0.5456 - val_acc: 0.7215 - val_auc_46: 0.7998 - val_recall_m: 0.7871 - val_precision_m: 0.6943 - val_f1_m: 0.7091\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5250 - acc: 0.7410 - auc_46: 0.8150 - recall_m: 0.7957 - precision_m: 0.7278 - f1_m: 0.7383 - val_loss: 0.5477 - val_acc: 0.7183 - val_auc_46: 0.7995 - val_recall_m: 0.7626 - val_precision_m: 0.6988 - val_f1_m: 0.7040\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5247 - acc: 0.7388 - auc_46: 0.8153 - recall_m: 0.7902 - precision_m: 0.7356 - f1_m: 0.7384 - val_loss: 0.5447 - val_acc: 0.7202 - val_auc_46: 0.7999 - val_recall_m: 0.7533 - val_precision_m: 0.6934 - val_f1_m: 0.6968\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5245 - acc: 0.7378 - auc_46: 0.8153 - recall_m: 0.7891 - precision_m: 0.7253 - f1_m: 0.7312 - val_loss: 0.5487 - val_acc: 0.7196 - val_auc_46: 0.8001 - val_recall_m: 0.7947 - val_precision_m: 0.6962 - val_f1_m: 0.7193\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5248 - acc: 0.7383 - auc_46: 0.8147 - recall_m: 0.8000 - precision_m: 0.7294 - f1_m: 0.7403 - val_loss: 0.5454 - val_acc: 0.7170 - val_auc_46: 0.7992 - val_recall_m: 0.7473 - val_precision_m: 0.6946 - val_f1_m: 0.6935\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.53872\n",
      "6275/6275 - 3s - loss: 0.5249 - acc: 0.7399 - auc_46: 0.8151 - recall_m: 0.7920 - precision_m: 0.7271 - f1_m: 0.7348 - val_loss: 0.5480 - val_acc: 0.7202 - val_auc_46: 0.7997 - val_recall_m: 0.7803 - val_precision_m: 0.7053 - val_f1_m: 0.7170\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.53872\n",
      "6275/6275 - 3s - loss: 0.5251 - acc: 0.7399 - auc_46: 0.8149 - recall_m: 0.7892 - precision_m: 0.7291 - f1_m: 0.7330 - val_loss: 0.5453 - val_acc: 0.7183 - val_auc_46: 0.7997 - val_recall_m: 0.7575 - val_precision_m: 0.6973 - val_f1_m: 0.7015\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.53872\n",
      "6275/6275 - 3s - loss: 0.5251 - acc: 0.7402 - auc_46: 0.8148 - recall_m: 0.7882 - precision_m: 0.7272 - f1_m: 0.7333 - val_loss: 0.5479 - val_acc: 0.7170 - val_auc_46: 0.7991 - val_recall_m: 0.7622 - val_precision_m: 0.6956 - val_f1_m: 0.7085\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5247 - acc: 0.7402 - auc_46: 0.8151 - recall_m: 0.7894 - precision_m: 0.7263 - f1_m: 0.7337 - val_loss: 0.5457 - val_acc: 0.7234 - val_auc_46: 0.7990 - val_recall_m: 0.7818 - val_precision_m: 0.6985 - val_f1_m: 0.7123\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5245 - acc: 0.7409 - auc_46: 0.8154 - recall_m: 0.7939 - precision_m: 0.7272 - f1_m: 0.7333 - val_loss: 0.5484 - val_acc: 0.7183 - val_auc_46: 0.7988 - val_recall_m: 0.7706 - val_precision_m: 0.7023 - val_f1_m: 0.7129\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5246 - acc: 0.7402 - auc_46: 0.8151 - recall_m: 0.7955 - precision_m: 0.7389 - f1_m: 0.7430 - val_loss: 0.5484 - val_acc: 0.7215 - val_auc_46: 0.7989 - val_recall_m: 0.8024 - val_precision_m: 0.6997 - val_f1_m: 0.7183\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5246 - acc: 0.7394 - auc_46: 0.8149 - recall_m: 0.7977 - precision_m: 0.7332 - f1_m: 0.7413 - val_loss: 0.5462 - val_acc: 0.7240 - val_auc_46: 0.7990 - val_recall_m: 0.7875 - val_precision_m: 0.7007 - val_f1_m: 0.7182\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5244 - acc: 0.7422 - auc_46: 0.8156 - recall_m: 0.7915 - precision_m: 0.7349 - f1_m: 0.7368 - val_loss: 0.5489 - val_acc: 0.7202 - val_auc_46: 0.7989 - val_recall_m: 0.7552 - val_precision_m: 0.6920 - val_f1_m: 0.6994\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5247 - acc: 0.7406 - auc_46: 0.8152 - recall_m: 0.7931 - precision_m: 0.7297 - f1_m: 0.7365 - val_loss: 0.5479 - val_acc: 0.7183 - val_auc_46: 0.7981 - val_recall_m: 0.7593 - val_precision_m: 0.7013 - val_f1_m: 0.7043\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5249 - acc: 0.7398 - auc_46: 0.8151 - recall_m: 0.7927 - precision_m: 0.7283 - f1_m: 0.7374 - val_loss: 0.5469 - val_acc: 0.7208 - val_auc_46: 0.7993 - val_recall_m: 0.7963 - val_precision_m: 0.7088 - val_f1_m: 0.7246\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5240 - acc: 0.7401 - auc_46: 0.8158 - recall_m: 0.7871 - precision_m: 0.7305 - f1_m: 0.7353 - val_loss: 0.5485 - val_acc: 0.7189 - val_auc_46: 0.7998 - val_recall_m: 0.7904 - val_precision_m: 0.6984 - val_f1_m: 0.7201\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5243 - acc: 0.7399 - auc_46: 0.8154 - recall_m: 0.7927 - precision_m: 0.7338 - f1_m: 0.7357 - val_loss: 0.5484 - val_acc: 0.7228 - val_auc_46: 0.7990 - val_recall_m: 0.7606 - val_precision_m: 0.6866 - val_f1_m: 0.7048\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5241 - acc: 0.7382 - auc_46: 0.8155 - recall_m: 0.7905 - precision_m: 0.7313 - f1_m: 0.7358 - val_loss: 0.5464 - val_acc: 0.7208 - val_auc_46: 0.7984 - val_recall_m: 0.7716 - val_precision_m: 0.6829 - val_f1_m: 0.7014\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5242 - acc: 0.7394 - auc_46: 0.8157 - recall_m: 0.7882 - precision_m: 0.7304 - f1_m: 0.7350 - val_loss: 0.5482 - val_acc: 0.7177 - val_auc_46: 0.7981 - val_recall_m: 0.7655 - val_precision_m: 0.7038 - val_f1_m: 0.7093\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5244 - acc: 0.7431 - auc_46: 0.8150 - recall_m: 0.7916 - precision_m: 0.7331 - f1_m: 0.7379 - val_loss: 0.5473 - val_acc: 0.7202 - val_auc_46: 0.7981 - val_recall_m: 0.7599 - val_precision_m: 0.6919 - val_f1_m: 0.7014\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5239 - acc: 0.7398 - auc_46: 0.8159 - recall_m: 0.7908 - precision_m: 0.7334 - f1_m: 0.7393 - val_loss: 0.5487 - val_acc: 0.7208 - val_auc_46: 0.7982 - val_recall_m: 0.7840 - val_precision_m: 0.6845 - val_f1_m: 0.7104\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5245 - acc: 0.7409 - auc_46: 0.8153 - recall_m: 0.7904 - precision_m: 0.7235 - f1_m: 0.7316 - val_loss: 0.5464 - val_acc: 0.7228 - val_auc_46: 0.7987 - val_recall_m: 0.7737 - val_precision_m: 0.6862 - val_f1_m: 0.7002\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5240 - acc: 0.7407 - auc_46: 0.8158 - recall_m: 0.7891 - precision_m: 0.7350 - f1_m: 0.7356 - val_loss: 0.5493 - val_acc: 0.7189 - val_auc_46: 0.7991 - val_recall_m: 0.7907 - val_precision_m: 0.6922 - val_f1_m: 0.7132\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5234 - acc: 0.7398 - auc_46: 0.8164 - recall_m: 0.7932 - precision_m: 0.7265 - f1_m: 0.7348 - val_loss: 0.5472 - val_acc: 0.7196 - val_auc_46: 0.7986 - val_recall_m: 0.7535 - val_precision_m: 0.7067 - val_f1_m: 0.7038\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5237 - acc: 0.7398 - auc_46: 0.8159 - recall_m: 0.7905 - precision_m: 0.7337 - f1_m: 0.7367 - val_loss: 0.5502 - val_acc: 0.7215 - val_auc_46: 0.7985 - val_recall_m: 0.7899 - val_precision_m: 0.6959 - val_f1_m: 0.7171\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5241 - acc: 0.7422 - auc_46: 0.8154 - recall_m: 0.7919 - precision_m: 0.7345 - f1_m: 0.7388 - val_loss: 0.5485 - val_acc: 0.7215 - val_auc_46: 0.7987 - val_recall_m: 0.7825 - val_precision_m: 0.6805 - val_f1_m: 0.7050\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5236 - acc: 0.7410 - auc_46: 0.8162 - recall_m: 0.7911 - precision_m: 0.7338 - f1_m: 0.7399 - val_loss: 0.5484 - val_acc: 0.7215 - val_auc_46: 0.7981 - val_recall_m: 0.7712 - val_precision_m: 0.6941 - val_f1_m: 0.7060\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5240 - acc: 0.7414 - auc_46: 0.8157 - recall_m: 0.7894 - precision_m: 0.7286 - f1_m: 0.7348 - val_loss: 0.5478 - val_acc: 0.7208 - val_auc_46: 0.7983 - val_recall_m: 0.7914 - val_precision_m: 0.6933 - val_f1_m: 0.7138\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5237 - acc: 0.7404 - auc_46: 0.8163 - recall_m: 0.7903 - precision_m: 0.7304 - f1_m: 0.7341 - val_loss: 0.5470 - val_acc: 0.7247 - val_auc_46: 0.7983 - val_recall_m: 0.7720 - val_precision_m: 0.6991 - val_f1_m: 0.7098\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5235 - acc: 0.7415 - auc_46: 0.8161 - recall_m: 0.7959 - precision_m: 0.7309 - f1_m: 0.7404 - val_loss: 0.5490 - val_acc: 0.7208 - val_auc_46: 0.7974 - val_recall_m: 0.7628 - val_precision_m: 0.7124 - val_f1_m: 0.7102\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5238 - acc: 0.7412 - auc_46: 0.8156 - recall_m: 0.8004 - precision_m: 0.7372 - f1_m: 0.7410 - val_loss: 0.5501 - val_acc: 0.7240 - val_auc_46: 0.7981 - val_recall_m: 0.7907 - val_precision_m: 0.7030 - val_f1_m: 0.7172\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5238 - acc: 0.7428 - auc_46: 0.8160 - recall_m: 0.7914 - precision_m: 0.7337 - f1_m: 0.7376 - val_loss: 0.5472 - val_acc: 0.7259 - val_auc_46: 0.7978 - val_recall_m: 0.7804 - val_precision_m: 0.7068 - val_f1_m: 0.7178\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5234 - acc: 0.7402 - auc_46: 0.8164 - recall_m: 0.7906 - precision_m: 0.7363 - f1_m: 0.7388 - val_loss: 0.5487 - val_acc: 0.7240 - val_auc_46: 0.7984 - val_recall_m: 0.7826 - val_precision_m: 0.6973 - val_f1_m: 0.7126\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5237 - acc: 0.7404 - auc_46: 0.8163 - recall_m: 0.7948 - precision_m: 0.7310 - f1_m: 0.7366 - val_loss: 0.5486 - val_acc: 0.7221 - val_auc_46: 0.7985 - val_recall_m: 0.7776 - val_precision_m: 0.6978 - val_f1_m: 0.7116\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5233 - acc: 0.7423 - auc_46: 0.8163 - recall_m: 0.7900 - precision_m: 0.7327 - f1_m: 0.7365 - val_loss: 0.5475 - val_acc: 0.7234 - val_auc_46: 0.7978 - val_recall_m: 0.7425 - val_precision_m: 0.7047 - val_f1_m: 0.7017\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5232 - acc: 0.7415 - auc_46: 0.8166 - recall_m: 0.7934 - precision_m: 0.7303 - f1_m: 0.7384 - val_loss: 0.5478 - val_acc: 0.7215 - val_auc_46: 0.7984 - val_recall_m: 0.7694 - val_precision_m: 0.7138 - val_f1_m: 0.7100\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5232 - acc: 0.7415 - auc_46: 0.8166 - recall_m: 0.7937 - precision_m: 0.7327 - f1_m: 0.7400 - val_loss: 0.5482 - val_acc: 0.7247 - val_auc_46: 0.7986 - val_recall_m: 0.7645 - val_precision_m: 0.6906 - val_f1_m: 0.7036\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5237 - acc: 0.7410 - auc_46: 0.8162 - recall_m: 0.7900 - precision_m: 0.7291 - f1_m: 0.7351 - val_loss: 0.5477 - val_acc: 0.7253 - val_auc_46: 0.7986 - val_recall_m: 0.7819 - val_precision_m: 0.7059 - val_f1_m: 0.7157\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5229 - acc: 0.7393 - auc_46: 0.8165 - recall_m: 0.7842 - precision_m: 0.7315 - f1_m: 0.7338 - val_loss: 0.5484 - val_acc: 0.7221 - val_auc_46: 0.7986 - val_recall_m: 0.7753 - val_precision_m: 0.6919 - val_f1_m: 0.7056\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5228 - acc: 0.7433 - auc_46: 0.8169 - recall_m: 0.7879 - precision_m: 0.7359 - f1_m: 0.7370 - val_loss: 0.5471 - val_acc: 0.7228 - val_auc_46: 0.7981 - val_recall_m: 0.7730 - val_precision_m: 0.7119 - val_f1_m: 0.7153\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5236 - acc: 0.7402 - auc_46: 0.8159 - recall_m: 0.7913 - precision_m: 0.7360 - f1_m: 0.7407 - val_loss: 0.5475 - val_acc: 0.7247 - val_auc_46: 0.7978 - val_recall_m: 0.7767 - val_precision_m: 0.7155 - val_f1_m: 0.7192\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5234 - acc: 0.7407 - auc_46: 0.8164 - recall_m: 0.7931 - precision_m: 0.7310 - f1_m: 0.7379 - val_loss: 0.5469 - val_acc: 0.7202 - val_auc_46: 0.7985 - val_recall_m: 0.7761 - val_precision_m: 0.6896 - val_f1_m: 0.7087\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5228 - acc: 0.7414 - auc_46: 0.8166 - recall_m: 0.7894 - precision_m: 0.7328 - f1_m: 0.7354 - val_loss: 0.5457 - val_acc: 0.7228 - val_auc_46: 0.7988 - val_recall_m: 0.7539 - val_precision_m: 0.6982 - val_f1_m: 0.7048\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5229 - acc: 0.7431 - auc_46: 0.8169 - recall_m: 0.7930 - precision_m: 0.7369 - f1_m: 0.7397 - val_loss: 0.5472 - val_acc: 0.7253 - val_auc_46: 0.7979 - val_recall_m: 0.7781 - val_precision_m: 0.7006 - val_f1_m: 0.7118\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5229 - acc: 0.7410 - auc_46: 0.8167 - recall_m: 0.7924 - precision_m: 0.7304 - f1_m: 0.7380 - val_loss: 0.5483 - val_acc: 0.7208 - val_auc_46: 0.7977 - val_recall_m: 0.7615 - val_precision_m: 0.7009 - val_f1_m: 0.7050\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5229 - acc: 0.7404 - auc_46: 0.8169 - recall_m: 0.7816 - precision_m: 0.7290 - f1_m: 0.7314 - val_loss: 0.5518 - val_acc: 0.7208 - val_auc_46: 0.7986 - val_recall_m: 0.7898 - val_precision_m: 0.6823 - val_f1_m: 0.7126\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5228 - acc: 0.7434 - auc_46: 0.8169 - recall_m: 0.7909 - precision_m: 0.7359 - f1_m: 0.7392 - val_loss: 0.5462 - val_acc: 0.7234 - val_auc_46: 0.7983 - val_recall_m: 0.7875 - val_precision_m: 0.7048 - val_f1_m: 0.7203\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5222 - acc: 0.7420 - auc_46: 0.8174 - recall_m: 0.7923 - precision_m: 0.7368 - f1_m: 0.7385 - val_loss: 0.5493 - val_acc: 0.7247 - val_auc_46: 0.7982 - val_recall_m: 0.7895 - val_precision_m: 0.6954 - val_f1_m: 0.7185\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5227 - acc: 0.7445 - auc_46: 0.8167 - recall_m: 0.7950 - precision_m: 0.7404 - f1_m: 0.7429 - val_loss: 0.5481 - val_acc: 0.7259 - val_auc_46: 0.7979 - val_recall_m: 0.7727 - val_precision_m: 0.6849 - val_f1_m: 0.7049\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5231 - acc: 0.7418 - auc_46: 0.8165 - recall_m: 0.7916 - precision_m: 0.7274 - f1_m: 0.7351 - val_loss: 0.5484 - val_acc: 0.7234 - val_auc_46: 0.7980 - val_recall_m: 0.7775 - val_precision_m: 0.6978 - val_f1_m: 0.7131\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5223 - acc: 0.7450 - auc_46: 0.8170 - recall_m: 0.7947 - precision_m: 0.7344 - f1_m: 0.7404 - val_loss: 0.5495 - val_acc: 0.7266 - val_auc_46: 0.7979 - val_recall_m: 0.7976 - val_precision_m: 0.6997 - val_f1_m: 0.7232\n",
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5226 - acc: 0.7429 - auc_46: 0.8169 - recall_m: 0.7928 - precision_m: 0.7367 - f1_m: 0.7401 - val_loss: 0.5499 - val_acc: 0.7189 - val_auc_46: 0.7967 - val_recall_m: 0.7450 - val_precision_m: 0.7133 - val_f1_m: 0.7003\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5229 - acc: 0.7428 - auc_46: 0.8166 - recall_m: 0.7896 - precision_m: 0.7383 - f1_m: 0.7385 - val_loss: 0.5498 - val_acc: 0.7240 - val_auc_46: 0.7976 - val_recall_m: 0.7855 - val_precision_m: 0.6971 - val_f1_m: 0.7141\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5224 - acc: 0.7417 - auc_46: 0.8172 - recall_m: 0.7989 - precision_m: 0.7319 - f1_m: 0.7398 - val_loss: 0.5507 - val_acc: 0.7228 - val_auc_46: 0.7976 - val_recall_m: 0.7765 - val_precision_m: 0.7080 - val_f1_m: 0.7136\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5221 - acc: 0.7441 - auc_46: 0.8177 - recall_m: 0.7969 - precision_m: 0.7391 - f1_m: 0.7438 - val_loss: 0.5487 - val_acc: 0.7202 - val_auc_46: 0.7971 - val_recall_m: 0.7504 - val_precision_m: 0.7076 - val_f1_m: 0.7034\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5225 - acc: 0.7406 - auc_46: 0.8169 - recall_m: 0.7915 - precision_m: 0.7324 - f1_m: 0.7366 - val_loss: 0.5498 - val_acc: 0.7240 - val_auc_46: 0.7969 - val_recall_m: 0.7661 - val_precision_m: 0.6935 - val_f1_m: 0.6995\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5227 - acc: 0.7425 - auc_46: 0.8169 - recall_m: 0.7937 - precision_m: 0.7355 - f1_m: 0.7391 - val_loss: 0.5481 - val_acc: 0.7266 - val_auc_46: 0.7977 - val_recall_m: 0.7898 - val_precision_m: 0.7030 - val_f1_m: 0.7226\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5225 - acc: 0.7410 - auc_46: 0.8172 - recall_m: 0.7850 - precision_m: 0.7326 - f1_m: 0.7353 - val_loss: 0.5493 - val_acc: 0.7259 - val_auc_46: 0.7974 - val_recall_m: 0.7762 - val_precision_m: 0.7022 - val_f1_m: 0.7159\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5225 - acc: 0.7447 - auc_46: 0.8173 - recall_m: 0.7923 - precision_m: 0.7390 - f1_m: 0.7433 - val_loss: 0.5489 - val_acc: 0.7221 - val_auc_46: 0.7976 - val_recall_m: 0.7849 - val_precision_m: 0.6974 - val_f1_m: 0.7186\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5214 - acc: 0.7437 - auc_46: 0.8179 - recall_m: 0.7988 - precision_m: 0.7374 - f1_m: 0.7432 - val_loss: 0.5497 - val_acc: 0.7228 - val_auc_46: 0.7966 - val_recall_m: 0.7621 - val_precision_m: 0.6988 - val_f1_m: 0.7096\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5220 - acc: 0.7423 - auc_46: 0.8173 - recall_m: 0.7920 - precision_m: 0.7368 - f1_m: 0.7401 - val_loss: 0.5492 - val_acc: 0.7228 - val_auc_46: 0.7968 - val_recall_m: 0.7773 - val_precision_m: 0.7016 - val_f1_m: 0.7122\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5224 - acc: 0.7422 - auc_46: 0.8174 - recall_m: 0.7913 - precision_m: 0.7327 - f1_m: 0.7385 - val_loss: 0.5497 - val_acc: 0.7228 - val_auc_46: 0.7966 - val_recall_m: 0.7791 - val_precision_m: 0.6967 - val_f1_m: 0.7108\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5221 - acc: 0.7447 - auc_46: 0.8174 - recall_m: 0.7969 - precision_m: 0.7378 - f1_m: 0.7425 - val_loss: 0.5489 - val_acc: 0.7259 - val_auc_46: 0.7971 - val_recall_m: 0.7870 - val_precision_m: 0.7127 - val_f1_m: 0.7200\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5220 - acc: 0.7429 - auc_46: 0.8177 - recall_m: 0.7938 - precision_m: 0.7271 - f1_m: 0.7376 - val_loss: 0.5491 - val_acc: 0.7253 - val_auc_46: 0.7970 - val_recall_m: 0.7795 - val_precision_m: 0.6925 - val_f1_m: 0.7114\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5224 - acc: 0.7431 - auc_46: 0.8170 - recall_m: 0.7954 - precision_m: 0.7354 - f1_m: 0.7377 - val_loss: 0.5498 - val_acc: 0.7221 - val_auc_46: 0.7971 - val_recall_m: 0.7935 - val_precision_m: 0.7029 - val_f1_m: 0.7156\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5216 - acc: 0.7433 - auc_46: 0.8177 - recall_m: 0.7968 - precision_m: 0.7347 - f1_m: 0.7402 - val_loss: 0.5499 - val_acc: 0.7253 - val_auc_46: 0.7973 - val_recall_m: 0.7785 - val_precision_m: 0.7018 - val_f1_m: 0.7107\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5222 - acc: 0.7436 - auc_46: 0.8175 - recall_m: 0.7904 - precision_m: 0.7258 - f1_m: 0.7343 - val_loss: 0.5498 - val_acc: 0.7215 - val_auc_46: 0.7977 - val_recall_m: 0.7775 - val_precision_m: 0.6806 - val_f1_m: 0.7005\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5210 - acc: 0.7444 - auc_46: 0.8183 - recall_m: 0.8015 - precision_m: 0.7363 - f1_m: 0.7434 - val_loss: 0.5518 - val_acc: 0.7234 - val_auc_46: 0.7954 - val_recall_m: 0.7488 - val_precision_m: 0.7000 - val_f1_m: 0.7011\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5218 - acc: 0.7444 - auc_46: 0.8177 - recall_m: 0.7948 - precision_m: 0.7421 - f1_m: 0.7443 - val_loss: 0.5516 - val_acc: 0.7234 - val_auc_46: 0.7965 - val_recall_m: 0.7812 - val_precision_m: 0.6942 - val_f1_m: 0.7125\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5218 - acc: 0.7429 - auc_46: 0.8179 - recall_m: 0.7970 - precision_m: 0.7354 - f1_m: 0.7407 - val_loss: 0.5488 - val_acc: 0.7228 - val_auc_46: 0.7971 - val_recall_m: 0.7855 - val_precision_m: 0.6894 - val_f1_m: 0.7083\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5218 - acc: 0.7437 - auc_46: 0.8178 - recall_m: 0.7926 - precision_m: 0.7354 - f1_m: 0.7409 - val_loss: 0.5488 - val_acc: 0.7215 - val_auc_46: 0.7967 - val_recall_m: 0.7806 - val_precision_m: 0.6956 - val_f1_m: 0.7115\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5216 - acc: 0.7398 - auc_46: 0.8180 - recall_m: 0.7935 - precision_m: 0.7301 - f1_m: 0.7373 - val_loss: 0.5503 - val_acc: 0.7247 - val_auc_46: 0.7965 - val_recall_m: 0.7578 - val_precision_m: 0.6996 - val_f1_m: 0.7023\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5213 - acc: 0.7426 - auc_46: 0.8182 - recall_m: 0.7864 - precision_m: 0.7321 - f1_m: 0.7351 - val_loss: 0.5547 - val_acc: 0.7177 - val_auc_46: 0.7976 - val_recall_m: 0.7968 - val_precision_m: 0.6977 - val_f1_m: 0.7201\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5214 - acc: 0.7450 - auc_46: 0.8180 - recall_m: 0.8069 - precision_m: 0.7341 - f1_m: 0.7434 - val_loss: 0.5481 - val_acc: 0.7240 - val_auc_46: 0.7973 - val_recall_m: 0.7739 - val_precision_m: 0.7019 - val_f1_m: 0.7133\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5214 - acc: 0.7433 - auc_46: 0.8178 - recall_m: 0.7938 - precision_m: 0.7289 - f1_m: 0.7385 - val_loss: 0.5500 - val_acc: 0.7221 - val_auc_46: 0.7958 - val_recall_m: 0.7428 - val_precision_m: 0.7089 - val_f1_m: 0.7027\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5213 - acc: 0.7436 - auc_46: 0.8182 - recall_m: 0.7928 - precision_m: 0.7369 - f1_m: 0.7406 - val_loss: 0.5478 - val_acc: 0.7215 - val_auc_46: 0.7974 - val_recall_m: 0.7802 - val_precision_m: 0.7109 - val_f1_m: 0.7149\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5214 - acc: 0.7439 - auc_46: 0.8182 - recall_m: 0.7897 - precision_m: 0.7350 - f1_m: 0.7384 - val_loss: 0.5478 - val_acc: 0.7208 - val_auc_46: 0.7974 - val_recall_m: 0.7757 - val_precision_m: 0.6918 - val_f1_m: 0.7104\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5215 - acc: 0.7415 - auc_46: 0.8180 - recall_m: 0.7848 - precision_m: 0.7294 - f1_m: 0.7308 - val_loss: 0.5522 - val_acc: 0.7177 - val_auc_46: 0.7978 - val_recall_m: 0.7899 - val_precision_m: 0.6886 - val_f1_m: 0.7132\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5212 - acc: 0.7433 - auc_46: 0.8182 - recall_m: 0.7934 - precision_m: 0.7263 - f1_m: 0.7360 - val_loss: 0.5499 - val_acc: 0.7202 - val_auc_46: 0.7960 - val_recall_m: 0.7621 - val_precision_m: 0.7137 - val_f1_m: 0.7089\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5216 - acc: 0.7431 - auc_46: 0.8181 - recall_m: 0.7997 - precision_m: 0.7358 - f1_m: 0.7432 - val_loss: 0.5510 - val_acc: 0.7240 - val_auc_46: 0.7956 - val_recall_m: 0.7767 - val_precision_m: 0.7052 - val_f1_m: 0.7139\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5208 - acc: 0.7447 - auc_46: 0.8187 - recall_m: 0.7979 - precision_m: 0.7360 - f1_m: 0.7419 - val_loss: 0.5495 - val_acc: 0.7208 - val_auc_46: 0.7972 - val_recall_m: 0.7789 - val_precision_m: 0.6935 - val_f1_m: 0.7103\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5213 - acc: 0.7444 - auc_46: 0.8182 - recall_m: 0.7954 - precision_m: 0.7330 - f1_m: 0.7389 - val_loss: 0.5504 - val_acc: 0.7189 - val_auc_46: 0.7967 - val_recall_m: 0.7826 - val_precision_m: 0.6870 - val_f1_m: 0.7076\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5209 - acc: 0.7466 - auc_46: 0.8187 - recall_m: 0.7994 - precision_m: 0.7370 - f1_m: 0.7420 - val_loss: 0.5493 - val_acc: 0.7240 - val_auc_46: 0.7968 - val_recall_m: 0.7815 - val_precision_m: 0.7071 - val_f1_m: 0.7146\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5211 - acc: 0.7461 - auc_46: 0.8184 - recall_m: 0.7931 - precision_m: 0.7414 - f1_m: 0.7447 - val_loss: 0.5493 - val_acc: 0.7247 - val_auc_46: 0.7968 - val_recall_m: 0.7666 - val_precision_m: 0.6957 - val_f1_m: 0.7061\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5209 - acc: 0.7445 - auc_46: 0.8184 - recall_m: 0.7940 - precision_m: 0.7311 - f1_m: 0.7370 - val_loss: 0.5511 - val_acc: 0.7208 - val_auc_46: 0.7968 - val_recall_m: 0.7839 - val_precision_m: 0.6950 - val_f1_m: 0.7148\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5209 - acc: 0.7422 - auc_46: 0.8184 - recall_m: 0.7946 - precision_m: 0.7323 - f1_m: 0.7373 - val_loss: 0.5491 - val_acc: 0.7202 - val_auc_46: 0.7973 - val_recall_m: 0.7820 - val_precision_m: 0.6926 - val_f1_m: 0.7149\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5209 - acc: 0.7437 - auc_46: 0.8186 - recall_m: 0.7943 - precision_m: 0.7363 - f1_m: 0.7391 - val_loss: 0.5496 - val_acc: 0.7259 - val_auc_46: 0.7967 - val_recall_m: 0.7820 - val_precision_m: 0.7138 - val_f1_m: 0.7181\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5203 - acc: 0.7434 - auc_46: 0.8190 - recall_m: 0.7866 - precision_m: 0.7233 - f1_m: 0.7327 - val_loss: 0.5504 - val_acc: 0.7240 - val_auc_46: 0.7960 - val_recall_m: 0.7732 - val_precision_m: 0.6973 - val_f1_m: 0.7112\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5205 - acc: 0.7434 - auc_46: 0.8188 - recall_m: 0.7917 - precision_m: 0.7287 - f1_m: 0.7367 - val_loss: 0.5503 - val_acc: 0.7247 - val_auc_46: 0.7969 - val_recall_m: 0.7790 - val_precision_m: 0.7011 - val_f1_m: 0.7136\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5208 - acc: 0.7453 - auc_46: 0.8189 - recall_m: 0.7982 - precision_m: 0.7385 - f1_m: 0.7436 - val_loss: 0.5480 - val_acc: 0.7240 - val_auc_46: 0.7973 - val_recall_m: 0.7892 - val_precision_m: 0.7065 - val_f1_m: 0.7170\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5197 - acc: 0.7468 - auc_46: 0.8194 - recall_m: 0.8028 - precision_m: 0.7413 - f1_m: 0.7469 - val_loss: 0.5513 - val_acc: 0.7221 - val_auc_46: 0.7962 - val_recall_m: 0.7495 - val_precision_m: 0.7091 - val_f1_m: 0.6960\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5212 - acc: 0.7425 - auc_46: 0.8182 - recall_m: 0.7959 - precision_m: 0.7418 - f1_m: 0.7417 - val_loss: 0.5498 - val_acc: 0.7240 - val_auc_46: 0.7974 - val_recall_m: 0.7745 - val_precision_m: 0.6858 - val_f1_m: 0.7037\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5204 - acc: 0.7425 - auc_46: 0.8187 - recall_m: 0.7945 - precision_m: 0.7359 - f1_m: 0.7406 - val_loss: 0.5491 - val_acc: 0.7221 - val_auc_46: 0.7976 - val_recall_m: 0.7783 - val_precision_m: 0.6961 - val_f1_m: 0.7117\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5210 - acc: 0.7439 - auc_46: 0.8186 - recall_m: 0.7914 - precision_m: 0.7285 - f1_m: 0.7357 - val_loss: 0.5476 - val_acc: 0.7215 - val_auc_46: 0.7980 - val_recall_m: 0.7723 - val_precision_m: 0.6953 - val_f1_m: 0.7061\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5208 - acc: 0.7428 - auc_46: 0.8185 - recall_m: 0.7958 - precision_m: 0.7355 - f1_m: 0.7389 - val_loss: 0.5489 - val_acc: 0.7228 - val_auc_46: 0.7980 - val_recall_m: 0.7623 - val_precision_m: 0.6803 - val_f1_m: 0.6986\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5201 - acc: 0.7450 - auc_46: 0.8191 - recall_m: 0.7969 - precision_m: 0.7404 - f1_m: 0.7438 - val_loss: 0.5515 - val_acc: 0.7240 - val_auc_46: 0.7970 - val_recall_m: 0.7532 - val_precision_m: 0.6976 - val_f1_m: 0.7040\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5205 - acc: 0.7445 - auc_46: 0.8184 - recall_m: 0.7876 - precision_m: 0.7336 - f1_m: 0.7379 - val_loss: 0.5478 - val_acc: 0.7228 - val_auc_46: 0.7977 - val_recall_m: 0.7552 - val_precision_m: 0.6944 - val_f1_m: 0.6959\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5204 - acc: 0.7436 - auc_46: 0.8190 - recall_m: 0.7934 - precision_m: 0.7361 - f1_m: 0.7398 - val_loss: 0.5479 - val_acc: 0.7240 - val_auc_46: 0.7972 - val_recall_m: 0.7613 - val_precision_m: 0.7059 - val_f1_m: 0.7107\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5200 - acc: 0.7428 - auc_46: 0.8193 - recall_m: 0.7980 - precision_m: 0.7377 - f1_m: 0.7442 - val_loss: 0.5505 - val_acc: 0.7234 - val_auc_46: 0.7974 - val_recall_m: 0.7776 - val_precision_m: 0.6934 - val_f1_m: 0.7059\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5201 - acc: 0.7452 - auc_46: 0.8192 - recall_m: 0.8008 - precision_m: 0.7337 - f1_m: 0.7426 - val_loss: 0.5488 - val_acc: 0.7240 - val_auc_46: 0.7970 - val_recall_m: 0.7795 - val_precision_m: 0.7054 - val_f1_m: 0.7165\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.53872\n",
      "6275/6275 - 1s - loss: 0.5196 - acc: 0.7428 - auc_46: 0.8195 - recall_m: 0.7962 - precision_m: 0.7325 - f1_m: 0.7402 - val_loss: 0.5489 - val_acc: 0.7253 - val_auc_46: 0.7974 - val_recall_m: 0.7792 - val_precision_m: 0.7014 - val_f1_m: 0.7130\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5202 - acc: 0.7463 - auc_46: 0.8190 - recall_m: 0.7987 - precision_m: 0.7347 - f1_m: 0.7426 - val_loss: 0.5509 - val_acc: 0.7247 - val_auc_46: 0.7964 - val_recall_m: 0.7694 - val_precision_m: 0.6915 - val_f1_m: 0.7082\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5194 - acc: 0.7450 - auc_46: 0.8200 - recall_m: 0.7919 - precision_m: 0.7403 - f1_m: 0.7391 - val_loss: 0.5509 - val_acc: 0.7247 - val_auc_46: 0.7968 - val_recall_m: 0.7977 - val_precision_m: 0.7033 - val_f1_m: 0.7200\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5198 - acc: 0.7439 - auc_46: 0.8192 - recall_m: 0.7948 - precision_m: 0.7345 - f1_m: 0.7410 - val_loss: 0.5514 - val_acc: 0.7228 - val_auc_46: 0.7960 - val_recall_m: 0.7670 - val_precision_m: 0.6940 - val_f1_m: 0.7075\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5201 - acc: 0.7468 - auc_46: 0.8191 - recall_m: 0.7938 - precision_m: 0.7336 - f1_m: 0.7407 - val_loss: 0.5500 - val_acc: 0.7259 - val_auc_46: 0.7977 - val_recall_m: 0.7804 - val_precision_m: 0.6994 - val_f1_m: 0.7157\n",
      "Epoch 154/200\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5201 - acc: 0.7453 - auc_46: 0.8193 - recall_m: 0.7955 - precision_m: 0.7355 - f1_m: 0.7404 - val_loss: 0.5498 - val_acc: 0.7221 - val_auc_46: 0.7964 - val_recall_m: 0.7601 - val_precision_m: 0.6991 - val_f1_m: 0.7016\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5197 - acc: 0.7466 - auc_46: 0.8191 - recall_m: 0.7976 - precision_m: 0.7375 - f1_m: 0.7417 - val_loss: 0.5501 - val_acc: 0.7240 - val_auc_46: 0.7969 - val_recall_m: 0.7741 - val_precision_m: 0.6933 - val_f1_m: 0.7090\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5195 - acc: 0.7458 - auc_46: 0.8197 - recall_m: 0.7946 - precision_m: 0.7348 - f1_m: 0.7395 - val_loss: 0.5497 - val_acc: 0.7253 - val_auc_46: 0.7962 - val_recall_m: 0.7583 - val_precision_m: 0.7103 - val_f1_m: 0.7147\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5195 - acc: 0.7484 - auc_46: 0.8196 - recall_m: 0.7935 - precision_m: 0.7396 - f1_m: 0.7430 - val_loss: 0.5500 - val_acc: 0.7253 - val_auc_46: 0.7965 - val_recall_m: 0.7724 - val_precision_m: 0.7061 - val_f1_m: 0.7160\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5197 - acc: 0.7442 - auc_46: 0.8196 - recall_m: 0.7939 - precision_m: 0.7368 - f1_m: 0.7427 - val_loss: 0.5507 - val_acc: 0.7202 - val_auc_46: 0.7970 - val_recall_m: 0.7925 - val_precision_m: 0.7046 - val_f1_m: 0.7193\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5195 - acc: 0.7452 - auc_46: 0.8193 - recall_m: 0.7906 - precision_m: 0.7333 - f1_m: 0.7399 - val_loss: 0.5525 - val_acc: 0.7208 - val_auc_46: 0.7962 - val_recall_m: 0.7592 - val_precision_m: 0.7119 - val_f1_m: 0.7095\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5194 - acc: 0.7442 - auc_46: 0.8197 - recall_m: 0.7845 - precision_m: 0.7347 - f1_m: 0.7372 - val_loss: 0.5519 - val_acc: 0.7228 - val_auc_46: 0.7975 - val_recall_m: 0.7766 - val_precision_m: 0.6893 - val_f1_m: 0.7041\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5197 - acc: 0.7457 - auc_46: 0.8196 - recall_m: 0.8013 - precision_m: 0.7377 - f1_m: 0.7441 - val_loss: 0.5485 - val_acc: 0.7228 - val_auc_46: 0.7966 - val_recall_m: 0.7591 - val_precision_m: 0.7142 - val_f1_m: 0.7115\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5194 - acc: 0.7437 - auc_46: 0.8197 - recall_m: 0.7943 - precision_m: 0.7336 - f1_m: 0.7384 - val_loss: 0.5487 - val_acc: 0.7221 - val_auc_46: 0.7965 - val_recall_m: 0.7857 - val_precision_m: 0.7138 - val_f1_m: 0.7182\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.53872\n",
      "6275/6275 - 2s - loss: 0.5194 - acc: 0.7457 - auc_46: 0.8198 - recall_m: 0.7983 - precision_m: 0.7393 - f1_m: 0.7442 - val_loss: 0.5507 - val_acc: 0.7240 - val_auc_46: 0.7969 - val_recall_m: 0.7890 - val_precision_m: 0.6975 - val_f1_m: 0.7175\n",
      "Epoch 164/200\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"FF_WOE\"\n",
    "EPOCHS = 200\n",
    "PATIENCE = 200\n",
    "BATCH_SIZE = 8\n",
    "HIDDEN_LAYERS = [10]\n",
    "ACTIVATION = 'relu'\n",
    "L_RATE = 0.001\n",
    "model_name = '{}_ep{}_pa{}_bs{}_hs{}_lr_{}_{}'.format(base_model_name,EPOCHS,PATIENCE,BATCH_SIZE,HIDDEN_LAYERS,L_RATE,ACTIVATION)\n",
    "model_name\n",
    "model = get_FFNN_model(X_train, y_onehot, HIDDEN_LAYERS)\n",
    "\n",
    "model_path = os.path.join(RESULT_PATH, model_name)\n",
    "\n",
    "forge_gen = True\n",
    "\n",
    "if not os.path.exists(model_path) or forge_gen:\n",
    "    history = net_train(model, model_path, X_train, y_train, X_test, y_test, EPOCHS, save_model=True, VERBOSE=2)    \n",
    "    \n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    plt.figure(figsize=(14,6))\n",
    "    for key in history.history.keys():\n",
    "        plt.plot(history.history[key], label=key)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=.2)\n",
    "    plt.title(f'batch_size = {BATCH_SIZE}, epochs = {EPOCHS}')\n",
    "    plt.draw()\n",
    "else:\n",
    "    print('Model loaded.')\n",
    "    model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: \n",
      "acc: 73.75%\n",
      "Test Set: \n",
      "acc: 72.81%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 808,  416],\n",
       "       [ 295, 1096]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores = model.evaluate(X_onehot_train.values, y_onehot_train)\n",
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Training Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# scores = model.evaluate(X_onehot_test.values, y_onehot_test)\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Test Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "confusion_matrix(y_test, model.predict_classes(X_test), sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x00000183886EF288>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838C4EC288>]\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_38 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2092/2092 [==============================] - 0s 38us/sample - loss: 0.5592 - acc: 0.7247 - auc_34: 0.7910 - recall_m: 0.7631 - precision_m: 0.6984 - f1_m: 0.7196\n",
      "2\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x000001838C4E8F48>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838F7A5DC8>]\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2092/2092 [==============================] - 0s 43us/sample - loss: 0.5719 - acc: 0.7132 - auc_36: 0.7775 - recall_m: 0.7225 - precision_m: 0.6703 - f1_m: 0.6862\n",
      "3\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x0000018381BD3D48>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838F9AD608>]\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2092/2092 [==============================] - 0s 34us/sample - loss: 0.5555 - acc: 0.7199 - auc_38: 0.7917 - recall_m: 0.7338 - precision_m: 0.6936 - f1_m: 0.7023\n",
      "4\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x000001838C4E8988>, <tensorflow.python.keras.layers.core.Dense object at 0x000001838FA1E1C8>]\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2092/2092 [==============================] - 0s 63us/sample - loss: 0.5660 - acc: 0.7137 - auc_40: 0.7800 - recall_m: 0.7588 - precision_m: 0.6839 - f1_m: 0.7083\n",
      "5\n",
      "[<tensorflow.python.keras.layers.core.Dense object at 0x000001838C28FF48>, <tensorflow.python.keras.layers.core.Dense object at 0x0000018390BAB5C8>]\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 10)                240       \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 251\n",
      "Trainable params: 251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2091/2091 [==============================] - 0s 46us/sample - loss: 0.5667 - acc: 0.7207 - auc_42: 0.7863 - recall_m: 0.7536 - precision_m: 0.6827 - f1_m: 0.7083\n",
      "\u001b[1m\n",
      "Mean Accuracy=  0.7184 , Sd=  0.0044\n",
      "Mean AUC=  0.7853 , Sd=  0.0057\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run_kf = True\n",
    "if run_kf:\n",
    "    n_folds = 5\n",
    "    kf = KFold(n_splits = n_folds, random_state = 1, shuffle = True)\n",
    "    \n",
    "    fold = 0\n",
    "    result_metrics_list = []\n",
    "    for train_idx, val_idx in kf.split(X1):\n",
    "        fold += 1\n",
    "        print(fold)\n",
    "        X_train = X1.iloc[train_idx]\n",
    "        y_train = y_onehot.iloc[train_idx]\n",
    "        X_val = X1.iloc[val_idx]\n",
    "        y_val = y_onehot.iloc[val_idx]\n",
    "\n",
    "        model = get_FFNN_model(X_train, y_train, HIDDEN_LAYERS)\n",
    "        history = net_train(model, model_path, X_train, y_train, X_val, y_val, EPOCHS, save_model=False, VERBOSE=False)    \n",
    "        score = model.evaluate(X_val, y_val)\n",
    "      \n",
    "        result_metrics_list.append([score])\n",
    "        \n",
    "    print(\"\\033[1m\")\n",
    "    print(\"Mean Accuracy= \",\"{:.4}\".format(np.mean(result_metrics_list,axis=0)[0][1]),\", Sd= \",\"{:.2}\".format(np.std(result_metrics_list,axis=0)[0][1]))\n",
    "    print(\"Mean AUC= \",\"{:.4}\".format(np.mean(result_metrics_list,axis=0)[0][2]),\", Sd= \",\"{:.2}\".format(np.std(result_metrics_list, axis=0)[0][2]))\n",
    "    print(\"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weights and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join('results')\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "print(latest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: \n",
      "acc: 73.57%\n",
      "Test Set: \n",
      "acc: 72.20%\n"
     ]
    }
   ],
   "source": [
    "new_model = load_model(checkpoint_dir,  compile=False)\n",
    "new_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.1),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy',recall_m,precision_m])\n",
    "\n",
    "def get_train_test_accuracy(model):\n",
    "    # scores = model.evaluate(X_onehot_train.values, y_onehot_train)\n",
    "    scores = model.evaluate(X_onehot_train, y_onehot_train, verbose=0)\n",
    "    print(\"Training Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "    # scores = model.evaluate(X_onehot_test.values, y_onehot_test)\n",
    "    scores = model.evaluate(X_onehot_test, y_onehot_test, verbose = 0)\n",
    "    print(\"Test Set:\", \"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "get_train_test_accuracy(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.72275335\n"
     ]
    }
   ],
   "source": [
    "baseline_model_accuracy = model.evaluate(\n",
    "    X_onehot_test, y_onehot_test, verbose=0)[1]\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276 0.001 0.0001\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_dense_10 (None, 10)                3712      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_batch_no (None, 10)                41        \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_11 (None, 1)                 23        \n",
      "=================================================================\n",
      "Total params: 3,776\n",
      "Trainable params: 1,891\n",
      "Non-trainable params: 1,885\n",
      "_________________________________________________________________\n",
      "Train on 5883 samples, validate on 1961 samples\n",
      "Epoch 1/3\n",
      "5883/5883 [==============================] - 2s 260us/sample - loss: 0.6288 - acc: 0.7178 - recall_m: 0.7542 - precision_m: 0.7215 - val_loss: 0.7146 - val_acc: 0.6512 - val_recall_m: 0.9392 - val_precision_m: 0.5988\n",
      "Epoch 2/3\n",
      "5883/5883 [==============================] - 1s 111us/sample - loss: 0.6218 - acc: 0.7182 - recall_m: 0.7424 - precision_m: 0.7287 - val_loss: 0.6841 - val_acc: 0.6910 - val_recall_m: 0.8998 - val_precision_m: 0.6368\n",
      "Epoch 3/3\n",
      "5883/5883 [==============================] - 1s 113us/sample - loss: 0.6149 - acc: 0.7180 - recall_m: 0.7489 - precision_m: 0.7254 - val_loss: 0.6321 - val_acc: 0.7160 - val_recall_m: 0.8365 - val_precision_m: 0.6779\n",
      "Baseline test accuracy: 0.72275335\n",
      "Pruned test accuracy: 0.7086042\n"
     ]
    }
   ],
   "source": [
    "run = True\n",
    "if run is not None:\n",
    "    import tensorflow_model_optimization as tfmot\n",
    "\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "    # Compute end step to finish pruning after 2 epochs.\n",
    "    validation_split = .25 # 10% of training set will be used for validation set. \n",
    "\n",
    "    num_images = X_onehot_train.shape[0] * (1 - validation_split)\n",
    "    end_step = np.ceil(num_images / BATCH_SIZE).astype(np.int32) * EPOCHS\n",
    "    initial_sparsity = 0.001\n",
    "    final_sparsity = 0.0001\n",
    "\n",
    "    print(end_step,initial_sparsity,final_sparsity)\n",
    "\n",
    "    # Define model for pruning.\n",
    "    pruning_params = {\n",
    "          'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "                                                                   final_sparsity=final_sparsity,\n",
    "                                                                   begin_step=0,\n",
    "                                                                   end_step=end_step)\n",
    "    }\n",
    "\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy',recall_m,precision_m])\n",
    "\n",
    "    model_for_pruning.summary()\n",
    "    \n",
    "    import tempfile\n",
    "    logdir = tempfile.mkdtemp()\n",
    "    EPOCHS = 3\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "      tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "    ]\n",
    "\n",
    "    model_for_pruning.fit(X_onehot_train, y_onehot_train,\n",
    "                      batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=validation_split,\n",
    "                      callbacks=callbacks)\n",
    "    \n",
    "    # strip dropped parameters and evaluate accuracy\n",
    "    model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "    model_for_export.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model_for_pruning_accuracy = model_for_export.evaluate(\n",
    "       X_onehot_test, y_onehot_test, verbose=0)[1]\n",
    "\n",
    "    print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "    print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # fit and evaluate a model\n",
    "# def evaluate_model(trainX, trainy, testX, testy, n_kernel):\n",
    "# \tverbose, epochs, batch_size = 1, 10, 10\n",
    "# \tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "# \tmodel = Sequential()\n",
    "# \tmodel.add(Conv1D(filters=10, kernel_size=n_kernel, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "# # \tmodel.add(Conv1D(filters=64, kernel_size=n_kernel, activation='relu'))\n",
    "# \tmodel.add(Dropout(0.5))\n",
    "# \tmodel.add(MaxPooling1D(pool_size=2))\n",
    "# \tmodel.add(Flatten())\n",
    "# \tmodel.add(Dense(10, activation='relu'))\n",
    "# \tmodel.add(Dense(n_outputs, activation='softmax'))\n",
    "# \tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# \t# fit network\n",
    "# \tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "# \t# evaluate model\n",
    "# \t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "# \treturn accuracy\n",
    " \n",
    "# # summarize scores\n",
    "# def summarize_results(scores, params):\n",
    "# \tprint(scores, params)\n",
    "# \t# summarize mean and standard deviation\n",
    "# \tfor i in range(len(scores)):\n",
    "# \t\tm, s = mean(scores[i]), std(scores[i])\n",
    "# \t\tprint('Param=%d: %.3f%% (+/-%.3f)' % (params[i], m, s))\n",
    "# \t# boxplot of scores\n",
    "# \tpyplot.boxplot(scores, labels=params)\n",
    "# \tpyplot.savefig('exp_cnn_kernel.png')\n",
    " \n",
    "# # run an experiment\n",
    "# def run_experiment(params, repeats=10):\n",
    "# \t# load data\n",
    "# \ttrainX, trainy, testX, testy = np.expand_dims(X_onehot_train,axis=2) ,y_onehot_train , np.expand_dims(X_onehot_test,axis=2) , y_onehot_test\n",
    "# \t# test each parameter\n",
    "# \tall_scores = list()\n",
    "# \tfor p in params:\n",
    "# \t\t# repeat experiment\n",
    "# \t\tscores = list()\n",
    "# \t\tfor r in range(repeats):\n",
    "# \t\t\tscore = evaluate_model(trainX, trainy, testX, testy, p)\n",
    "# \t\t\tscore = score * 100.0\n",
    "# \t\t\tprint('>p=%d #%d: %.3f' % (p, r+1, score))\n",
    "# \t\t\tscores.append(score)\n",
    "# \t\tall_scores.append(scores)\n",
    "# \t# summarize results\n",
    "# \tsummarize_results(all_scores, params)\n",
    " \n",
    "# # run the experiment\n",
    "# n_params = [2]\n",
    "# run_experiment(n_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
